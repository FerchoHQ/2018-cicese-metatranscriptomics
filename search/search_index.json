{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Environmental Metatranscriptomics, Nov 2018 These are the online materials for the environmental metatranscriptomics workshop to be run at CICESE . This workshop runs under a Code of Conduct . Please respect it and be excellent to each other! Day 1: Nov 8, 2018 Morning: 8:00-9:00am Introduction and Metatranscriptomics overview Logging into the cluster Bioconda for software installation Working on the command line Evaluating short-read quality Lunch Afternoon: Metatranscriptomic workflows and considerations Assembling your short read data set with MEGAHIT Evaluating your assemblies Day 2: Nov X, 2018 Morning: Mapping short reads to the assembly time permitting Sourmash compare Annotating your assembly with Prokka Binning genomes out of your metagenome Lunch Afternoon: Quantifying abundance across samples with Salmon Taxonomic classification with sourmash gather Putting it all together with Anvi\u2019o ) A brief discussion of workflows & repeatability Readings: Read the Critical Assessment of Metagenome Interpretation (CAMI) Paper; Read Hu et al. 2016. This is the paper from which we pulled all of our sample data. Additional Resources: SEQ Answers Biostars Data Carpentry DIB Summer Institute","title":"Overview"},{"location":"#environmental-metatranscriptomics-nov-2018","text":"These are the online materials for the environmental metatranscriptomics workshop to be run at CICESE . This workshop runs under a Code of Conduct . Please respect it and be excellent to each other! Day 1: Nov 8, 2018 Morning: 8:00-9:00am Introduction and Metatranscriptomics overview Logging into the cluster Bioconda for software installation Working on the command line Evaluating short-read quality Lunch Afternoon: Metatranscriptomic workflows and considerations Assembling your short read data set with MEGAHIT Evaluating your assemblies Day 2: Nov X, 2018 Morning: Mapping short reads to the assembly time permitting Sourmash compare Annotating your assembly with Prokka Binning genomes out of your metagenome Lunch Afternoon: Quantifying abundance across samples with Salmon Taxonomic classification with sourmash gather Putting it all together with Anvi\u2019o ) A brief discussion of workflows & repeatability Readings: Read the Critical Assessment of Metagenome Interpretation (CAMI) Paper; Read Hu et al. 2016. This is the paper from which we pulled all of our sample data. Additional Resources: SEQ Answers Biostars Data Carpentry DIB Summer Institute","title":"Environmental Metatranscriptomics, Nov 2018"},{"location":"anvio/","text":"Using Anvi'o to Knit Everything Together Now we are going bring it all together and visuzlize our assembly with Anvi'o . Anvi'o is a powerful and extensible tool that might be easily applied to pan-genomic analysis as well as metagenomic analysis. The anvi'o group has a series of fantstic online tutorials, including one on metagenomic analysis . They also run workshops periodically ( schedule here ) that throughly cover the use of the software. Today, we are adapting their tutorial on metagenomic analysis to work with the subset dataset that we have. The goals of this tutorial are to: Install anvi'o Become familiar with the anvi'o workflow Visualizing the assembly with anvi'o Become familiar with the anvi'o interface * Learn how to refine and visuzlize genome bins with anvi'o Installing anvi'o (and a few other programs) The first thing we need to do is install anvi'o. To install anvi'o we will be be using Anaconda . cd ~ wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.sh bash Anaconda3-4.4.0-Linux-x86_64.sh Now, follow the prompts for the Anaconda installation. To finish the install it will ask you if you would like to add anaconda to the $PATH in you .bashrc . You should say yes. Now, you just need to source your .bashrc to make sure you can use conda. source .bashrc Anaconda should now be installed. We will now use anaconda ( conda ) to install anvi'o (and all its dependencies) as well as source an environment in which to to run conda. Now, install anvi'o using conda, create an environment in which to run it, and source the environment: conda create -n anvio232 -c bioconda -c conda-forge gsl anvio=2.3.2 source activate anvio232 Anvi'o should now be installed. But, let's double check that it worked. They have a nice little test case to check that everything is working well as follows: anvi-self-test --suite mini This prompt will start anvi'o processing and ultimately it will generate an interactive window with the anvi'o environment. This is accessible through port 8080 (typically, though it might create go to a different port that will be specified) at your ec2 machine address. Now, open a new tab in your browser (NOTE: This only works in Google Chrome) and paste in the following: [Your EC Address]:8080 This should open up the anvi'o interface which is interactive and pretty good looking. Now, we just need to install a few other programs, namely, samtools and Bowtie2, which we will use for mapping and looking at our mapped data. wget https://downloads.sourceforge.net/project/bowtie-bio/bowtie2/2.3.2/bowtie2-2.3.2-linux-x86_64.zip unzip bowtie2-2.3.2-linux-x86_64.zip echo 'export PATH=$PATH:~/bowtie2-2.3.2' >> ~/.bashrc source ~/.bashrc sudo apt-get -y install samtools Alright, now onto a complete re-analysis of our data with the anvi'o pipeline. Getting it into Anvi'o format Anvi'o takes in 1) an assembly and 2) the raw read data. We have both of those already created, so let's go ahead and download those data (trimmed reads and asssemblies): mkdir ~/anvio-work cd ~/anvio-work curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/SRR1976948.abundtrim.subset.pe.fq.gz curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/SRR1977249.abundtrim.subset.pe.fq.gz curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/subset_assembly.fa.gz And, gunzip those files: for file in *gz do gunzip $file done We now need to get our assembly into the correct format so that anvi'o interpret it. anvi-script-reformat-fasta subset_assembly.fa -o anvio-contigs.fa --min-len 2000 --simplify-names --report name_conversions.txt Take a look at the output files. What has changed? Mapping data We need to map our reads to our anvi'o corrected assembly. This is going to take a little bit of time. First, build the an index for bowtie2: bowtie2-build anvio-contigs.fa anvio-contigs We can write a for loop to map our two datasets and produce .bam files for the files: for file in *fq do bowtie2 --threads 8 -x anvio-contigs --interleaved $file -S ${file/.fq/}.sam samtools view -U 4 -bS ${file/.fq/}.sam > ${file/.fq/}.bam done As above, we need to make these data readable for anvi'o: for file in *.bam do anvi-init-bam ${file} -o ${file/.bam/}.anvio.bam done Generating contigs database In this step we are asking anvi'o to create a database with information about your contigs. The contig database is fairly extensible and can contain lots of different information (taxonomic, functional, etc.). Here, we are primarily asking it to do three things: 1) 'Soft split' long contigs (>20k): Anvi'o shows the generalized statistics for each contig (GC content, etc.). For long contigs these stats are calculated across split contigs (which remain grouped) 2) Identify and locate open reading frames in your contigs (using Prodigal) 3) Estimate Single Copy Gene content (using hmmer against defined gene sets for bacteria and archaea ) 3) Calculate k-mer frequencies for the contigs in our assemblies So, run the following command to generate the database: anvi-gen-contigs-database -f anvio-contigs.fa -o anvio-contigs.db Then, run this command to perform the hmm search and identify single copy gene content: anvi-run-hmms -c anvio-contigs.db --num-threads 28 Now, we can layer on the coverge information from our two samples: for file in *.anvio.bam do anvi-profile -i $file -c anvio-contigs.db -T 28 done And finally, we run the merge step. This will pull all the information together and create a merged anvi'o profile. This step will also run CONCOCT ( another binning algorithm) that will identify bins in our data. Finally, this step calculates the hierarchical relationship betwewen our contigs based on a variety of parameters. anvi-merge *ANVIO_PROFILE/PROFILE.db -o MERGED-SAMPLES -c anvio-contigs.db --enforce-hierarchical-clustering Now we can visualize our data! anvi-interactive -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db Identifying and refining genome bins First, let's summarize the bin information for our data. This will produce a series of text-based output files detailing some statistics on our genome bins: anvi-summarize -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -o SAMPLES-SUMMARY -C CONCOCT Take a look at the output in SAMPLES-SUMMARY . What does it report? Now you can visualize those data in the anvi'o style by simply adding the -C flag to the previous anvi-interactive command: anvi-interactive -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -C CONCOCT Now, we can actually refine the genome bins using anvi'o. This allows us to use human intuition and pattern recognition to better identify contigs that should co-occur. It is important that we make a copy of the original data so that we don't accidentally overwrite it. So make a copy of the directory: cp -avr SAMPLES-SUMMARY/ SAMPLES-SUMMARY-ORIGININAL/ Now, let's refine a bin! Let's start with Bin_4. anvi-refine -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -b Bin_4 -C CONCOCT Finally, it is time to interact with the anvi'o interface. Here are some screenshots to help guide you in your quest. And of course a big thank you to Meren for providing us with extra materials to help create this tutorial!","title":"Using Anvi'o to Knit Everything Together"},{"location":"anvio/#using-anvio-to-knit-everything-together","text":"Now we are going bring it all together and visuzlize our assembly with Anvi'o . Anvi'o is a powerful and extensible tool that might be easily applied to pan-genomic analysis as well as metagenomic analysis. The anvi'o group has a series of fantstic online tutorials, including one on metagenomic analysis . They also run workshops periodically ( schedule here ) that throughly cover the use of the software. Today, we are adapting their tutorial on metagenomic analysis to work with the subset dataset that we have. The goals of this tutorial are to: Install anvi'o Become familiar with the anvi'o workflow Visualizing the assembly with anvi'o Become familiar with the anvi'o interface * Learn how to refine and visuzlize genome bins with anvi'o","title":"Using Anvi'o to Knit Everything Together"},{"location":"anvio/#installing-anvio-and-a-few-other-programs","text":"The first thing we need to do is install anvi'o. To install anvi'o we will be be using Anaconda . cd ~ wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.sh bash Anaconda3-4.4.0-Linux-x86_64.sh Now, follow the prompts for the Anaconda installation. To finish the install it will ask you if you would like to add anaconda to the $PATH in you .bashrc . You should say yes. Now, you just need to source your .bashrc to make sure you can use conda. source .bashrc Anaconda should now be installed. We will now use anaconda ( conda ) to install anvi'o (and all its dependencies) as well as source an environment in which to to run conda. Now, install anvi'o using conda, create an environment in which to run it, and source the environment: conda create -n anvio232 -c bioconda -c conda-forge gsl anvio=2.3.2 source activate anvio232 Anvi'o should now be installed. But, let's double check that it worked. They have a nice little test case to check that everything is working well as follows: anvi-self-test --suite mini This prompt will start anvi'o processing and ultimately it will generate an interactive window with the anvi'o environment. This is accessible through port 8080 (typically, though it might create go to a different port that will be specified) at your ec2 machine address. Now, open a new tab in your browser (NOTE: This only works in Google Chrome) and paste in the following: [Your EC Address]:8080 This should open up the anvi'o interface which is interactive and pretty good looking. Now, we just need to install a few other programs, namely, samtools and Bowtie2, which we will use for mapping and looking at our mapped data. wget https://downloads.sourceforge.net/project/bowtie-bio/bowtie2/2.3.2/bowtie2-2.3.2-linux-x86_64.zip unzip bowtie2-2.3.2-linux-x86_64.zip echo 'export PATH=$PATH:~/bowtie2-2.3.2' >> ~/.bashrc source ~/.bashrc sudo apt-get -y install samtools Alright, now onto a complete re-analysis of our data with the anvi'o pipeline.","title":"Installing anvi'o (and a few other programs)"},{"location":"anvio/#getting-it-into-anvio-format","text":"Anvi'o takes in 1) an assembly and 2) the raw read data. We have both of those already created, so let's go ahead and download those data (trimmed reads and asssemblies): mkdir ~/anvio-work cd ~/anvio-work curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/SRR1976948.abundtrim.subset.pe.fq.gz curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/SRR1977249.abundtrim.subset.pe.fq.gz curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/subset_assembly.fa.gz And, gunzip those files: for file in *gz do gunzip $file done We now need to get our assembly into the correct format so that anvi'o interpret it. anvi-script-reformat-fasta subset_assembly.fa -o anvio-contigs.fa --min-len 2000 --simplify-names --report name_conversions.txt Take a look at the output files. What has changed?","title":"Getting it into Anvi'o format"},{"location":"anvio/#mapping-data","text":"We need to map our reads to our anvi'o corrected assembly. This is going to take a little bit of time. First, build the an index for bowtie2: bowtie2-build anvio-contigs.fa anvio-contigs We can write a for loop to map our two datasets and produce .bam files for the files: for file in *fq do bowtie2 --threads 8 -x anvio-contigs --interleaved $file -S ${file/.fq/}.sam samtools view -U 4 -bS ${file/.fq/}.sam > ${file/.fq/}.bam done As above, we need to make these data readable for anvi'o: for file in *.bam do anvi-init-bam ${file} -o ${file/.bam/}.anvio.bam done","title":"Mapping data"},{"location":"anvio/#generating-contigs-database","text":"In this step we are asking anvi'o to create a database with information about your contigs. The contig database is fairly extensible and can contain lots of different information (taxonomic, functional, etc.). Here, we are primarily asking it to do three things: 1) 'Soft split' long contigs (>20k): Anvi'o shows the generalized statistics for each contig (GC content, etc.). For long contigs these stats are calculated across split contigs (which remain grouped) 2) Identify and locate open reading frames in your contigs (using Prodigal) 3) Estimate Single Copy Gene content (using hmmer against defined gene sets for bacteria and archaea ) 3) Calculate k-mer frequencies for the contigs in our assemblies So, run the following command to generate the database: anvi-gen-contigs-database -f anvio-contigs.fa -o anvio-contigs.db Then, run this command to perform the hmm search and identify single copy gene content: anvi-run-hmms -c anvio-contigs.db --num-threads 28 Now, we can layer on the coverge information from our two samples: for file in *.anvio.bam do anvi-profile -i $file -c anvio-contigs.db -T 28 done And finally, we run the merge step. This will pull all the information together and create a merged anvi'o profile. This step will also run CONCOCT ( another binning algorithm) that will identify bins in our data. Finally, this step calculates the hierarchical relationship betwewen our contigs based on a variety of parameters. anvi-merge *ANVIO_PROFILE/PROFILE.db -o MERGED-SAMPLES -c anvio-contigs.db --enforce-hierarchical-clustering Now we can visualize our data! anvi-interactive -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db","title":"Generating contigs database"},{"location":"anvio/#identifying-and-refining-genome-bins","text":"First, let's summarize the bin information for our data. This will produce a series of text-based output files detailing some statistics on our genome bins: anvi-summarize -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -o SAMPLES-SUMMARY -C CONCOCT Take a look at the output in SAMPLES-SUMMARY . What does it report? Now you can visualize those data in the anvi'o style by simply adding the -C flag to the previous anvi-interactive command: anvi-interactive -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -C CONCOCT Now, we can actually refine the genome bins using anvi'o. This allows us to use human intuition and pattern recognition to better identify contigs that should co-occur. It is important that we make a copy of the original data so that we don't accidentally overwrite it. So make a copy of the directory: cp -avr SAMPLES-SUMMARY/ SAMPLES-SUMMARY-ORIGININAL/ Now, let's refine a bin! Let's start with Bin_4. anvi-refine -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -b Bin_4 -C CONCOCT Finally, it is time to interact with the anvi'o interface. Here are some screenshots to help guide you in your quest. And of course a big thank you to Meren for providing us with extra materials to help create this tutorial!","title":"Identifying and refining genome bins"},{"location":"code-of-conduct/","text":"Workshop Code of Conduct All attendees, speakers, sponsors and volunteers at our workshop are required to agree with the following code of conduct. Organisers will enforce this code throughout the event. We are expecting cooperation from all participants to help ensuring a safe environment for everybody. tl; dr: be excellent to each other. Need Help? You can reach the course director, Titus Brown, at ctbrown@ucdavis.edu. You can also talk to any of the instructors or TAs if you need immediate help. The Quick Version Our workshop is dedicated to providing a harassment-free workshop experience for everyone, regardless of gender, age, sexual orientation, disability, physical appearance, body size, race, or religion (or lack thereof). We do not tolerate harassment of workshop participants in any form. Sexual language and imagery is not appropriate for any workshop venue, including talks, workshops, parties, Twitter and other online media. Workshop participants violating these rules may be sanctioned or expelled from the workshop without a refund at the discretion of the workshop organisers. The Less Quick Version Harassment includes offensive verbal comments related to gender, age, sexual orientation, disability, physical appearance, body size, race, religion, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention. Participants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, the workshop organisers may take any action they deem appropriate, including warning the offender or expulsion from the workshop with no refund. If you are being harassed, notice that someone else is being harassed, or have any other concerns, please contact a member of workshop staff immediately. Workshop instructors and TAs will be happy to help participants contact KBS security or local law enforcement, provide escorts, or otherwise assist those experiencing harassment to feel safe for the duration of the workshop. We value your attendance. We expect participants to follow these rules at workshop and workshop venues and workshop-related social events. This work is licensed under a Creative Commons Attribution 3.0 Unported License . This Code of Conduct taken from http://confcodeofconduct.com/. See http://www.ashedryden.com/blog/codes-of-conduct-101-faq for more information on codes of conduct.","title":"Code of Conduct"},{"location":"code-of-conduct/#workshop-code-of-conduct","text":"All attendees, speakers, sponsors and volunteers at our workshop are required to agree with the following code of conduct. Organisers will enforce this code throughout the event. We are expecting cooperation from all participants to help ensuring a safe environment for everybody. tl; dr: be excellent to each other.","title":"Workshop Code of Conduct"},{"location":"code-of-conduct/#need-help","text":"You can reach the course director, Titus Brown, at ctbrown@ucdavis.edu. You can also talk to any of the instructors or TAs if you need immediate help.","title":"Need Help?"},{"location":"code-of-conduct/#the-quick-version","text":"Our workshop is dedicated to providing a harassment-free workshop experience for everyone, regardless of gender, age, sexual orientation, disability, physical appearance, body size, race, or religion (or lack thereof). We do not tolerate harassment of workshop participants in any form. Sexual language and imagery is not appropriate for any workshop venue, including talks, workshops, parties, Twitter and other online media. Workshop participants violating these rules may be sanctioned or expelled from the workshop without a refund at the discretion of the workshop organisers.","title":"The Quick Version"},{"location":"code-of-conduct/#the-less-quick-version","text":"Harassment includes offensive verbal comments related to gender, age, sexual orientation, disability, physical appearance, body size, race, religion, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention. Participants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, the workshop organisers may take any action they deem appropriate, including warning the offender or expulsion from the workshop with no refund. If you are being harassed, notice that someone else is being harassed, or have any other concerns, please contact a member of workshop staff immediately. Workshop instructors and TAs will be happy to help participants contact KBS security or local law enforcement, provide escorts, or otherwise assist those experiencing harassment to feel safe for the duration of the workshop. We value your attendance. We expect participants to follow these rules at workshop and workshop venues and workshop-related social events. This work is licensed under a Creative Commons Attribution 3.0 Unported License . This Code of Conduct taken from http://confcodeofconduct.com/. See http://www.ashedryden.com/blog/codes-of-conduct-101-faq for more information on codes of conduct.","title":"The Less Quick Version"},{"location":"count_transcriptomes/","text":"Counting the number of transcriptomes in a metatranscriptome In metagenomics, we use binning to approximate the number of genomes (or species) in a sample. Although we can apply taxonomic classification methods like sourmash to metatranscriptomic samples, these methods only work reliably when sequences already exist for the organism of interest. This is still a relatively rare occurrence in environmental metatranscriptomics. We will make use of other data in our samples to approximate the number of species present. Although our samples were poly-A selected, inevitably a some ribosomal RNA is always sequenced given its abundance in samples. We can estimate the number of species in our metatranscriptome by counting the unique number of ribosomal protein sequences we see. There are many ribosomal proteins one could use for this analysis, and we provide a table of these at the bottom of this lesson. We will use three sequences here, and average the number of unique sequences we detect between the three sequences to estimate the number of transcriptomes in our sample. We will first search for these proteins in the amino acid sequences derived from our de novo assembly of our metatranscriptome. We will use Pfam domains and a tool called HMMER to help us locate all of the matching sequences. Then, we will XXX. Because we are only searching in our assembly, this method only captures the number of transcriptomes that assembled. First let's download some Pfam domains. wget Next we'll build a HMM profile of the Pfam domains. hmmbuild We then use the HMM profile to search the proteins from our assembly hmmscan Let's take a look at one of the files output by this search less -S The xx column contains the names of our protein sequences that matched these domains. We can use those names to extract our matches from our assembly # Grab the names # extract the matches Let's count the number of sequences that matched grep \">\" XXX | wc -l Some matches are quite similar to each other. Let's cluster our sequences at 97% similarity and see how this changes the number of unique proteins we detect. cdhit ... From this, we estimate there are XX transcriptomes assembled from our metatranscriptome. This technique can also be used to find any Pfam domain of interest. For instance, if you are interested in photosynthesis, you could extract all photosynthetic proteins by searching with the Pfam domains. Other proteins to use for quantifcation We used three proteins to quantify the number of transcriptomes in our sample. There are more that can be used, and they are list in the table below. They are originally derived from Carradec et al. 2018 . name COG PFAM RpsG COG0049 PF00177 RpsB COG0052 PF00318 RplK COG0080 PF03946 RplA COG0081 PF00687 RplC COG0087 PF00297 RplD COG0088 PF00573 RplV COG0091 PF00237 RpsC COG0092 PF00189 RplN COG0093 PF01929 RplE COG0094 PF00281 RpsH COG0096 PF00410 RplF COG0097 NA RpsE COG0098 PF03719, PF00333 RpsM COG0099 PF00416 RpsK COG0100 PF00411 RplM COG0102 PF00572 RpsI COG0103 PF00380 RpsO COG0184 NA RpsS COG0185 PF00203 RpsQ COG0186 PF00366 GyrB COG0187 PF00204 GyrA COG0188 PF00521 RimK COG0189 PF08443 FolD COG0190 PF00763, PF02882 Fba COG0191 NA MetK COG0192 PF01941 Pth COG0193 PF01195 Gmk COG0194 PF00625 RplO COG0200 PF00827 RplR COG0256 NA RpsD COG0522 PF00163","title":"Count Transcriptomes"},{"location":"count_transcriptomes/#counting-the-number-of-transcriptomes-in-a-metatranscriptome","text":"In metagenomics, we use binning to approximate the number of genomes (or species) in a sample. Although we can apply taxonomic classification methods like sourmash to metatranscriptomic samples, these methods only work reliably when sequences already exist for the organism of interest. This is still a relatively rare occurrence in environmental metatranscriptomics. We will make use of other data in our samples to approximate the number of species present. Although our samples were poly-A selected, inevitably a some ribosomal RNA is always sequenced given its abundance in samples. We can estimate the number of species in our metatranscriptome by counting the unique number of ribosomal protein sequences we see. There are many ribosomal proteins one could use for this analysis, and we provide a table of these at the bottom of this lesson. We will use three sequences here, and average the number of unique sequences we detect between the three sequences to estimate the number of transcriptomes in our sample. We will first search for these proteins in the amino acid sequences derived from our de novo assembly of our metatranscriptome. We will use Pfam domains and a tool called HMMER to help us locate all of the matching sequences. Then, we will XXX. Because we are only searching in our assembly, this method only captures the number of transcriptomes that assembled.","title":"Counting the number of transcriptomes in a metatranscriptome"},{"location":"count_transcriptomes/#other-proteins-to-use-for-quantifcation","text":"We used three proteins to quantify the number of transcriptomes in our sample. There are more that can be used, and they are list in the table below. They are originally derived from Carradec et al. 2018 . name COG PFAM RpsG COG0049 PF00177 RpsB COG0052 PF00318 RplK COG0080 PF03946 RplA COG0081 PF00687 RplC COG0087 PF00297 RplD COG0088 PF00573 RplV COG0091 PF00237 RpsC COG0092 PF00189 RplN COG0093 PF01929 RplE COG0094 PF00281 RpsH COG0096 PF00410 RplF COG0097 NA RpsE COG0098 PF03719, PF00333 RpsM COG0099 PF00416 RpsK COG0100 PF00411 RplM COG0102 PF00572 RpsI COG0103 PF00380 RpsO COG0184 NA RpsS COG0185 PF00203 RpsQ COG0186 PF00366 GyrB COG0187 PF00204 GyrA COG0188 PF00521 RimK COG0189 PF08443 FolD COG0190 PF00763, PF02882 Fba COG0191 NA MetK COG0192 PF01941 Pth COG0193 PF01195 Gmk COG0194 PF00625 RplO COG0200 PF00827 RplR COG0256 NA RpsD COG0522 PF00163","title":"Other proteins to use for quantifcation"},{"location":"for-instructors/","text":"For Instructors: To use mkdocs to deploy lessons: If mkdocs is already set up All docs should be written in md in the docs folder, and committed to the repo as usual. If you added new documentation, be sure to add it into the site navigation in mkdocs.yml , or link to it from another doc. Install mkdocs and ghp-import if necessary: conda install -c conda-forge mkdocs conda install -c conda-forge ghp-import Build docs folder into site/ mkdocs build View changes locally: mkdocs serve Push docs changes to gh-pages branch ghp-import site -p If setting up mkdocs: Grab the mkdocs repo, dib lab flavor, and follow setup instructions git clone https://github.com/dib-lab/mkdocs-material-dib.git","title":"For Instructors:"},{"location":"for-instructors/#for-instructors","text":"To use mkdocs to deploy lessons:","title":"For Instructors:"},{"location":"for-instructors/#if-mkdocs-is-already-set-up","text":"All docs should be written in md in the docs folder, and committed to the repo as usual. If you added new documentation, be sure to add it into the site navigation in mkdocs.yml , or link to it from another doc. Install mkdocs and ghp-import if necessary: conda install -c conda-forge mkdocs conda install -c conda-forge ghp-import Build docs folder into site/ mkdocs build View changes locally: mkdocs serve Push docs changes to gh-pages branch ghp-import site -p","title":"If mkdocs is already set up"},{"location":"for-instructors/#if-setting-up-mkdocs","text":"Grab the mkdocs repo, dib lab flavor, and follow setup instructions git clone https://github.com/dib-lab/mkdocs-material-dib.git","title":"If setting up mkdocs:"},{"location":"getting-started-rstudio/","text":"Rstudio - Getting started Connect to RStudio by setting your password (note, password will not be visible on the screen): sudo passwd $USER figuring out your username: echo My username is $USER and finding YOUR RStudio server interface Web address: echo http://$(hostname):8787/ Now go to that Web address in your Web browser, and log in with the username and password from above.","title":"Getting started rstudio"},{"location":"getting-started-rstudio/#rstudio-getting-started","text":"Connect to RStudio by setting your password (note, password will not be visible on the screen): sudo passwd $USER figuring out your username: echo My username is $USER and finding YOUR RStudio server interface Web address: echo http://$(hostname):8787/ Now go to that Web address in your Web browser, and log in with the username and password from above.","title":"Rstudio - Getting started"},{"location":"working-with-bioconda/","text":"Jetstream: working with bioconda. Learning objectives: learn what bioconda is understand basic conda commands learn how to list installed software packages learn how to manage multiple installation environments What is bioconda? See the bioconda paper and the bioconda web site . Bioconda is a community-enabled repository of 3,000+ bioinformatics packages, installable via the conda package manager. It consists of a set of recipes, like this one, for sourmash , that are maintained by the community. It just works, and it's effin' magic!! What problems does conda (and therefore bioconda) solve? Conda tracks installed packages and their versions, and makes sure that different installed packages don't have conflicting dependencies (we'll explain this below). Installing conda and enabling bioconda Download and install miniconda in your home directory: cd curl -O -L https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3 #install in $HOME directory echo export PATH=$PATH:/$HOME/miniconda3/bin >> ~/.bashrc Then, run the following command (or start a new terminal session) in order to activate the conda environment: source ~/.bashrc Configure channels for installing software. Note: It is important to add them in this order so that the priority is set correctly (that is, conda-forge is highest priority). conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Try installing something: conda install sourmash and running it -- sourmash will produce some output. (We'll tell you more about sourmash later.) yay! ## Using conda Conda is a \"package manager\" or software installer. See [the full list of commands](https://conda.io/docs/commands.html). `conda install` to install a package. `conda list` to list installed packages. `conda search` to search packages. Note that you'll see one package for every version of the software and for every version of Python (e.g. `conda search sourmash`). ## Using bioconda bioconda is a channel for conda, which just means that you can \"add\" it to conda as a source of packages. That's what the `conda config` above does. Note, Bioconda supports only 64-bit Linux and Mac OSX. You can check out [the bioconda site](https://bioconda.github.io/). ### Finding bioconda packages You can use `conda search`, or you can use google, or you can go visit [the list of recipes](https://bioconda.github.io/recipes.html#recipes). ### Freezing an environment This will save the list of **conda-installed** software you have in a particular environment to the file `packages.txt`: conda list --export packages.txt (it will not record the software versions for software not installed by conda.) conda install --file=packages.txt will install those packages in your local environment. ### Constructing and using multiple environments A feature that we do not use much here, but that can be very handy in some circumstances, is different environments. \"Environments\" are multiple different collections of installed software. There are two reasons you might want to do this: * first, you might want to try to exactly replicate a specific software install, so that you can replicate a paper or an old condition. * second, you might be working with incompatible software, e.g. sometimes different software pipelines need different version of the same software. An example of this is older bioinformatics software that needs python2, while other software needs python3. To create a new environment named `pony`, type: conda create -n pony Then to activate (switch to) that environment, type: source activate pony And now when you run `conda install`, it will install packages into this new environment, e.g. conda install -y checkm-genome (note here that checkm-genome *requires* python 2). To list environments, type: conda env list and you will see that you have two environments, `base` and `pony`, and `pony` has a `*` next to it because that's your current environment. And finally, to switch back to your base environment, do: source activate base ``` and you'll be back in the original environment. Meditations on reproducibility and provenance If you want to impress reviewers and also keep track of what your software versions are, you can: manage all your software inside of conda use conda list --export software.txt to create a list of all your software and put it in your supplementary material. This is also something that you can record for yourself, so that if you are trying to exactly reproduce Using it on your own compute system (laptop or HPC) conda works on Windows, Mac, and Linux. bioconda works on Mac and Linux. It does not require admin privileges to install, so you can install it on your own local cluster quite easily.","title":"Bioconda"},{"location":"working-with-bioconda/#jetstream-working-with-bioconda","text":"Learning objectives: learn what bioconda is understand basic conda commands learn how to list installed software packages learn how to manage multiple installation environments","title":"Jetstream: working with bioconda."},{"location":"working-with-bioconda/#what-is-bioconda","text":"See the bioconda paper and the bioconda web site . Bioconda is a community-enabled repository of 3,000+ bioinformatics packages, installable via the conda package manager. It consists of a set of recipes, like this one, for sourmash , that are maintained by the community. It just works, and it's effin' magic!!","title":"What is bioconda?"},{"location":"working-with-bioconda/#what-problems-does-conda-and-therefore-bioconda-solve","text":"Conda tracks installed packages and their versions, and makes sure that different installed packages don't have conflicting dependencies (we'll explain this below).","title":"What problems does conda (and therefore bioconda) solve?"},{"location":"working-with-bioconda/#installing-conda-and-enabling-bioconda","text":"Download and install miniconda in your home directory: cd curl -O -L https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3 #install in $HOME directory echo export PATH=$PATH:/$HOME/miniconda3/bin >> ~/.bashrc Then, run the following command (or start a new terminal session) in order to activate the conda environment: source ~/.bashrc Configure channels for installing software. Note: It is important to add them in this order so that the priority is set correctly (that is, conda-forge is highest priority). conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Try installing something: conda install sourmash and running it -- sourmash will produce some output. (We'll tell you more about sourmash later.) yay! ## Using conda Conda is a \"package manager\" or software installer. See [the full list of commands](https://conda.io/docs/commands.html). `conda install` to install a package. `conda list` to list installed packages. `conda search` to search packages. Note that you'll see one package for every version of the software and for every version of Python (e.g. `conda search sourmash`). ## Using bioconda bioconda is a channel for conda, which just means that you can \"add\" it to conda as a source of packages. That's what the `conda config` above does. Note, Bioconda supports only 64-bit Linux and Mac OSX. You can check out [the bioconda site](https://bioconda.github.io/). ### Finding bioconda packages You can use `conda search`, or you can use google, or you can go visit [the list of recipes](https://bioconda.github.io/recipes.html#recipes). ### Freezing an environment This will save the list of **conda-installed** software you have in a particular environment to the file `packages.txt`: conda list --export packages.txt (it will not record the software versions for software not installed by conda.) conda install --file=packages.txt will install those packages in your local environment. ### Constructing and using multiple environments A feature that we do not use much here, but that can be very handy in some circumstances, is different environments. \"Environments\" are multiple different collections of installed software. There are two reasons you might want to do this: * first, you might want to try to exactly replicate a specific software install, so that you can replicate a paper or an old condition. * second, you might be working with incompatible software, e.g. sometimes different software pipelines need different version of the same software. An example of this is older bioinformatics software that needs python2, while other software needs python3. To create a new environment named `pony`, type: conda create -n pony Then to activate (switch to) that environment, type: source activate pony And now when you run `conda install`, it will install packages into this new environment, e.g. conda install -y checkm-genome (note here that checkm-genome *requires* python 2). To list environments, type: conda env list and you will see that you have two environments, `base` and `pony`, and `pony` has a `*` next to it because that's your current environment. And finally, to switch back to your base environment, do: source activate base ``` and you'll be back in the original environment.","title":"Installing conda and enabling bioconda"},{"location":"working-with-bioconda/#meditations-on-reproducibility-and-provenance","text":"If you want to impress reviewers and also keep track of what your software versions are, you can: manage all your software inside of conda use conda list --export software.txt to create a list of all your software and put it in your supplementary material. This is also something that you can record for yourself, so that if you are trying to exactly reproduce","title":"Meditations on reproducibility and provenance"},{"location":"working-with-bioconda/#using-it-on-your-own-compute-system-laptop-or-hpc","text":"conda works on Windows, Mac, and Linux. bioconda works on Mac and Linux. It does not require admin privileges to install, so you can install it on your own local cluster quite easily.","title":"Using it on your own compute system (laptop or HPC)"}]}