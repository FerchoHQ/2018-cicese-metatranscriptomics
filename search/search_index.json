{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Environmental Metatranscriptomics, Nov 2018 These are the online materials for the environmental metatranscriptomics workshop to be run at CICESE . This workshop runs under a Code of Conduct . Please respect it and be excellent to each other! Please use the following hackmd for notes: https://tinyurl.com/cicese-metaT Day 1: Nov 8, 2018 Morning: Introduction and Metatranscriptomics overview Bienvenidos! Logging into the cluster Setting up the software Intro to TARA Oceans sample data Short read quality control Lunch Afternoon: Taxonomic classification with sourmash gather Sample Comparison Metatranscriptomic workflows and considerations Day 2: Nov 9, 2018 Morning: Error Trimming Metatranscriptome Assembly with Megahit Assessing assembly quality Annotating your assembly Counting transcriptomes Lunch Afternoon: Filezilla Bioconda for software installation A brief discussion of workflows & repeatability Additional Resources: Installing this software in the future References SEQ Answers Biostars Data Carpentry DIB Summer Institute Suggested readings A global ocean atlas of eukaryotic genes - This is the paper from which we pulled all of our sample data.","title":"Schedule"},{"location":"#environmental-metatranscriptomics-nov-2018","text":"These are the online materials for the environmental metatranscriptomics workshop to be run at CICESE . This workshop runs under a Code of Conduct . Please respect it and be excellent to each other! Please use the following hackmd for notes: https://tinyurl.com/cicese-metaT","title":"Environmental Metatranscriptomics, Nov 2018"},{"location":"#day-1-nov-8-2018","text":"Morning: Introduction and Metatranscriptomics overview Bienvenidos! Logging into the cluster Setting up the software Intro to TARA Oceans sample data Short read quality control Lunch Afternoon: Taxonomic classification with sourmash gather Sample Comparison Metatranscriptomic workflows and considerations","title":"Day 1: Nov 8, 2018"},{"location":"#day-2-nov-9-2018","text":"Morning: Error Trimming Metatranscriptome Assembly with Megahit Assessing assembly quality Annotating your assembly Counting transcriptomes Lunch Afternoon: Filezilla Bioconda for software installation A brief discussion of workflows & repeatability","title":"Day 2: Nov 9, 2018"},{"location":"#additional-resources","text":"Installing this software in the future References SEQ Answers Biostars Data Carpentry DIB Summer Institute","title":"Additional Resources:"},{"location":"#suggested-readings","text":"A global ocean atlas of eukaryotic genes - This is the paper from which we pulled all of our sample data.","title":"Suggested readings"},{"location":"2017-metaG-metaspades-assembly/","text":"Assembly with metaSPAdes Here are the instructions for assembling the metagenomic data with a different assembler. Here we are showing metaSPAdes , one of the assembler options highlighted in the CAMI paper. metaSPAdes takes longer than MEGAHIT to run the assembly, so we will not be running it during the course. Rather it is available for download here . If you are interested in running the assembly yourself, however, please see below. Install metaSPAdes: wget http://cab.spbu.ru/files/release3.10.1/SPAdes-3.10.1-Linux.tar.gz tar -xzf SPAdes-3.10.1-Linux.tar.gz export PATH=$PATH:~/SPAdes-3.10.1-Linux/bin/ Concatenate the two sets of reads: cd ~/data for x in *gz do gunzip $x done cat *fq > coassembly.fq cd ~ mkdir assembly-spades cd assembly-spades And assemble with metaSPAdes: metaspades.py --12 ~/data/coassembly.fq -o metaS-assembly","title":"Assembly with metaSPAdes"},{"location":"2017-metaG-metaspades-assembly/#assembly-with-metaspades","text":"Here are the instructions for assembling the metagenomic data with a different assembler. Here we are showing metaSPAdes , one of the assembler options highlighted in the CAMI paper. metaSPAdes takes longer than MEGAHIT to run the assembly, so we will not be running it during the course. Rather it is available for download here . If you are interested in running the assembly yourself, however, please see below. Install metaSPAdes: wget http://cab.spbu.ru/files/release3.10.1/SPAdes-3.10.1-Linux.tar.gz tar -xzf SPAdes-3.10.1-Linux.tar.gz export PATH=$PATH:~/SPAdes-3.10.1-Linux/bin/ Concatenate the two sets of reads: cd ~/data for x in *gz do gunzip $x done cat *fq > coassembly.fq cd ~ mkdir assembly-spades cd assembly-spades And assemble with metaSPAdes: metaspades.py --12 ~/data/coassembly.fq -o metaS-assembly","title":"Assembly with metaSPAdes"},{"location":"annotation/","text":"Annotating a Metatranscriptome Our assembly contains a set of contigs that represent transcripts, or fragments of transcripts. To get at the functional content of these transcripts, we must first find the most likely open reading frames (ORFs) Annotating transcriptomes and metatranscriptomes with dammit dammit! dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: Pfam-A , Rfam , OrthoDB , uniref90 (uniref is optional with --full ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al. 2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome. Installation Annotation necessarily requires a lot of software! dammit attempts to simplify this and make it as reliable as possible, and now conda makes it even easier. We've already installed dammit int eh tara environment, but if you need to install it in the future, here's the command: conda install dammit Database Preparation dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. If you just run dammit databases on its own, you should get a notification that some database tasks are not up-to-date -- we need to install them! We're going to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. In the future, running full run would take longer to install and run, but you'll have access to the full annotation pipeline. export DAMMIT_DB_DIR=/LUSTRE/bioinformatica_data/bioinformatica2018/dammit_databases dammit databases --install --busco-group eukaryota --quick Note: the dammit databases can be quite large, so make sure you have a lot of space for them. Don't put them in your home directory on a cluster, for example! We used the \"metazoa\" BUSCO group. We can use any of the BUSCO databases, so long as we install them with the dammit databases subcommand. You can see the whole list by running dammit databases -h . You should try to match your species as closely as possible for the best results. If we want to install another, for example: dammit databases --install --busco-group metazoa --quick Note: By default, dammit installs databases in the home directory. However, when you have limited space, as we do here, we can choose to install the databases in another location (e.g. /LUSTRE/bioinformatica_data/bioinformatica2018/dammit_databases ) Annotating your metatranscriptome Keep things organized! Let's make a project directory: Make sure you still have the PROJECT variable: echo $PROJECT If you don't see any output, set the PROJECT variable again: export PROJECT=~/work Now let's make a directory for annotation cd $PROJECT mkdir -p annotation cd annotation We ran megahit earlier to generate an assembly. Let's link that assembly to this directory ln -s $PROJECT/assembly/tara135_SRF_megahit.fasta ./ Make sure you run ls and see the assembly file. Just annotate it, Dammit! dammit annotate tara135_SRF_megahit.fasta --busco-group eukaryota --n_threads 6 While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make. This not only helps organize the underlying workflow, but also means that if we interrupt it, it will properly resume! After a successful run, you'll have a new directory called tara135_SRF_megahit.fasta.dammit . If you look inside, you'll see a lot of files: ls tara135_SRF_megahit.fasta.dammit/ The most important files for you are tara135_SRF_megahit.fasta.dammit.fasta , tara135_SRF_megahit.fasta.dammit.gff3 , and tara135_SRF_megahit.fasta.dammit.stats.json . If the above dammit command is run again, there will be a message: **Pipeline is already completed!**","title":"Assembly Annotation"},{"location":"annotation/#annotating-a-metatranscriptome","text":"Our assembly contains a set of contigs that represent transcripts, or fragments of transcripts. To get at the functional content of these transcripts, we must first find the most likely open reading frames (ORFs)","title":"Annotating a Metatranscriptome"},{"location":"annotation/#annotating-transcriptomes-and-metatranscriptomes-with-dammit","text":"dammit! dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: Pfam-A , Rfam , OrthoDB , uniref90 (uniref is optional with --full ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al. 2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome.","title":"Annotating transcriptomes and metatranscriptomes with dammit"},{"location":"annotation/#installation","text":"Annotation necessarily requires a lot of software! dammit attempts to simplify this and make it as reliable as possible, and now conda makes it even easier. We've already installed dammit int eh tara environment, but if you need to install it in the future, here's the command: conda install dammit","title":"Installation"},{"location":"annotation/#database-preparation","text":"dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. If you just run dammit databases on its own, you should get a notification that some database tasks are not up-to-date -- we need to install them! We're going to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. In the future, running full run would take longer to install and run, but you'll have access to the full annotation pipeline. export DAMMIT_DB_DIR=/LUSTRE/bioinformatica_data/bioinformatica2018/dammit_databases dammit databases --install --busco-group eukaryota --quick Note: the dammit databases can be quite large, so make sure you have a lot of space for them. Don't put them in your home directory on a cluster, for example! We used the \"metazoa\" BUSCO group. We can use any of the BUSCO databases, so long as we install them with the dammit databases subcommand. You can see the whole list by running dammit databases -h . You should try to match your species as closely as possible for the best results. If we want to install another, for example: dammit databases --install --busco-group metazoa --quick Note: By default, dammit installs databases in the home directory. However, when you have limited space, as we do here, we can choose to install the databases in another location (e.g. /LUSTRE/bioinformatica_data/bioinformatica2018/dammit_databases )","title":"Database Preparation"},{"location":"annotation/#annotating-your-metatranscriptome","text":"Keep things organized! Let's make a project directory: Make sure you still have the PROJECT variable: echo $PROJECT If you don't see any output, set the PROJECT variable again: export PROJECT=~/work Now let's make a directory for annotation cd $PROJECT mkdir -p annotation cd annotation We ran megahit earlier to generate an assembly. Let's link that assembly to this directory ln -s $PROJECT/assembly/tara135_SRF_megahit.fasta ./ Make sure you run ls and see the assembly file.","title":"Annotating your metatranscriptome"},{"location":"annotation/#just-annotate-it-dammit","text":"dammit annotate tara135_SRF_megahit.fasta --busco-group eukaryota --n_threads 6 While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make. This not only helps organize the underlying workflow, but also means that if we interrupt it, it will properly resume! After a successful run, you'll have a new directory called tara135_SRF_megahit.fasta.dammit . If you look inside, you'll see a lot of files: ls tara135_SRF_megahit.fasta.dammit/ The most important files for you are tara135_SRF_megahit.fasta.dammit.fasta , tara135_SRF_megahit.fasta.dammit.gff3 , and tara135_SRF_megahit.fasta.dammit.stats.json . If the above dammit command is run again, there will be a message: **Pipeline is already completed!**","title":"Just annotate it, Dammit!"},{"location":"anvio/","text":"Using Anvi'o to Knit Everything Together Now we are going bring it all together and visuzlize our assembly with Anvi'o . Anvi'o is a powerful and extensible tool that might be easily applied to pan-genomic analysis as well as metagenomic analysis. The anvi'o group has a series of fantstic online tutorials, including one on metagenomic analysis . They also run workshops periodically ( schedule here ) that throughly cover the use of the software. Today, we are adapting their tutorial on metagenomic analysis to work with the subset dataset that we have. The goals of this tutorial are to: Install anvi'o Become familiar with the anvi'o workflow Visualizing the assembly with anvi'o Become familiar with the anvi'o interface * Learn how to refine and visuzlize genome bins with anvi'o Installing anvi'o (and a few other programs) The first thing we need to do is install anvi'o. To install anvi'o we will be be using Anaconda . cd ~ wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.sh bash Anaconda3-4.4.0-Linux-x86_64.sh Now, follow the prompts for the Anaconda installation. To finish the install it will ask you if you would like to add anaconda to the $PATH in you .bashrc . You should say yes. Now, you just need to source your .bashrc to make sure you can use conda. source .bashrc Anaconda should now be installed. We will now use anaconda ( conda ) to install anvi'o (and all its dependencies) as well as source an environment in which to to run conda. Now, install anvi'o using conda, create an environment in which to run it, and source the environment: conda create -n anvio232 -c bioconda -c conda-forge gsl anvio=2.3.2 source activate anvio232 Anvi'o should now be installed. But, let's double check that it worked. They have a nice little test case to check that everything is working well as follows: anvi-self-test --suite mini This prompt will start anvi'o processing and ultimately it will generate an interactive window with the anvi'o environment. This is accessible through port 8080 (typically, though it might create go to a different port that will be specified) at your ec2 machine address. Now, open a new tab in your browser (NOTE: This only works in Google Chrome) and paste in the following: [Your EC Address]:8080 This should open up the anvi'o interface which is interactive and pretty good looking. Now, we just need to install a few other programs, namely, samtools and Bowtie2, which we will use for mapping and looking at our mapped data. wget https://downloads.sourceforge.net/project/bowtie-bio/bowtie2/2.3.2/bowtie2-2.3.2-linux-x86_64.zip unzip bowtie2-2.3.2-linux-x86_64.zip echo 'export PATH=$PATH:~/bowtie2-2.3.2' >> ~/.bashrc source ~/.bashrc sudo apt-get -y install samtools Alright, now onto a complete re-analysis of our data with the anvi'o pipeline. Getting it into Anvi'o format Anvi'o takes in 1) an assembly and 2) the raw read data. We have both of those already created, so let's go ahead and download those data (trimmed reads and asssemblies): mkdir ~/anvio-work cd ~/anvio-work curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/SRR1976948.abundtrim.subset.pe.fq.gz curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/SRR1977249.abundtrim.subset.pe.fq.gz curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/subset_assembly.fa.gz And, gunzip those files: for file in *gz do gunzip $file done We now need to get our assembly into the correct format so that anvi'o interpret it. anvi-script-reformat-fasta subset_assembly.fa -o anvio-contigs.fa --min-len 2000 --simplify-names --report name_conversions.txt Take a look at the output files. What has changed? Mapping data We need to map our reads to our anvi'o corrected assembly. This is going to take a little bit of time. First, build the an index for bowtie2: bowtie2-build anvio-contigs.fa anvio-contigs We can write a for loop to map our two datasets and produce .bam files for the files: for file in *fq do bowtie2 --threads 8 -x anvio-contigs --interleaved $file -S ${file/.fq/}.sam samtools view -U 4 -bS ${file/.fq/}.sam > ${file/.fq/}.bam done As above, we need to make these data readable for anvi'o: for file in *.bam do anvi-init-bam ${file} -o ${file/.bam/}.anvio.bam done Generating contigs database In this step we are asking anvi'o to create a database with information about your contigs. The contig database is fairly extensible and can contain lots of different information (taxonomic, functional, etc.). Here, we are primarily asking it to do three things: 1) 'Soft split' long contigs (>20k): Anvi'o shows the generalized statistics for each contig (GC content, etc.). For long contigs these stats are calculated across split contigs (which remain grouped) 2) Identify and locate open reading frames in your contigs (using Prodigal) 3) Estimate Single Copy Gene content (using hmmer against defined gene sets for bacteria and archaea ) 3) Calculate k-mer frequencies for the contigs in our assemblies So, run the following command to generate the database: anvi-gen-contigs-database -f anvio-contigs.fa -o anvio-contigs.db Then, run this command to perform the hmm search and identify single copy gene content: anvi-run-hmms -c anvio-contigs.db --num-threads 28 Now, we can layer on the coverge information from our two samples: for file in *.anvio.bam do anvi-profile -i $file -c anvio-contigs.db -T 28 done And finally, we run the merge step. This will pull all the information together and create a merged anvi'o profile. This step will also run CONCOCT ( another binning algorithm) that will identify bins in our data. Finally, this step calculates the hierarchical relationship betwewen our contigs based on a variety of parameters. anvi-merge *ANVIO_PROFILE/PROFILE.db -o MERGED-SAMPLES -c anvio-contigs.db --enforce-hierarchical-clustering Now we can visualize our data! anvi-interactive -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db Identifying and refining genome bins First, let's summarize the bin information for our data. This will produce a series of text-based output files detailing some statistics on our genome bins: anvi-summarize -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -o SAMPLES-SUMMARY -C CONCOCT Take a look at the output in SAMPLES-SUMMARY . What does it report? Now you can visualize those data in the anvi'o style by simply adding the -C flag to the previous anvi-interactive command: anvi-interactive -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -C CONCOCT Now, we can actually refine the genome bins using anvi'o. This allows us to use human intuition and pattern recognition to better identify contigs that should co-occur. It is important that we make a copy of the original data so that we don't accidentally overwrite it. So make a copy of the directory: cp -avr SAMPLES-SUMMARY/ SAMPLES-SUMMARY-ORIGININAL/ Now, let's refine a bin! Let's start with Bin_4. anvi-refine -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -b Bin_4 -C CONCOCT Finally, it is time to interact with the anvi'o interface. Here are some screenshots to help guide you in your quest. And of course a big thank you to Meren for providing us with extra materials to help create this tutorial!","title":"Using Anvi'o to Knit Everything Together"},{"location":"anvio/#using-anvio-to-knit-everything-together","text":"Now we are going bring it all together and visuzlize our assembly with Anvi'o . Anvi'o is a powerful and extensible tool that might be easily applied to pan-genomic analysis as well as metagenomic analysis. The anvi'o group has a series of fantstic online tutorials, including one on metagenomic analysis . They also run workshops periodically ( schedule here ) that throughly cover the use of the software. Today, we are adapting their tutorial on metagenomic analysis to work with the subset dataset that we have. The goals of this tutorial are to: Install anvi'o Become familiar with the anvi'o workflow Visualizing the assembly with anvi'o Become familiar with the anvi'o interface * Learn how to refine and visuzlize genome bins with anvi'o","title":"Using Anvi'o to Knit Everything Together"},{"location":"anvio/#installing-anvio-and-a-few-other-programs","text":"The first thing we need to do is install anvi'o. To install anvi'o we will be be using Anaconda . cd ~ wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.sh bash Anaconda3-4.4.0-Linux-x86_64.sh Now, follow the prompts for the Anaconda installation. To finish the install it will ask you if you would like to add anaconda to the $PATH in you .bashrc . You should say yes. Now, you just need to source your .bashrc to make sure you can use conda. source .bashrc Anaconda should now be installed. We will now use anaconda ( conda ) to install anvi'o (and all its dependencies) as well as source an environment in which to to run conda. Now, install anvi'o using conda, create an environment in which to run it, and source the environment: conda create -n anvio232 -c bioconda -c conda-forge gsl anvio=2.3.2 source activate anvio232 Anvi'o should now be installed. But, let's double check that it worked. They have a nice little test case to check that everything is working well as follows: anvi-self-test --suite mini This prompt will start anvi'o processing and ultimately it will generate an interactive window with the anvi'o environment. This is accessible through port 8080 (typically, though it might create go to a different port that will be specified) at your ec2 machine address. Now, open a new tab in your browser (NOTE: This only works in Google Chrome) and paste in the following: [Your EC Address]:8080 This should open up the anvi'o interface which is interactive and pretty good looking. Now, we just need to install a few other programs, namely, samtools and Bowtie2, which we will use for mapping and looking at our mapped data. wget https://downloads.sourceforge.net/project/bowtie-bio/bowtie2/2.3.2/bowtie2-2.3.2-linux-x86_64.zip unzip bowtie2-2.3.2-linux-x86_64.zip echo 'export PATH=$PATH:~/bowtie2-2.3.2' >> ~/.bashrc source ~/.bashrc sudo apt-get -y install samtools Alright, now onto a complete re-analysis of our data with the anvi'o pipeline.","title":"Installing anvi'o (and a few other programs)"},{"location":"anvio/#getting-it-into-anvio-format","text":"Anvi'o takes in 1) an assembly and 2) the raw read data. We have both of those already created, so let's go ahead and download those data (trimmed reads and asssemblies): mkdir ~/anvio-work cd ~/anvio-work curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/SRR1976948.abundtrim.subset.pe.fq.gz curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/SRR1977249.abundtrim.subset.pe.fq.gz curl -O https://s3-us-west-1.amazonaws.com/dib-training.ucdavis.edu/metagenomics-scripps-2016-10-12/subset_assembly.fa.gz And, gunzip those files: for file in *gz do gunzip $file done We now need to get our assembly into the correct format so that anvi'o interpret it. anvi-script-reformat-fasta subset_assembly.fa -o anvio-contigs.fa --min-len 2000 --simplify-names --report name_conversions.txt Take a look at the output files. What has changed?","title":"Getting it into Anvi'o format"},{"location":"anvio/#mapping-data","text":"We need to map our reads to our anvi'o corrected assembly. This is going to take a little bit of time. First, build the an index for bowtie2: bowtie2-build anvio-contigs.fa anvio-contigs We can write a for loop to map our two datasets and produce .bam files for the files: for file in *fq do bowtie2 --threads 8 -x anvio-contigs --interleaved $file -S ${file/.fq/}.sam samtools view -U 4 -bS ${file/.fq/}.sam > ${file/.fq/}.bam done As above, we need to make these data readable for anvi'o: for file in *.bam do anvi-init-bam ${file} -o ${file/.bam/}.anvio.bam done","title":"Mapping data"},{"location":"anvio/#generating-contigs-database","text":"In this step we are asking anvi'o to create a database with information about your contigs. The contig database is fairly extensible and can contain lots of different information (taxonomic, functional, etc.). Here, we are primarily asking it to do three things: 1) 'Soft split' long contigs (>20k): Anvi'o shows the generalized statistics for each contig (GC content, etc.). For long contigs these stats are calculated across split contigs (which remain grouped) 2) Identify and locate open reading frames in your contigs (using Prodigal) 3) Estimate Single Copy Gene content (using hmmer against defined gene sets for bacteria and archaea ) 3) Calculate k-mer frequencies for the contigs in our assemblies So, run the following command to generate the database: anvi-gen-contigs-database -f anvio-contigs.fa -o anvio-contigs.db Then, run this command to perform the hmm search and identify single copy gene content: anvi-run-hmms -c anvio-contigs.db --num-threads 28 Now, we can layer on the coverge information from our two samples: for file in *.anvio.bam do anvi-profile -i $file -c anvio-contigs.db -T 28 done And finally, we run the merge step. This will pull all the information together and create a merged anvi'o profile. This step will also run CONCOCT ( another binning algorithm) that will identify bins in our data. Finally, this step calculates the hierarchical relationship betwewen our contigs based on a variety of parameters. anvi-merge *ANVIO_PROFILE/PROFILE.db -o MERGED-SAMPLES -c anvio-contigs.db --enforce-hierarchical-clustering Now we can visualize our data! anvi-interactive -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db","title":"Generating contigs database"},{"location":"anvio/#identifying-and-refining-genome-bins","text":"First, let's summarize the bin information for our data. This will produce a series of text-based output files detailing some statistics on our genome bins: anvi-summarize -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -o SAMPLES-SUMMARY -C CONCOCT Take a look at the output in SAMPLES-SUMMARY . What does it report? Now you can visualize those data in the anvi'o style by simply adding the -C flag to the previous anvi-interactive command: anvi-interactive -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -C CONCOCT Now, we can actually refine the genome bins using anvi'o. This allows us to use human intuition and pattern recognition to better identify contigs that should co-occur. It is important that we make a copy of the original data so that we don't accidentally overwrite it. So make a copy of the directory: cp -avr SAMPLES-SUMMARY/ SAMPLES-SUMMARY-ORIGININAL/ Now, let's refine a bin! Let's start with Bin_4. anvi-refine -p MERGED-SAMPLES/PROFILE.db -c anvio-contigs.db -b Bin_4 -C CONCOCT Finally, it is time to interact with the anvi'o interface. Here are some screenshots to help guide you in your quest. And of course a big thank you to Meren for providing us with extra materials to help create this tutorial!","title":"Identifying and refining genome bins"},{"location":"automation-qc/","text":"Automation via shell scripts and snakemake For everything we have done so far, we have copied and pasted a lot of commands to accomplish what we want. This works! But can also be time consuming, and is more prone to error. We will show you next how to put all of these commands into a shell script. A shell script is a text file full of shell commands, that run just as if you're running them interactively at the command line. Here, we'll create one that takes the commands from the and runs 'em all at once. Writing a shell script Let's put all of our commands from the quality control lesson into one script. We'll call it run_qc.sh . The sh at the end of the tells you that this is a bash script. Using nano, edit the file run-qc.sh and put the following content there: cd ${PROJECT} mkdir -p quality cd quality ln -s ../data/*.fq.gz ./ printf \"I see $(ls -1 *.fq.gz | wc -l) files here.\\n\" fastqc *.fq.gz multiqc . This is now a shell script that you can use to execute all of those commands in one go -- try it out! Run: cd ~/ bash run-qc.sh Re-running the shell script Suppose you wanted to re-run the script. How would you do that? Well, note that the quality directory is created at the top of the script, and everything is executed in that directory. So if you remove the quality directory like so, rm -fr quality you can then do bash run-qc.sh Some tricks for writing shell scripts Make it executable You can get rid of the bash part of the command above with some magic: Put #! /bin/bash at the top of the file, and then run chmod +x ~/run-qc.sh at the command line. You can now run ./run-qc.sh instead of bash run-rnaseq.sh . You might be thinking, ok, why is this important? Well, you can do the same with R scripts and Python scripts (but put /usr/bin/env Rscript or /usr/bin/env python at the top, instead of /bin/bash ). This basically annotates the script with the language it's written in, so you don't have to know or remember yourself. So: it's not necessary but it's a nice trick. You can also always force a script to be run in a particular language by specifying bash <scriptname> or Rscript <Scriptname> , too. Another good addition: make it fail nicely One sort of weird aspect of shell scripts is that (by default) they keep running even if there's an error. This is bad behavior and we should turn it off. You can observe this behavior by rerunning the script above without deleting the directory rnaseq/ - the mkdir command will print an error because the directory still exists, but A good addition to every shell script is to make it fail on the first error. Do this by putting set -e at the top - this tells bash to exit at the first error, rather than continuing bravely onwards. A final good addition: make shell scripts print out the commands they're running! You might notice that the shell script gives you the output of the commands it's running, but doesn't tell you which commands it's running. If you add set -x at the top of the shell script and then re-run it, cd ~/ rm -fr quality ./run-qc.sh then you'll see the full set of commands being run! A final note on shell scripts: set -e and set -x only work in shell scripts - they're bash commands. You need to use other approaches in Python and R. Automation with Snakemake! Automation via shell script is wonderful, but there are a few problems here. First, you have to run the entire workflow each time and it recomputes everything every time. If you're running a workflow that takes 4 days, and you change a command at the end, you'll have to manually go in and just run the stuff that depends on the changed command. Second, it's very explicit and not very generalizable . If you want to run it on a different RNAseq data set, you're going to have to change a lot of commands. snakemake is one of several workflow systems that help solve these problems. (You can read the documentation here.) Let's take a look! First, let's activate our snakemake environment source deactivate source activate snake We're going to automate the same script for trimming, but in snakemake. rule all: input: \"trim/TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz\", \"trim/TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz\" rule trim_reads: input: r1=\"data/TARA_135_SRF_5-20_rep1_1m_1.fq.gz\", r2=\"data/TARA_135_SRF_5-20_rep1_1m_2.fq.gz\", adapters=\"trim/combined.fa\" ouput: p1=\"trim/TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz\", p2=\"trim/TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz\", s1=\"trim/TARA_135_SRF_5-20_rep1_1m_1_s1.qc.fq.gz\", s2=\"trim/TARA_135_SRF_5-20_rep1_1m_2_s2.qc.fq.gz\" shell:''' trimmomatic PE {input.r1} \\ {input.r2} \\ {output.p1} {output.s1} \\ {output.p2} {output.s2} \\ ILLUMINACLIP:{input.adapters}:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25 ''' Now we can run this cd $PROJECT snakemake you should see \"Nothing to be done.\" That's because the trimmed files already exist! Let's fix that: rm trim/TARA_135_SRF_5-20_rep1* and now, when you run snakemake , you should see the Trimmomatic being run. Yay w00t! Then if you run snakemake again, you will see that it doesn't need to do anything - all the files are \"up to date\". Adding and environment We've been using conda environments throughout our workshop. We showed you have to export your tara environment in the bioconda lesson using conda env export -n tara -f $PROJECT/tara_conda_environment.yaml . We can use this environment in our snakemake rule as well! rule all: input: \"trim/TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz\", \"trim/TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz\" rule trim_reads: input: r1=\"data/TARA_135_SRF_5-20_rep1_1m_1.fq.gz\", r2=\"data/TARA_135_SRF_5-20_rep1_1m_2.fq.gz\", adapters=\"trim/combined.fa\" ouput: p1=\"trim/TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz\", p2=\"trim/TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz\", s1=\"trim/TARA_135_SRF_5-20_rep1_1m_1_s1.qc.fq.gz\", s2=\"trim/TARA_135_SRF_5-20_rep1_1m_2_s2.qc.fq.gz\" conda: \"tara_conda_environment.yaml\" shell:''' trimmomatic PE {input.r1} \\ {input.r2} \\ {output.p1} {output.s1} \\ {output.p2} {output.s2} \\ ILLUMINACLIP:{input.adapters}:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25 ''' We aren't going to run this on the cluster right now, because it requires you to be able to download things and we can't do that on nodo. However, this is the syntax to do this in the future. Other resources We've covered some basics of snakemake today, but if you want another tutorial, we've add one here .","title":"Workflows and Repeatability"},{"location":"automation-qc/#automation-via-shell-scripts-and-snakemake","text":"For everything we have done so far, we have copied and pasted a lot of commands to accomplish what we want. This works! But can also be time consuming, and is more prone to error. We will show you next how to put all of these commands into a shell script. A shell script is a text file full of shell commands, that run just as if you're running them interactively at the command line. Here, we'll create one that takes the commands from the and runs 'em all at once.","title":"Automation via shell scripts and snakemake"},{"location":"automation-qc/#writing-a-shell-script","text":"Let's put all of our commands from the quality control lesson into one script. We'll call it run_qc.sh . The sh at the end of the tells you that this is a bash script. Using nano, edit the file run-qc.sh and put the following content there: cd ${PROJECT} mkdir -p quality cd quality ln -s ../data/*.fq.gz ./ printf \"I see $(ls -1 *.fq.gz | wc -l) files here.\\n\" fastqc *.fq.gz multiqc . This is now a shell script that you can use to execute all of those commands in one go -- try it out! Run: cd ~/ bash run-qc.sh","title":"Writing a shell script"},{"location":"automation-qc/#re-running-the-shell-script","text":"Suppose you wanted to re-run the script. How would you do that? Well, note that the quality directory is created at the top of the script, and everything is executed in that directory. So if you remove the quality directory like so, rm -fr quality you can then do bash run-qc.sh","title":"Re-running the shell script"},{"location":"automation-qc/#some-tricks-for-writing-shell-scripts","text":"","title":"Some tricks for writing shell scripts"},{"location":"automation-qc/#make-it-executable","text":"You can get rid of the bash part of the command above with some magic: Put #! /bin/bash at the top of the file, and then run chmod +x ~/run-qc.sh at the command line. You can now run ./run-qc.sh instead of bash run-rnaseq.sh . You might be thinking, ok, why is this important? Well, you can do the same with R scripts and Python scripts (but put /usr/bin/env Rscript or /usr/bin/env python at the top, instead of /bin/bash ). This basically annotates the script with the language it's written in, so you don't have to know or remember yourself. So: it's not necessary but it's a nice trick. You can also always force a script to be run in a particular language by specifying bash <scriptname> or Rscript <Scriptname> , too.","title":"Make it executable"},{"location":"automation-qc/#another-good-addition-make-it-fail-nicely","text":"One sort of weird aspect of shell scripts is that (by default) they keep running even if there's an error. This is bad behavior and we should turn it off. You can observe this behavior by rerunning the script above without deleting the directory rnaseq/ - the mkdir command will print an error because the directory still exists, but A good addition to every shell script is to make it fail on the first error. Do this by putting set -e at the top - this tells bash to exit at the first error, rather than continuing bravely onwards.","title":"Another good addition: make it fail nicely"},{"location":"automation-qc/#a-final-good-addition-make-shell-scripts-print-out-the-commands-theyre-running","text":"You might notice that the shell script gives you the output of the commands it's running, but doesn't tell you which commands it's running. If you add set -x at the top of the shell script and then re-run it, cd ~/ rm -fr quality ./run-qc.sh then you'll see the full set of commands being run!","title":"A final good addition: make shell scripts print out the commands they're running!"},{"location":"automation-qc/#a-final-note-on-shell-scripts","text":"set -e and set -x only work in shell scripts - they're bash commands. You need to use other approaches in Python and R.","title":"A final note on shell scripts:"},{"location":"automation-qc/#automation-with-snakemake","text":"Automation via shell script is wonderful, but there are a few problems here. First, you have to run the entire workflow each time and it recomputes everything every time. If you're running a workflow that takes 4 days, and you change a command at the end, you'll have to manually go in and just run the stuff that depends on the changed command. Second, it's very explicit and not very generalizable . If you want to run it on a different RNAseq data set, you're going to have to change a lot of commands. snakemake is one of several workflow systems that help solve these problems. (You can read the documentation here.) Let's take a look! First, let's activate our snakemake environment source deactivate source activate snake We're going to automate the same script for trimming, but in snakemake. rule all: input: \"trim/TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz\", \"trim/TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz\" rule trim_reads: input: r1=\"data/TARA_135_SRF_5-20_rep1_1m_1.fq.gz\", r2=\"data/TARA_135_SRF_5-20_rep1_1m_2.fq.gz\", adapters=\"trim/combined.fa\" ouput: p1=\"trim/TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz\", p2=\"trim/TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz\", s1=\"trim/TARA_135_SRF_5-20_rep1_1m_1_s1.qc.fq.gz\", s2=\"trim/TARA_135_SRF_5-20_rep1_1m_2_s2.qc.fq.gz\" shell:''' trimmomatic PE {input.r1} \\ {input.r2} \\ {output.p1} {output.s1} \\ {output.p2} {output.s2} \\ ILLUMINACLIP:{input.adapters}:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25 ''' Now we can run this cd $PROJECT snakemake you should see \"Nothing to be done.\" That's because the trimmed files already exist! Let's fix that: rm trim/TARA_135_SRF_5-20_rep1* and now, when you run snakemake , you should see the Trimmomatic being run. Yay w00t! Then if you run snakemake again, you will see that it doesn't need to do anything - all the files are \"up to date\".","title":"Automation with Snakemake!"},{"location":"automation-qc/#adding-and-environment","text":"We've been using conda environments throughout our workshop. We showed you have to export your tara environment in the bioconda lesson using conda env export -n tara -f $PROJECT/tara_conda_environment.yaml . We can use this environment in our snakemake rule as well! rule all: input: \"trim/TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz\", \"trim/TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz\" rule trim_reads: input: r1=\"data/TARA_135_SRF_5-20_rep1_1m_1.fq.gz\", r2=\"data/TARA_135_SRF_5-20_rep1_1m_2.fq.gz\", adapters=\"trim/combined.fa\" ouput: p1=\"trim/TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz\", p2=\"trim/TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz\", s1=\"trim/TARA_135_SRF_5-20_rep1_1m_1_s1.qc.fq.gz\", s2=\"trim/TARA_135_SRF_5-20_rep1_1m_2_s2.qc.fq.gz\" conda: \"tara_conda_environment.yaml\" shell:''' trimmomatic PE {input.r1} \\ {input.r2} \\ {output.p1} {output.s1} \\ {output.p2} {output.s2} \\ ILLUMINACLIP:{input.adapters}:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25 ''' We aren't going to run this on the cluster right now, because it requires you to be able to download things and we can't do that on nodo. However, this is the syntax to do this in the future.","title":"Adding and environment"},{"location":"automation-qc/#other-resources","text":"We've covered some basics of snakemake today, but if you want another tutorial, we've add one here .","title":"Other resources"},{"location":"automation/","text":"Automation via shell scripts and snakemake Note: This lesson was written as part of DIBSI 2018 by C. Titus Brown. Links to the data are all provided, so you should be able to follow along. Learning objectives: understand how to write shell scripts understand the differences between shell scripts and workflow engines develop a first snakemake workflow. We're going to reprise the RNAseq differential expression, but we're going to automate the heck out of it. Clean off old stuff cd ~/ rm -fr data rnaseq Install the necessary software and data You'll need to have conda and RStudio installed (see installation instructions ). Then install salmon and edgeR: cd ~/ conda install -y salmon curl -L -O https://raw.githubusercontent.com/ngs-docs/angus/2018/scripts/install-edgeR.R sudo Rscript --no-save install-edgeR.R You'll also need the yeast data from trimming again: cd ~/ mkdir -p data cd data curl -L https://osf.io/5daup/download -o ERR458493.fastq.gz curl -L https://osf.io/8rvh5/download -o ERR458494.fastq.gz curl -L https://osf.io/2wvn3/download -o ERR458495.fastq.gz curl -L https://osf.io/xju4a/download -o ERR458500.fastq.gz curl -L https://osf.io/nmqe6/download -o ERR458501.fastq.gz curl -L https://osf.io/qfsze/download -o ERR458502.fastq.gz And, finally, download the necessary analysis code into your home directory cd ~/ curl -L -O https://raw.githubusercontent.com/ngs-docs/angus/2018/scripts/gather-counts.py curl -L -O https://raw.githubusercontent.com/ngs-docs/angus/2018/scripts/yeast.salmon.R Write a shell script to run RNAseq analysis A shell script is a text file full of shell commands, that run just as if you're running them interactively at the command line. Here, we'll create one that takes the commands from the RNAseq differential expression tutorial and runs 'em all at once. Put all of the RNAseq commands into a single text file We'll script everything but the install and data download part of the RNAseq differential expression tutorial . Use RStudio to create a new file named run-rnaseq.sh in your home directory on the Jetstream computer, and write the following commands into it: # make an rnaseq directory and change into it mkdir rnaseq cd rnaseq # link the data in from the other directory ln -fs ~/data/*.fastq.gz . # download the reference transcriptome curl -O https://downloads.yeastgenome.org/sequence/S288C_reference/orf_dna/orf_coding.fasta.gz # index the reference transcriptome salmon index --index yeast_orfs --type quasi --transcripts orf_coding.fasta.gz # run salmon quant on each of the input read data sets for i in *.fastq.gz do salmon quant -i yeast_orfs --libType U -r $i -o $i.quant --seqBias --gcBias done # collect the counts python2 ~/gather-counts.py # run the edgeR script! Rscript --no-save ~/yeast.salmon.R This is now a shell script that you can use to execute all of those commands in one go -- try it out! Run: cd ~/ bash run-rnaseq.sh You should now have a directory rnaseq/ containing a bunch of files, including the PDF outputs of the RNAseq pipeline, yeast-edgeR-MA-plot.pdf , yeast-edgeR-MDS.pdf , and yeast-edgeR.csv . Pretty cool, eh? Re-running the shell script Suppose you wanted to re-run the script. How would you do that? Well, note that the rnaseq directory is created at the top of the script, and everything is executed in that directory. So if you remove the rnaseq directory like so, rm -fr rnaseq you can then do bash run-rnaseq.sh to just ...rerun everything. A trick: make it executable You can get rid of the bash part of the command above with some magic: Put #! /bin/bash at the top of the file, and then run chmod +x ~/run-rnaseq.sh at the command line. You can now run ./run-rnaseq.sh instead of bash run-rnaseq.sh . You might be thinking, ok, why is this important? Well, you can do the same with R scripts and Python scripts (but put /usr/bin/env Rscript or /usr/bin/env python at the top, instead of /bin/bash ). This basically annotates the script with the language it's written in, so you don't have to know or remember yourself. So: it's not necessary but it's a nice trick. You can also always force a script to be run in a particular language by specifying bash <scriptname> or Rscript <Scriptname> , too. Another good addition: make it fail nicely One sort of weird aspect of shell scripts is that (by default) they keep running even if there's an error. This is bad behavior and we should turn it off. You can observe this behavior by rerunning the script above without deleting the directory rnaseq/ - the mkdir command will print an error because the directory still exists, but A good addition to every shell script is to make it fail on the first error. Do this by putting set -e at the top - this tells bash to exit at the first error, rather than continuing bravely onwards. A final good addition: make shell scripts print out the commands they're running! You might notice that the shell script gives you the output of the commands it's running, but doesn't tell you which commands it's running. If you add set -x at the top of the shell script and then re-run it, cd ~/ rm -fr rnaseq ./run-rnaseq.sh then you'll see the full set of commands being run! Tracking output of shell scripts you can do: exec > output.log exec 2> error.log to save all output from that point on to those files. Or, you can do: nohup ./run-rnaseq.sh > output.log 2> error.log & and that will do the same thing, while putting it in the background. You can also run 'screen' or 'tmux' (google it). Torsten recommends: script session.log While you're at it, you should look up nohup. A final note on shell scripts: set -e and set -x only work in shell scripts - they're bash commands. You need to use other approaches in Python and R. snakemake for automation Automation via shell script is wonderful, but there are a few problems here. First, you have to run the entire workflow each time and it recomputes everything every time. If you're running a workflow that takes 4 days, and you change a command at the end, you'll have to manually go in and just run the stuff that depends on the changed command. Second, it's very explicit and not very generalizable . If you want to run it on a different RNAseq data set, you're going to have to change a lot of commands. snakemake is one of several workflow systems that help solve these problems. (You can read the documentation here.) Let's take a look! First, install snakemake. Run: conda install -y snakemake Writing your first snakemake rule We're going to automate the R script at the end of the shell script in snakemake. So, run the rnaseq pipeline: cd ~/ rm -fr rnaseq ./run-rnaseq.sh Now, using RStudio, create a new text file, and put the following text in it: rule make_plots: input: output: \"yeast-edgeR-MA-plot.pdf\", \"yeast-edgeR-MDS.pdf\", \"yeast-edgeR.csv\" shell: \"Rscript --no-save ~/yeast.salmon.R\" This rule says: to make the list of outputs (the PDFs and CSV file), you need the list of inputs (currently lbank) * and then you run the shell command. Save this with the name Snakefile in the rnaseq/ directory, and run snakemake make_plots : cd ~/rnaseq snakemake make_plots you should see \"Nothing to be done.\" That's because the PDFs and CSV file already exist! Let's fix that: rm yeast-*.pdf yeast-*.csv and now, when you run snakemake make_plots , you should see the R script being run. Yay w00t! Then if you run snakemake make_plots again, you will see that it doesn't need to do anything - all the files are \"up to date\". Challenge: add the explicit list of inputs. What does the make_plots rule take as input? List them! Hint: what does the yeast.salmon.R script take in? Once you're done, you can also update the timestamp on the counts files and it will regenerate the PDFs -- touch *.counts snakemake make_plots because it knows that the files the rule depends on have changed! Challenge: write a rule that generates the .counts files. Looking at the shell script, write a new Snakemake rule that generates the counts files as outputs. You can start by adding a new blank rule: rule make_counts: input: output: shell: \"\" and filling in from there. Note that rule order does not matter in the Snakefile! Hints: you should list each .counts file separately. To test the rule, do rm *.counts yeast-*.pdf and it will regenerate everything needed to build the PDF! Writing rules with wildcards Let's suppose we want to write a rule that takes a FASTQ file (say, ERR458501.fastq.gz ) and runs salmon on it to generate the directory ERR458501.fastq.gz.quant . How would we do that? We could write the rule as follows: rule generate_quant_dir_ERR458501: input: \"ERR458501.fastq.gz\" output: directory(\"ERR458501.fastq.gz.quant\") shell: \"salmon quant -i yeast_orfs --libType U -r ERR458501.fastq.gz -o ERR458501.fastq.gz.quant --seqBias --gcBias\" and that will work just fine -- you can run, rm -fr ERR458501.fastq.gz.quant snakemake ERR458501.fastq.gz.quant and it will happily run salmon for you. But there's a lot of repeated stuff in there. Can we do cool computer things to fix that? Yes! We can use wildcards! Try replacing ERR458501 with {accession} - this makes it into a wildcard that snakemake can automatically fill in: rule generate_quant_dir: input: \"{accession}.fastq.gz\" output: directory(\"{accession}.fastq.gz.quant\") shell: \"salmon quant -i yeast_orfs --libType U -r {wildcards.accession}.fastq.gz -o {wildcards.accession}.fastq.gz.quant --seqBias --gcBias\" Now, you can run snakemake ERR458501.fastq.gz.quant and it will work - but you can also run snakemake ERR458493.fastq.gz.quant and it will do the right thing there, as well. Using 'expand' to work with lists of files The challenge to write a rule that generates the .counts files should have left you with a list of six input files ( .quant directories), and make_plots has a very similar of six input files ( .quant.counts ). Can we make this more succinct? Yep. You can have a list that you expand in various ways. First, define the list of files at the top of the Snakefile: input_files=[\"ERR458493.fastq.gz\", \"ERR458494.fastq.gz\", \"ERR458495.fastq.gz\", \"ERR458500.fastq.gz\", \"ERR458501.fastq.gz\", \"ERR458502.fastq.gz\"] now, modify the make_plots rule to have the input be: rule make_plots: input: expand(\"{input_files}.quant.counts\", input_files=input_files) Challenge: Update the other rules that have lists of six files using expand . Visualizing workflow diagrams Try running: snakemake --dag yeast-edgeR.csv | dot -Tpng > dag.png Challenge: Extend the snakemake rules to download and prepare the reference 'nuff said' Other things snakemake can do You can specify what programs your pipeline depends in on your snakemake setup! snakemake interfaces well with conda (in fact, the authors of snakemake are also leads on the bioconda project) so you can pretty much just list your packages. Reproducibility and provenance If you write all your workflows like this, all you need to do is give readers your data files, your Snakefile, and your list of software... and they can reproduce everything you did! Answer to first challenge: rule make_plots: input: \"ERR458493.fastq.gz.quant.counts\", \"ERR458500.fastq.gz.quant.counts\", \"ERR458494.fastq.gz.quant.counts\", \"ERR458501.fastq.gz.quant.counts\", \"ERR458495.fastq.gz.quant.counts\", \"ERR458502.fastq.gz.quant.counts\" output: \"yeast-edgeR-MA-plot.pdf\", \"yeast-edgeR-MDS.pdf\", \"yeast-edgeR.csv\" shell: \"Rscript --no-save ~/yeast.salmon.R\" Answer to second challenge # make the .counts files rule make_counts: input: \"ERR458493.fastq.gz.quant\", \"ERR458500.fastq.gz.quant\", \"ERR458494.fastq.gz.quant\", \"ERR458501.fastq.gz.quant\", \"ERR458495.fastq.gz.quant\", \"ERR458502.fastq.gz.quant\" output: \"ERR458493.fastq.gz.quant.counts\", \"ERR458500.fastq.gz.quant.counts\", \"ERR458494.fastq.gz.quant.counts\", \"ERR458501.fastq.gz.quant.counts\", \"ERR458495.fastq.gz.quant.counts\", \"ERR458502.fastq.gz.quant.counts\" shell: \"python2 ~/gather-counts.py\" Final snakemake file input_files=[\"ERR458493.fastq.gz\", \"ERR458494.fastq.gz\", \"ERR458495.fastq.gz\", \"ERR458500.fastq.gz\", \"ERR458501.fastq.gz\", \"ERR458502.fastq.gz\"] rule make_plots: input: expand(\"{input_files}.quant.counts\", input_files=input_files) output: \"yeast-edgeR-MA-plot.pdf\", \"yeast-edgeR-MDS.pdf\", \"yeast-edgeR.csv\" shell: \"Rscript --no-save ~/yeast.salmon.R\" # make the .counts files rule make_counts: input: expand(\"{input_files}.quant\", input_files=input_files) output: expand(\"{input_files}.quant.counts\", input_files=input_files) shell: \"python2 ~/gather-counts.py\" rule generate_quant_dir: input: \"{accession}.fastq.gz\" output: \"{accession}.fastq.gz.quant\" shell: \"salmon quant -i yeast_orfs --libType U -r {wildcards.accession}.fastq.gz -o {wildcards.accession}.fastq.gz.quant --seqBias --gcBias\"","title":"Automation via shell scripts and snakemake"},{"location":"automation/#automation-via-shell-scripts-and-snakemake","text":"Note: This lesson was written as part of DIBSI 2018 by C. Titus Brown. Links to the data are all provided, so you should be able to follow along. Learning objectives: understand how to write shell scripts understand the differences between shell scripts and workflow engines develop a first snakemake workflow. We're going to reprise the RNAseq differential expression, but we're going to automate the heck out of it.","title":"Automation via shell scripts and snakemake"},{"location":"automation/#clean-off-old-stuff","text":"cd ~/ rm -fr data rnaseq","title":"Clean off old stuff"},{"location":"automation/#install-the-necessary-software-and-data","text":"You'll need to have conda and RStudio installed (see installation instructions ). Then install salmon and edgeR: cd ~/ conda install -y salmon curl -L -O https://raw.githubusercontent.com/ngs-docs/angus/2018/scripts/install-edgeR.R sudo Rscript --no-save install-edgeR.R You'll also need the yeast data from trimming again: cd ~/ mkdir -p data cd data curl -L https://osf.io/5daup/download -o ERR458493.fastq.gz curl -L https://osf.io/8rvh5/download -o ERR458494.fastq.gz curl -L https://osf.io/2wvn3/download -o ERR458495.fastq.gz curl -L https://osf.io/xju4a/download -o ERR458500.fastq.gz curl -L https://osf.io/nmqe6/download -o ERR458501.fastq.gz curl -L https://osf.io/qfsze/download -o ERR458502.fastq.gz And, finally, download the necessary analysis code into your home directory cd ~/ curl -L -O https://raw.githubusercontent.com/ngs-docs/angus/2018/scripts/gather-counts.py curl -L -O https://raw.githubusercontent.com/ngs-docs/angus/2018/scripts/yeast.salmon.R","title":"Install the necessary software and data"},{"location":"automation/#write-a-shell-script-to-run-rnaseq-analysis","text":"A shell script is a text file full of shell commands, that run just as if you're running them interactively at the command line. Here, we'll create one that takes the commands from the RNAseq differential expression tutorial and runs 'em all at once.","title":"Write a shell script to run RNAseq analysis"},{"location":"automation/#put-all-of-the-rnaseq-commands-into-a-single-text-file","text":"We'll script everything but the install and data download part of the RNAseq differential expression tutorial . Use RStudio to create a new file named run-rnaseq.sh in your home directory on the Jetstream computer, and write the following commands into it: # make an rnaseq directory and change into it mkdir rnaseq cd rnaseq # link the data in from the other directory ln -fs ~/data/*.fastq.gz . # download the reference transcriptome curl -O https://downloads.yeastgenome.org/sequence/S288C_reference/orf_dna/orf_coding.fasta.gz # index the reference transcriptome salmon index --index yeast_orfs --type quasi --transcripts orf_coding.fasta.gz # run salmon quant on each of the input read data sets for i in *.fastq.gz do salmon quant -i yeast_orfs --libType U -r $i -o $i.quant --seqBias --gcBias done # collect the counts python2 ~/gather-counts.py # run the edgeR script! Rscript --no-save ~/yeast.salmon.R This is now a shell script that you can use to execute all of those commands in one go -- try it out! Run: cd ~/ bash run-rnaseq.sh You should now have a directory rnaseq/ containing a bunch of files, including the PDF outputs of the RNAseq pipeline, yeast-edgeR-MA-plot.pdf , yeast-edgeR-MDS.pdf , and yeast-edgeR.csv . Pretty cool, eh?","title":"Put all of the RNAseq commands into a single text file"},{"location":"automation/#re-running-the-shell-script","text":"Suppose you wanted to re-run the script. How would you do that? Well, note that the rnaseq directory is created at the top of the script, and everything is executed in that directory. So if you remove the rnaseq directory like so, rm -fr rnaseq you can then do bash run-rnaseq.sh to just ...rerun everything.","title":"Re-running the shell script"},{"location":"automation/#a-trick-make-it-executable","text":"You can get rid of the bash part of the command above with some magic: Put #! /bin/bash at the top of the file, and then run chmod +x ~/run-rnaseq.sh at the command line. You can now run ./run-rnaseq.sh instead of bash run-rnaseq.sh . You might be thinking, ok, why is this important? Well, you can do the same with R scripts and Python scripts (but put /usr/bin/env Rscript or /usr/bin/env python at the top, instead of /bin/bash ). This basically annotates the script with the language it's written in, so you don't have to know or remember yourself. So: it's not necessary but it's a nice trick. You can also always force a script to be run in a particular language by specifying bash <scriptname> or Rscript <Scriptname> , too.","title":"A trick: make it executable"},{"location":"automation/#another-good-addition-make-it-fail-nicely","text":"One sort of weird aspect of shell scripts is that (by default) they keep running even if there's an error. This is bad behavior and we should turn it off. You can observe this behavior by rerunning the script above without deleting the directory rnaseq/ - the mkdir command will print an error because the directory still exists, but A good addition to every shell script is to make it fail on the first error. Do this by putting set -e at the top - this tells bash to exit at the first error, rather than continuing bravely onwards.","title":"Another good addition: make it fail nicely"},{"location":"automation/#a-final-good-addition-make-shell-scripts-print-out-the-commands-theyre-running","text":"You might notice that the shell script gives you the output of the commands it's running, but doesn't tell you which commands it's running. If you add set -x at the top of the shell script and then re-run it, cd ~/ rm -fr rnaseq ./run-rnaseq.sh then you'll see the full set of commands being run!","title":"A final good addition: make shell scripts print out the commands they're running!"},{"location":"automation/#tracking-output-of-shell-scripts","text":"you can do: exec > output.log exec 2> error.log to save all output from that point on to those files. Or, you can do: nohup ./run-rnaseq.sh > output.log 2> error.log & and that will do the same thing, while putting it in the background. You can also run 'screen' or 'tmux' (google it). Torsten recommends: script session.log While you're at it, you should look up nohup.","title":"Tracking output of shell scripts"},{"location":"automation/#a-final-note-on-shell-scripts","text":"set -e and set -x only work in shell scripts - they're bash commands. You need to use other approaches in Python and R.","title":"A final note on shell scripts:"},{"location":"automation/#snakemake-for-automation","text":"Automation via shell script is wonderful, but there are a few problems here. First, you have to run the entire workflow each time and it recomputes everything every time. If you're running a workflow that takes 4 days, and you change a command at the end, you'll have to manually go in and just run the stuff that depends on the changed command. Second, it's very explicit and not very generalizable . If you want to run it on a different RNAseq data set, you're going to have to change a lot of commands. snakemake is one of several workflow systems that help solve these problems. (You can read the documentation here.) Let's take a look!","title":"snakemake for automation"},{"location":"automation/#first-install-snakemake","text":"Run: conda install -y snakemake","title":"First, install snakemake."},{"location":"automation/#writing-your-first-snakemake-rule","text":"We're going to automate the R script at the end of the shell script in snakemake. So, run the rnaseq pipeline: cd ~/ rm -fr rnaseq ./run-rnaseq.sh Now, using RStudio, create a new text file, and put the following text in it: rule make_plots: input: output: \"yeast-edgeR-MA-plot.pdf\", \"yeast-edgeR-MDS.pdf\", \"yeast-edgeR.csv\" shell: \"Rscript --no-save ~/yeast.salmon.R\" This rule says: to make the list of outputs (the PDFs and CSV file), you need the list of inputs (currently lbank) * and then you run the shell command. Save this with the name Snakefile in the rnaseq/ directory, and run snakemake make_plots : cd ~/rnaseq snakemake make_plots you should see \"Nothing to be done.\" That's because the PDFs and CSV file already exist! Let's fix that: rm yeast-*.pdf yeast-*.csv and now, when you run snakemake make_plots , you should see the R script being run. Yay w00t! Then if you run snakemake make_plots again, you will see that it doesn't need to do anything - all the files are \"up to date\".","title":"Writing your first snakemake rule"},{"location":"automation/#challenge-add-the-explicit-list-of-inputs","text":"What does the make_plots rule take as input? List them! Hint: what does the yeast.salmon.R script take in? Once you're done, you can also update the timestamp on the counts files and it will regenerate the PDFs -- touch *.counts snakemake make_plots because it knows that the files the rule depends on have changed!","title":"Challenge: add the explicit list of inputs."},{"location":"automation/#challenge-write-a-rule-that-generates-the-counts-files","text":"Looking at the shell script, write a new Snakemake rule that generates the counts files as outputs. You can start by adding a new blank rule: rule make_counts: input: output: shell: \"\" and filling in from there. Note that rule order does not matter in the Snakefile! Hints: you should list each .counts file separately. To test the rule, do rm *.counts yeast-*.pdf and it will regenerate everything needed to build the PDF!","title":"Challenge: write a rule that generates the .counts files."},{"location":"automation/#writing-rules-with-wildcards","text":"Let's suppose we want to write a rule that takes a FASTQ file (say, ERR458501.fastq.gz ) and runs salmon on it to generate the directory ERR458501.fastq.gz.quant . How would we do that? We could write the rule as follows: rule generate_quant_dir_ERR458501: input: \"ERR458501.fastq.gz\" output: directory(\"ERR458501.fastq.gz.quant\") shell: \"salmon quant -i yeast_orfs --libType U -r ERR458501.fastq.gz -o ERR458501.fastq.gz.quant --seqBias --gcBias\" and that will work just fine -- you can run, rm -fr ERR458501.fastq.gz.quant snakemake ERR458501.fastq.gz.quant and it will happily run salmon for you. But there's a lot of repeated stuff in there. Can we do cool computer things to fix that? Yes! We can use wildcards! Try replacing ERR458501 with {accession} - this makes it into a wildcard that snakemake can automatically fill in: rule generate_quant_dir: input: \"{accession}.fastq.gz\" output: directory(\"{accession}.fastq.gz.quant\") shell: \"salmon quant -i yeast_orfs --libType U -r {wildcards.accession}.fastq.gz -o {wildcards.accession}.fastq.gz.quant --seqBias --gcBias\" Now, you can run snakemake ERR458501.fastq.gz.quant and it will work - but you can also run snakemake ERR458493.fastq.gz.quant and it will do the right thing there, as well.","title":"Writing rules with wildcards"},{"location":"automation/#using-expand-to-work-with-lists-of-files","text":"The challenge to write a rule that generates the .counts files should have left you with a list of six input files ( .quant directories), and make_plots has a very similar of six input files ( .quant.counts ). Can we make this more succinct? Yep. You can have a list that you expand in various ways. First, define the list of files at the top of the Snakefile: input_files=[\"ERR458493.fastq.gz\", \"ERR458494.fastq.gz\", \"ERR458495.fastq.gz\", \"ERR458500.fastq.gz\", \"ERR458501.fastq.gz\", \"ERR458502.fastq.gz\"] now, modify the make_plots rule to have the input be: rule make_plots: input: expand(\"{input_files}.quant.counts\", input_files=input_files)","title":"Using 'expand' to work with lists of files"},{"location":"automation/#challenge","text":"Update the other rules that have lists of six files using expand .","title":"Challenge:"},{"location":"automation/#visualizing-workflow-diagrams","text":"Try running: snakemake --dag yeast-edgeR.csv | dot -Tpng > dag.png","title":"Visualizing workflow diagrams"},{"location":"automation/#challenge-extend-the-snakemake-rules-to-download-and-prepare-the-reference","text":"'nuff said'","title":"Challenge: Extend the snakemake rules to download and prepare the reference"},{"location":"automation/#other-things-snakemake-can-do","text":"You can specify what programs your pipeline depends in on your snakemake setup! snakemake interfaces well with conda (in fact, the authors of snakemake are also leads on the bioconda project) so you can pretty much just list your packages.","title":"Other things snakemake can do"},{"location":"automation/#reproducibility-and-provenance","text":"If you write all your workflows like this, all you need to do is give readers your data files, your Snakefile, and your list of software... and they can reproduce everything you did!","title":"Reproducibility and provenance"},{"location":"automation/#answer-to-first-challenge","text":"rule make_plots: input: \"ERR458493.fastq.gz.quant.counts\", \"ERR458500.fastq.gz.quant.counts\", \"ERR458494.fastq.gz.quant.counts\", \"ERR458501.fastq.gz.quant.counts\", \"ERR458495.fastq.gz.quant.counts\", \"ERR458502.fastq.gz.quant.counts\" output: \"yeast-edgeR-MA-plot.pdf\", \"yeast-edgeR-MDS.pdf\", \"yeast-edgeR.csv\" shell: \"Rscript --no-save ~/yeast.salmon.R\"","title":"Answer to first challenge:"},{"location":"automation/#answer-to-second-challenge","text":"# make the .counts files rule make_counts: input: \"ERR458493.fastq.gz.quant\", \"ERR458500.fastq.gz.quant\", \"ERR458494.fastq.gz.quant\", \"ERR458501.fastq.gz.quant\", \"ERR458495.fastq.gz.quant\", \"ERR458502.fastq.gz.quant\" output: \"ERR458493.fastq.gz.quant.counts\", \"ERR458500.fastq.gz.quant.counts\", \"ERR458494.fastq.gz.quant.counts\", \"ERR458501.fastq.gz.quant.counts\", \"ERR458495.fastq.gz.quant.counts\", \"ERR458502.fastq.gz.quant.counts\" shell: \"python2 ~/gather-counts.py\"","title":"Answer to second challenge"},{"location":"automation/#final-snakemake-file","text":"input_files=[\"ERR458493.fastq.gz\", \"ERR458494.fastq.gz\", \"ERR458495.fastq.gz\", \"ERR458500.fastq.gz\", \"ERR458501.fastq.gz\", \"ERR458502.fastq.gz\"] rule make_plots: input: expand(\"{input_files}.quant.counts\", input_files=input_files) output: \"yeast-edgeR-MA-plot.pdf\", \"yeast-edgeR-MDS.pdf\", \"yeast-edgeR.csv\" shell: \"Rscript --no-save ~/yeast.salmon.R\" # make the .counts files rule make_counts: input: expand(\"{input_files}.quant\", input_files=input_files) output: expand(\"{input_files}.quant.counts\", input_files=input_files) shell: \"python2 ~/gather-counts.py\" rule generate_quant_dir: input: \"{accession}.fastq.gz\" output: \"{accession}.fastq.gz.quant\" shell: \"salmon quant -i yeast_orfs --libType U -r {wildcards.accession}.fastq.gz -o {wildcards.accession}.fastq.gz.quant --seqBias --gcBias\"","title":"Final snakemake file"},{"location":"cicese-cluster/","text":"About your \"omica\" cluster \"omica\" is a small cluster, with master and login node 'omica', 30 processing nodes, named nodo1, nodo2, ..., nodo30 and a distributed file storage with lustre filesystem, mounted on /LUSTRE mount point. Jobs management and scheduler is slurm software. For this course, we will be installing all software into: /LUSTRE/apps/workshop and will keep test data in: /LUSTRE/bioinformatica_data/bioinformatica2018 Logging In If you're on a windows machine, open PUTTY and log into the omica cluster: ip: 158.97.9.9 port: 22 You will need to enter your password. If you have a mac, log in via ssh instead: ssh <usuario>@omica.cicese.mx You will need to enter your password. Working on the cluster After logging in, make sure to switch to a node: ~/.works18 Your prompt should now look like this: [<usuario>@nodo11] Now, to enter our conda environment, run: source activate tara Setting up the environment To set up the software we have previously installed you'll need to execute the following: echo 'export PATH=/LUSTRE/apps/workshop/miniconda3/bin:$PATH' >> ~/.bashrc echo 'export PATH=/LUSTRE/apps/workshop/transrate-1.0.3-linux-x86_64:$PATH' >> ~/.bashrc source ~/.bashrc Future Cluster Use Normally, apps are installed on: /LUSTRE/apps and /LUSTRE/bioinformatica Working directories are created by each user on; /LUSTRE/bioinformatica_data/\"group_subdirectory\"","title":"Cluster login"},{"location":"cicese-cluster/#about-your-omica-cluster","text":"\"omica\" is a small cluster, with master and login node 'omica', 30 processing nodes, named nodo1, nodo2, ..., nodo30 and a distributed file storage with lustre filesystem, mounted on /LUSTRE mount point. Jobs management and scheduler is slurm software. For this course, we will be installing all software into: /LUSTRE/apps/workshop and will keep test data in: /LUSTRE/bioinformatica_data/bioinformatica2018","title":"About your \"omica\" cluster"},{"location":"cicese-cluster/#logging-in","text":"If you're on a windows machine, open PUTTY and log into the omica cluster: ip: 158.97.9.9 port: 22 You will need to enter your password. If you have a mac, log in via ssh instead: ssh <usuario>@omica.cicese.mx You will need to enter your password.","title":"Logging In"},{"location":"cicese-cluster/#working-on-the-cluster","text":"After logging in, make sure to switch to a node: ~/.works18 Your prompt should now look like this: [<usuario>@nodo11] Now, to enter our conda environment, run: source activate tara","title":"Working on the cluster"},{"location":"cicese-cluster/#setting-up-the-environment","text":"To set up the software we have previously installed you'll need to execute the following: echo 'export PATH=/LUSTRE/apps/workshop/miniconda3/bin:$PATH' >> ~/.bashrc echo 'export PATH=/LUSTRE/apps/workshop/transrate-1.0.3-linux-x86_64:$PATH' >> ~/.bashrc source ~/.bashrc","title":"Setting up the environment"},{"location":"cicese-cluster/#future-cluster-use","text":"Normally, apps are installed on: /LUSTRE/apps and /LUSTRE/bioinformatica Working directories are created by each user on; /LUSTRE/bioinformatica_data/\"group_subdirectory\"","title":"Future Cluster Use"},{"location":"code-of-conduct/","text":"Workshop Code of Conduct All attendees, speakers, sponsors and volunteers at our workshop are required to agree with the following code of conduct. Organisers will enforce this code throughout the event. We are expecting cooperation from all participants to help ensuring a safe environment for everybody. tl; dr: be excellent to each other. Need Help? You can reach the course director, Titus Brown, at ctbrown@ucdavis.edu. You can also talk to any of the instructors or TAs if you need immediate help. The Quick Version Our workshop is dedicated to providing a harassment-free workshop experience for everyone, regardless of gender, age, sexual orientation, disability, physical appearance, body size, race, or religion (or lack thereof). We do not tolerate harassment of workshop participants in any form. Sexual language and imagery is not appropriate for any workshop venue, including talks, workshops, parties, Twitter and other online media. Workshop participants violating these rules may be sanctioned or expelled from the workshop without a refund at the discretion of the workshop organisers. The Less Quick Version Harassment includes offensive verbal comments related to gender, age, sexual orientation, disability, physical appearance, body size, race, religion, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention. Participants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, the workshop organisers may take any action they deem appropriate, including warning the offender or expulsion from the workshop with no refund. If you are being harassed, notice that someone else is being harassed, or have any other concerns, please contact a member of workshop staff immediately. Workshop instructors and TAs will be happy to help participants contact KBS security or local law enforcement, provide escorts, or otherwise assist those experiencing harassment to feel safe for the duration of the workshop. We value your attendance. We expect participants to follow these rules at workshop and workshop venues and workshop-related social events. This work is licensed under a Creative Commons Attribution 3.0 Unported License . This Code of Conduct taken from http://confcodeofconduct.com/. See http://www.ashedryden.com/blog/codes-of-conduct-101-faq for more information on codes of conduct.","title":"Code of Conduct"},{"location":"code-of-conduct/#workshop-code-of-conduct","text":"All attendees, speakers, sponsors and volunteers at our workshop are required to agree with the following code of conduct. Organisers will enforce this code throughout the event. We are expecting cooperation from all participants to help ensuring a safe environment for everybody. tl; dr: be excellent to each other.","title":"Workshop Code of Conduct"},{"location":"code-of-conduct/#need-help","text":"You can reach the course director, Titus Brown, at ctbrown@ucdavis.edu. You can also talk to any of the instructors or TAs if you need immediate help.","title":"Need Help?"},{"location":"code-of-conduct/#the-quick-version","text":"Our workshop is dedicated to providing a harassment-free workshop experience for everyone, regardless of gender, age, sexual orientation, disability, physical appearance, body size, race, or religion (or lack thereof). We do not tolerate harassment of workshop participants in any form. Sexual language and imagery is not appropriate for any workshop venue, including talks, workshops, parties, Twitter and other online media. Workshop participants violating these rules may be sanctioned or expelled from the workshop without a refund at the discretion of the workshop organisers.","title":"The Quick Version"},{"location":"code-of-conduct/#the-less-quick-version","text":"Harassment includes offensive verbal comments related to gender, age, sexual orientation, disability, physical appearance, body size, race, religion, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention. Participants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, the workshop organisers may take any action they deem appropriate, including warning the offender or expulsion from the workshop with no refund. If you are being harassed, notice that someone else is being harassed, or have any other concerns, please contact a member of workshop staff immediately. Workshop instructors and TAs will be happy to help participants contact KBS security or local law enforcement, provide escorts, or otherwise assist those experiencing harassment to feel safe for the duration of the workshop. We value your attendance. We expect participants to follow these rules at workshop and workshop venues and workshop-related social events. This work is licensed under a Creative Commons Attribution 3.0 Unported License . This Code of Conduct taken from http://confcodeofconduct.com/. See http://www.ashedryden.com/blog/codes-of-conduct-101-faq for more information on codes of conduct.","title":"The Less Quick Version"},{"location":"count_transcriptomes/","text":"Counting the number of transcriptomes in a metatranscriptome In metagenomics, we use binning to approximate the number of genomes (or species) in a sample. Although we can apply taxonomic classification methods like sourmash to metatranscriptomic samples, these methods only work reliably when sequences already exist for the organism of interest. This is still a relatively rare occurrence in environmental metatranscriptomics. We will make use of other data in our samples to approximate the number of species present. Although our samples were poly-A selected, inevitably a some ribosomal RNA is always sequenced given its abundance in samples. We can estimate the number of species in our metatranscriptome by counting the unique number of ribosomal protein sequences we see. There are many ribosomal proteins one could use for this analysis, and we provide a table of these at the bottom of this lesson. We will use one sequence here, however if we were to use more we would average the number of unique sequences we detect between the total sequences to estimate the number of transcriptomes in our sample. We will first search for our protein in the amino acid sequences derived from our de novo assembly of our metatranscriptome. We will use Pfam domains and a tool called HMMER to help us locate all of the matching sequences. Then, we will extract our matches and filter those that are highly similar. Below is a picture representation of the HMM profile we will be working with. Because we are only searching in our assembly, this method only captures the number of transcriptomes that assembled. First let's make a new directory and link in our annotation file. mkdir -p ${PROJECT}/count-transcriptomes cd ${PROJECT}/count-transcriptomes Then let's link in the peptide sequences (ORFs) predicted from our full transcriptome assembly. ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/tara_f135_megahit_full.transdecoder.pep . Next let's link in an alignment of a Pfam domain for RpsG. In the future, you can download this alignment here ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/PF00177-full.sto . Next we'll build a HMM profile of the Pfam domain. hmmbuild PF00177-full.hmm PF00177-full.sto hmmpress PF00177-full.hmm We then use the HMM profile to search the proteins from our assembly hmmscan -T 100 --tblout PF00177-full-tbl.txt --domtblout PF00177-full-domtbl.txt PF00177-full.hmm tara_f135_megahit_full.transdecoder.pep Let's take a look at one of the files output by this search less -S PF00177-full-tbl.txt The third column contains the names of our protein sequences that matched these domains. We can use those names to extract our matches from our assembly # Grab the names cat PF00177-full-tbl.txt | Rscript -e 'writeLines(noquote(read.table(\"stdin\", stringsAsFactors = F)$V3))' > PF00177-names.txt # extract the matches ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/extract-hmmscan-matches.py . python extract-hmmscan-matches.py PF00177-names.txt tara_f135_megahit_full.transdecoder.pep > PF00177.faa Let's count the number of sequences that matched grep \">\" PF00177.faa | wc -l Some matches are quite similar to each other. Let's cluster our sequences at 97% similarity and see how this changes the number of unique proteins we detect. cd-hit -i PF00177.faa -o PF00177-c97.faa -c .97 Let's see how much clustering at 97% reduced our estimate! grep \">\" PF00177-c97.faa | wc -l From this, we estimate there are 82 transcriptomes assembled from our metatranscriptome. Given that this protein is highly conserved, we could BLAST these sequences against NCBI nr/nt database to see if there are any matches. We could then compare it to our sourmash results and see how much overlap vs. how much we are missing from each! This technique can also be used to find any Pfam domain of interest. For instance, if you are interested in photosynthesis, you could extract all photosynthetic proteins by searching with the Pfam domains. Other proteins to use for quantifcation We used three proteins to quantify the number of transcriptomes in our sample. There are more that can be used, and they are list in the table below. They are originally derived from Carradec et al. 2018 . name COG PFAM RpsG COG0049 PF00177 RpsB COG0052 PF00318 RplK COG0080 PF03946 RplA COG0081 PF00687 RplC COG0087 PF00297 RplD COG0088 PF00573 RplV COG0091 PF00237 RpsC COG0092 PF00189 RplN COG0093 PF01929 RplE COG0094 PF00281 RpsH COG0096 PF00410 RplF COG0097 NA RpsE COG0098 PF03719, PF00333 RpsM COG0099 PF00416 RpsK COG0100 PF00411 RplM COG0102 PF00572 RpsI COG0103 PF00380 RpsO COG0184 NA RpsS COG0185 PF00203 RpsQ COG0186 PF00366 GyrB COG0187 PF00204 GyrA COG0188 PF00521 RimK COG0189 PF08443 FolD COG0190 PF00763, PF02882 Fba COG0191 NA MetK COG0192 PF01941 Pth COG0193 PF01195 Gmk COG0194 PF00625 RplO COG0200 PF00827 RplR COG0256 NA RpsD COG0522 PF00163","title":"Count Transcriptomes"},{"location":"count_transcriptomes/#counting-the-number-of-transcriptomes-in-a-metatranscriptome","text":"In metagenomics, we use binning to approximate the number of genomes (or species) in a sample. Although we can apply taxonomic classification methods like sourmash to metatranscriptomic samples, these methods only work reliably when sequences already exist for the organism of interest. This is still a relatively rare occurrence in environmental metatranscriptomics. We will make use of other data in our samples to approximate the number of species present. Although our samples were poly-A selected, inevitably a some ribosomal RNA is always sequenced given its abundance in samples. We can estimate the number of species in our metatranscriptome by counting the unique number of ribosomal protein sequences we see. There are many ribosomal proteins one could use for this analysis, and we provide a table of these at the bottom of this lesson. We will use one sequence here, however if we were to use more we would average the number of unique sequences we detect between the total sequences to estimate the number of transcriptomes in our sample. We will first search for our protein in the amino acid sequences derived from our de novo assembly of our metatranscriptome. We will use Pfam domains and a tool called HMMER to help us locate all of the matching sequences. Then, we will extract our matches and filter those that are highly similar. Below is a picture representation of the HMM profile we will be working with. Because we are only searching in our assembly, this method only captures the number of transcriptomes that assembled.","title":"Counting the number of transcriptomes in a metatranscriptome"},{"location":"count_transcriptomes/#other-proteins-to-use-for-quantifcation","text":"We used three proteins to quantify the number of transcriptomes in our sample. There are more that can be used, and they are list in the table below. They are originally derived from Carradec et al. 2018 . name COG PFAM RpsG COG0049 PF00177 RpsB COG0052 PF00318 RplK COG0080 PF03946 RplA COG0081 PF00687 RplC COG0087 PF00297 RplD COG0088 PF00573 RplV COG0091 PF00237 RpsC COG0092 PF00189 RplN COG0093 PF01929 RplE COG0094 PF00281 RpsH COG0096 PF00410 RplF COG0097 NA RpsE COG0098 PF03719, PF00333 RpsM COG0099 PF00416 RpsK COG0100 PF00411 RplM COG0102 PF00572 RpsI COG0103 PF00380 RpsO COG0184 NA RpsS COG0185 PF00203 RpsQ COG0186 PF00366 GyrB COG0187 PF00204 GyrA COG0188 PF00521 RimK COG0189 PF08443 FolD COG0190 PF00763, PF02882 Fba COG0191 NA MetK COG0192 PF01941 Pth COG0193 PF01195 Gmk COG0194 PF00625 RplO COG0200 PF00827 RplR COG0256 NA RpsD COG0522 PF00163","title":"Other proteins to use for quantifcation"},{"location":"evaluation-and-mapping/","text":"How well did our assembly work? Set up the directory First, make sure you have the PROJECT variable defined: echo $PROJECT if you don't see any output, make sure to redefine the $PROJECT variable. Now, let's create a folder evaluation to work in: cd $PROJECT mkdir -p evaluation cd evaluation Now, let's copy in the assembly we made in the previous lesson: cp $PROJECT/assembly/tara135_SRF_megahit.fasta ./ Generate assembly statistics with Transrate Transrate is a program that can be used for a couple different types of assembly evaluation. The first and simplest method is to calculate length-based metrics about the assembly, such as the total number of bases, and the N50 of the contigs. Transrate can also be used to compare two assemblies or give you a score which represents proportion of input reads that provide positive support for the assembly. For a further explanation of metrics and how to run the reference-based transrate, see the documentation and the paper by Smith-Unna et al. 2016 . We have installed transrate for you. However, if you need to install it in the future, see installation instructions here . See options for running transrate: transrate -h Let's use transrate to calculate some stats on our assembly contigs: transrate --assembly tara135_SRF_megahit.fasta You should see output that looks like this: [ INFO] 2018-11-06 23:50:35 : Loading assembly: /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/tara135_SRF_megahit.fasta [ INFO] 2018-11-06 23:50:35 : Analysing assembly: /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/tara135_SRF_megahit.fasta [ INFO] 2018-11-06 23:50:35 : Results will be saved in /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/transrate_results/tara135_SRF_megahit [ INFO] 2018-11-06 23:50:35 : Calculating contig metrics... [ INFO] 2018-11-06 23:50:35 : Contig metrics: [ INFO] 2018-11-06 23:50:35 : ----------------------------------- [ INFO] 2018-11-06 23:50:35 : n seqs 1502 [ INFO] 2018-11-06 23:50:35 : smallest 200 [ INFO] 2018-11-06 23:50:35 : largest 4998 [ INFO] 2018-11-06 23:50:35 : n bases 638347 [ INFO] 2018-11-06 23:50:35 : mean len 425.0 [ INFO] 2018-11-06 23:50:35 : n under 200 0 [ INFO] 2018-11-06 23:50:35 : n over 1k 40 [ INFO] 2018-11-06 23:50:35 : n over 10k 0 [ INFO] 2018-11-06 23:50:35 : n with orf 331 [ INFO] 2018-11-06 23:50:35 : mean orf percent 83.54 [ INFO] 2018-11-06 23:50:35 : n90 232 [ INFO] 2018-11-06 23:50:35 : n70 360 [ INFO] 2018-11-06 23:50:35 : n50 453 [ INFO] 2018-11-06 23:50:35 : n30 599 [ INFO] 2018-11-06 23:50:35 : n10 935 [ INFO] 2018-11-06 23:50:35 : gc 0.51 [ INFO] 2018-11-06 23:50:35 : bases n 0 [ INFO] 2018-11-06 23:50:35 : proportion n 0.0 [ INFO] 2018-11-06 23:50:35 : Contig metrics done in 0 seconds [ INFO] 2018-11-06 23:50:35 : No reads provided, skipping read diagnostics [ INFO] 2018-11-06 23:50:35 : No reference provided, skipping comparative diagnostics [ INFO] 2018-11-06 23:50:35 : Writing contig metrics for each contig to /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/transrate_results/tara135_SRF_megahit/contigs.csv [ INFO] 2018-11-06 23:50:35 : Writing analysis results to assemblies.csv Comparing Assemblies We built a metatranscriptome with the full set of TARA_SRF reads. Copy this into your evaluation directory cd ${PROJECT}/evaluation ln -s LUSTRE/bioinformatica_data/bioinformatica2018/assembly/tara_f135_full_megahit.fasta How do the two transcriptomes compare with each other? # full vs. subset transrate --reference=tara_f135_full_megahit.fasta --assembly=tara135_SRF_megahit.fasta --output=full_v_subset # subset vs. full transrate --assembly=tara_f135_full_megahit.fasta --reference=tara135_SRF_megahit.fasta --output=subset_v_full How well does this assembly represent our sequenced reads? It's useful to know how well the transcripts represent the sequenced reads. To do this, we'll need to link in the reads we used to generate this assembly: ln -s ${PROJECT}/trim/TARA_135_SRF_5-20_*.qc.fq.gz ./ Quantifying reads with Salmon We will use Salmon to quantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2017 ). You can read more about salmon-like \"pseudoalignment\" here: Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here 2018 paper: A direct comparison of genome alignment and transcriptome pseudoalignment Quantification with Salmon Check that salmon is available and see run options: salmon -h Now let's check that we still have the trimmed data we created day 1: set -u printf \"\\nMy trimmed data is in $PROJECT/trim/, and consists of $(ls -1 ${PROJECT}/trim/*.qc.fq.gz | wc -l) files\\n\\n\" set +u where set -u should let you know if you have any unset variables, i.e. if the $PROJECT variable is not defined. If you see -bash: PROJECT: unbound variable , then you need to set the $PROJECT variable. export PROJECT= ~/work and then re-run the printf code block. NOTE: if you do not have files, please rerun quality trimming steps here Create a directory to work in cd ${PROJECT} mkdir -p quant cd quant Link an assembly We use a full assembly to use for mapping. This assembly was made with all TARA_135 SRF reads, rather than the subset we used in the assembly tutorial. ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/tara_f135_full_megahit.fasta ./ Run Salmon Build an index for the metatranscriptome: salmon index --index tara135 --transcripts tara_f135_full_megahit.fasta Next, link in the QC reads (produced in quality : ln -s ../trim/*.qc.fq.gz ./ Then, run the salmon mapping: for sample in *1.qc.fq.gz do base=$(basename $sample _1.qc.fq.gz) echo $base salmon quant -i tara135 -p 2 -l A -1 ${base}_1.qc.fq.gz -2 ${base}_2.qc.fq.gz -o ${base}_quant done This will create a bunch of directories named something like TARA_135_SRF_5-20_rep1_quant , containing a bunch of files. Take a look at what files there are: You should see: aux_info cmd_info.json lib_format_counts.json libParams logs quant.sf The two most interesting files are quant.sf , which contains the counts, and salmon_quant.log (in the logs directory), which contains a log from the salmon run. Looking at count data The quant.sf files contain the relevant information about contig expression -- take a look head -20 TARA_135_SRF_5-20_rep1_1m_quant/quant.sf You should see output that looks like this: Name Length EffectiveLength TPM NumReads k119_5 212 63.757 0.000000 0.000 k119_10 231 75.730 0.000000 0.000 k119_14 261 97.683 0.000000 0.000 k119_16 301 130.690 11.736728 1.000 k119_18 302 131.560 0.000000 0.000 k119_21 203 58.357 0.000000 0.000 k119_22 308 136.790 0.000000 0.000 The first column contains the transcript names, and the fifth column is the \"raw counts\", which is what many differential expression programs need. Assess Mapping rate Okay, we got some numbers for how much the transcripts were expressed, but how well did the reads map overall? Let's go look at one of the log files to see. less TARA_135_SRF_5-20_rep1_1m_quant/logs/salmon_quant.log How well did the reads map to the metatranscriptome? Let's see how the rest of the files mapped. We can look at the mapping rate in each log file by executing the following: grep Mapping *quant/logs/* grep is a handy program that finds and prints lines in files that match a pattern. In this case, the pattern we're searching for is the word 'Mapping', and we're searching in any directory that ends in quant and has a logs directory with at least one file in it. How do the mapping rates compare? What does this tell us about our metatranscriptome? How well does quantification capture sample distances? We made an MDS plot of our sourmash compare results yesterday. With our salmon quant.sf files, we can also make and MDS plot and see how different the two are. This will demonstrate how well mapping to our assembly captures the information in our reads. We already ran the code to do this, but if you want to see what it looks like, you can find it here . It relies on the quant.sf files output by Salmon. Look how different they are! To be fair, we only mapped back to a full transcriptome with 1 million reads, but this is still a good test. What might this say about our samples? Another way to assess read mapping Transrate actually has a read assessment mode that uses salmon to \"align\" reads to the transcriptome and generates some metrics on read mapping. We won't run this today, but you could run it via: transrate --assembly=tara135_SRF_megahit.fasta --threads=2 --left=*_1.qc.fq.gz --right *_2.qc.fq.gz --output=${PROJECT}/evaluation/tara135_SRF_transrate Other useful tutorials and references https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html","title":"Assembly Evaluation and Quantification"},{"location":"evaluation-and-mapping/#set-up-the-directory","text":"First, make sure you have the PROJECT variable defined: echo $PROJECT if you don't see any output, make sure to redefine the $PROJECT variable. Now, let's create a folder evaluation to work in: cd $PROJECT mkdir -p evaluation cd evaluation Now, let's copy in the assembly we made in the previous lesson: cp $PROJECT/assembly/tara135_SRF_megahit.fasta ./","title":"Set up the directory"},{"location":"evaluation-and-mapping/#generate-assembly-statistics-with-transrate","text":"Transrate is a program that can be used for a couple different types of assembly evaluation. The first and simplest method is to calculate length-based metrics about the assembly, such as the total number of bases, and the N50 of the contigs. Transrate can also be used to compare two assemblies or give you a score which represents proportion of input reads that provide positive support for the assembly. For a further explanation of metrics and how to run the reference-based transrate, see the documentation and the paper by Smith-Unna et al. 2016 . We have installed transrate for you. However, if you need to install it in the future, see installation instructions here . See options for running transrate: transrate -h Let's use transrate to calculate some stats on our assembly contigs: transrate --assembly tara135_SRF_megahit.fasta You should see output that looks like this: [ INFO] 2018-11-06 23:50:35 : Loading assembly: /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/tara135_SRF_megahit.fasta [ INFO] 2018-11-06 23:50:35 : Analysing assembly: /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/tara135_SRF_megahit.fasta [ INFO] 2018-11-06 23:50:35 : Results will be saved in /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/transrate_results/tara135_SRF_megahit [ INFO] 2018-11-06 23:50:35 : Calculating contig metrics... [ INFO] 2018-11-06 23:50:35 : Contig metrics: [ INFO] 2018-11-06 23:50:35 : ----------------------------------- [ INFO] 2018-11-06 23:50:35 : n seqs 1502 [ INFO] 2018-11-06 23:50:35 : smallest 200 [ INFO] 2018-11-06 23:50:35 : largest 4998 [ INFO] 2018-11-06 23:50:35 : n bases 638347 [ INFO] 2018-11-06 23:50:35 : mean len 425.0 [ INFO] 2018-11-06 23:50:35 : n under 200 0 [ INFO] 2018-11-06 23:50:35 : n over 1k 40 [ INFO] 2018-11-06 23:50:35 : n over 10k 0 [ INFO] 2018-11-06 23:50:35 : n with orf 331 [ INFO] 2018-11-06 23:50:35 : mean orf percent 83.54 [ INFO] 2018-11-06 23:50:35 : n90 232 [ INFO] 2018-11-06 23:50:35 : n70 360 [ INFO] 2018-11-06 23:50:35 : n50 453 [ INFO] 2018-11-06 23:50:35 : n30 599 [ INFO] 2018-11-06 23:50:35 : n10 935 [ INFO] 2018-11-06 23:50:35 : gc 0.51 [ INFO] 2018-11-06 23:50:35 : bases n 0 [ INFO] 2018-11-06 23:50:35 : proportion n 0.0 [ INFO] 2018-11-06 23:50:35 : Contig metrics done in 0 seconds [ INFO] 2018-11-06 23:50:35 : No reads provided, skipping read diagnostics [ INFO] 2018-11-06 23:50:35 : No reference provided, skipping comparative diagnostics [ INFO] 2018-11-06 23:50:35 : Writing contig metrics for each contig to /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/transrate_results/tara135_SRF_megahit/contigs.csv [ INFO] 2018-11-06 23:50:35 : Writing analysis results to assemblies.csv","title":"Generate assembly statistics with Transrate"},{"location":"evaluation-and-mapping/#comparing-assemblies","text":"We built a metatranscriptome with the full set of TARA_SRF reads. Copy this into your evaluation directory cd ${PROJECT}/evaluation ln -s LUSTRE/bioinformatica_data/bioinformatica2018/assembly/tara_f135_full_megahit.fasta How do the two transcriptomes compare with each other? # full vs. subset transrate --reference=tara_f135_full_megahit.fasta --assembly=tara135_SRF_megahit.fasta --output=full_v_subset # subset vs. full transrate --assembly=tara_f135_full_megahit.fasta --reference=tara135_SRF_megahit.fasta --output=subset_v_full","title":"Comparing Assemblies"},{"location":"evaluation-and-mapping/#how-well-does-this-assembly-represent-our-sequenced-reads","text":"It's useful to know how well the transcripts represent the sequenced reads. To do this, we'll need to link in the reads we used to generate this assembly: ln -s ${PROJECT}/trim/TARA_135_SRF_5-20_*.qc.fq.gz ./","title":"How well does this assembly represent our sequenced reads?"},{"location":"evaluation-and-mapping/#quantifying-reads-with-salmon","text":"We will use Salmon to quantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2017 ). You can read more about salmon-like \"pseudoalignment\" here: Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here 2018 paper: A direct comparison of genome alignment and transcriptome pseudoalignment","title":"Quantifying reads with Salmon"},{"location":"evaluation-and-mapping/#quantification-with-salmon","text":"Check that salmon is available and see run options: salmon -h Now let's check that we still have the trimmed data we created day 1: set -u printf \"\\nMy trimmed data is in $PROJECT/trim/, and consists of $(ls -1 ${PROJECT}/trim/*.qc.fq.gz | wc -l) files\\n\\n\" set +u where set -u should let you know if you have any unset variables, i.e. if the $PROJECT variable is not defined. If you see -bash: PROJECT: unbound variable , then you need to set the $PROJECT variable. export PROJECT= ~/work and then re-run the printf code block. NOTE: if you do not have files, please rerun quality trimming steps here","title":"Quantification with Salmon"},{"location":"evaluation-and-mapping/#create-a-directory-to-work-in","text":"cd ${PROJECT} mkdir -p quant cd quant","title":"Create a directory to work in"},{"location":"evaluation-and-mapping/#link-an-assembly","text":"We use a full assembly to use for mapping. This assembly was made with all TARA_135 SRF reads, rather than the subset we used in the assembly tutorial. ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/tara_f135_full_megahit.fasta ./","title":"Link an assembly"},{"location":"evaluation-and-mapping/#run-salmon","text":"Build an index for the metatranscriptome: salmon index --index tara135 --transcripts tara_f135_full_megahit.fasta Next, link in the QC reads (produced in quality : ln -s ../trim/*.qc.fq.gz ./ Then, run the salmon mapping: for sample in *1.qc.fq.gz do base=$(basename $sample _1.qc.fq.gz) echo $base salmon quant -i tara135 -p 2 -l A -1 ${base}_1.qc.fq.gz -2 ${base}_2.qc.fq.gz -o ${base}_quant done This will create a bunch of directories named something like TARA_135_SRF_5-20_rep1_quant , containing a bunch of files. Take a look at what files there are: You should see: aux_info cmd_info.json lib_format_counts.json libParams logs quant.sf The two most interesting files are quant.sf , which contains the counts, and salmon_quant.log (in the logs directory), which contains a log from the salmon run.","title":"Run Salmon"},{"location":"evaluation-and-mapping/#looking-at-count-data","text":"The quant.sf files contain the relevant information about contig expression -- take a look head -20 TARA_135_SRF_5-20_rep1_1m_quant/quant.sf You should see output that looks like this: Name Length EffectiveLength TPM NumReads k119_5 212 63.757 0.000000 0.000 k119_10 231 75.730 0.000000 0.000 k119_14 261 97.683 0.000000 0.000 k119_16 301 130.690 11.736728 1.000 k119_18 302 131.560 0.000000 0.000 k119_21 203 58.357 0.000000 0.000 k119_22 308 136.790 0.000000 0.000 The first column contains the transcript names, and the fifth column is the \"raw counts\", which is what many differential expression programs need.","title":"Looking at count data"},{"location":"evaluation-and-mapping/#assess-mapping-rate","text":"Okay, we got some numbers for how much the transcripts were expressed, but how well did the reads map overall? Let's go look at one of the log files to see. less TARA_135_SRF_5-20_rep1_1m_quant/logs/salmon_quant.log How well did the reads map to the metatranscriptome? Let's see how the rest of the files mapped. We can look at the mapping rate in each log file by executing the following: grep Mapping *quant/logs/* grep is a handy program that finds and prints lines in files that match a pattern. In this case, the pattern we're searching for is the word 'Mapping', and we're searching in any directory that ends in quant and has a logs directory with at least one file in it. How do the mapping rates compare? What does this tell us about our metatranscriptome?","title":"Assess Mapping rate"},{"location":"evaluation-and-mapping/#how-well-does-quantification-capture-sample-distances","text":"We made an MDS plot of our sourmash compare results yesterday. With our salmon quant.sf files, we can also make and MDS plot and see how different the two are. This will demonstrate how well mapping to our assembly captures the information in our reads. We already ran the code to do this, but if you want to see what it looks like, you can find it here . It relies on the quant.sf files output by Salmon. Look how different they are! To be fair, we only mapped back to a full transcriptome with 1 million reads, but this is still a good test. What might this say about our samples?","title":"How well does quantification capture sample distances?"},{"location":"evaluation-and-mapping/#another-way-to-assess-read-mapping","text":"Transrate actually has a read assessment mode that uses salmon to \"align\" reads to the transcriptome and generates some metrics on read mapping. We won't run this today, but you could run it via: transrate --assembly=tara135_SRF_megahit.fasta --threads=2 --left=*_1.qc.fq.gz --right *_2.qc.fq.gz --output=${PROJECT}/evaluation/tara135_SRF_transrate","title":"Another way to assess read mapping"},{"location":"evaluation-and-mapping/#other-useful-tutorials-and-references","text":"https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html","title":"Other useful tutorials and references"},{"location":"filezilla/","text":"Downloading and Transferring Files Our goals for this lesson are: Transfer a file from a remote machine to your computer with FileZilla Transfer a file from your computer to a remote machine with FileZilla Name other tools that can be used for file transfer Transfer Files From Your Instance with Filezilla We've made a lot of plots and QC files that would be nice for us to be able to view. and we have something unique that we would like to view. We will show you have to use FileZilla to transfer a file from your local machine to the cluster, or vice versa. Open the site manager Open FileZilla , and click on the File tab. Choose 'Site Manager'. Add your instances as a site Within the 'Site Manager' window, do the following: Click on 'New Site', and name it something intuitive (e.g. omica) Host: address of the omica cluster: 158.97.9.9 (or omica.cicese.mx) Protocol: SFTP - SSH File Transfer Protocol Logon Type: Ask for password User: curso Password: The password you use to log on to your instance. Click 'Connect' You may see a dialogue pop up like this. Click ok! Then, at the top of the screen, enter your host, username, and password, and click connect. Filezilla - Step 3 In FileZilla, on the left side of the screen navigate to the location you would like to save the file, and on the right side of the screen navigate through your remote directory until you find one of the plots we made earlier. Double click on the file to transfer a copy, or click and drag over to the right hand panel. Open the file on your local machine, and bask in the glory of the plot you made! We could also use scp or rsync to transfer files. These are command line utilities. If you're transferring a large datafile, rsync may be slightly better, because if the download fails it will restart from where it left off. The omica cluster doesn't allow us to use these tools by default, but many other clusters or cloud machines do, so it's good to keep them in mind. + rsync + scp","title":"Filezilla"},{"location":"filezilla/#downloading-and-transferring-files","text":"Our goals for this lesson are: Transfer a file from a remote machine to your computer with FileZilla Transfer a file from your computer to a remote machine with FileZilla Name other tools that can be used for file transfer","title":"Downloading and Transferring Files"},{"location":"filezilla/#transfer-files-from-your-instance-with-filezilla","text":"We've made a lot of plots and QC files that would be nice for us to be able to view. and we have something unique that we would like to view. We will show you have to use FileZilla to transfer a file from your local machine to the cluster, or vice versa.","title":"Transfer Files From Your Instance with Filezilla"},{"location":"filezilla/#open-the-site-manager","text":"Open FileZilla , and click on the File tab. Choose 'Site Manager'.","title":"Open the site manager"},{"location":"filezilla/#add-your-instances-as-a-site","text":"Within the 'Site Manager' window, do the following: Click on 'New Site', and name it something intuitive (e.g. omica) Host: address of the omica cluster: 158.97.9.9 (or omica.cicese.mx) Protocol: SFTP - SSH File Transfer Protocol Logon Type: Ask for password User: curso Password: The password you use to log on to your instance. Click 'Connect' You may see a dialogue pop up like this. Click ok! Then, at the top of the screen, enter your host, username, and password, and click connect.","title":"Add your instances as a site"},{"location":"filezilla/#filezilla-step-3","text":"In FileZilla, on the left side of the screen navigate to the location you would like to save the file, and on the right side of the screen navigate through your remote directory until you find one of the plots we made earlier. Double click on the file to transfer a copy, or click and drag over to the right hand panel. Open the file on your local machine, and bask in the glory of the plot you made! We could also use scp or rsync to transfer files. These are command line utilities. If you're transferring a large datafile, rsync may be slightly better, because if the download fails it will restart from where it left off. The omica cluster doesn't allow us to use these tools by default, but many other clusters or cloud machines do, so it's good to keep them in mind. + rsync + scp","title":"Filezilla - Step 3"},{"location":"for-instructors/","text":"For Instructors: Editing Schedule and Sidebar Navigation: schedule: edit docs/index.md side nav: edit mkdocs.yml To use mkdocs to deploy lessons: If mkdocs is already set up All docs should be written in md in the docs folder, and committed to the repo as usual. If you added new documentation, be sure to add it into the site navigation in mkdocs.yml , or link to it from another doc. Install mkdocs and ghp-import if necessary: conda install -c conda-forge mkdocs conda install -c conda-forge ghp-import Build docs folder into site/ mkdocs build View changes locally: mkdocs serve Push docs changes to gh-pages branch ghp-import site -p If setting up mkdocs for a new repo: Grab the mkdocs repo, dib lab flavor, and follow setup instructions git clone https://github.com/dib-lab/mkdocs-material-dib.git","title":"For Instructors:"},{"location":"for-instructors/#for-instructors","text":"","title":"For Instructors:"},{"location":"for-instructors/#editing-schedule-and-sidebar-navigation","text":"schedule: edit docs/index.md side nav: edit mkdocs.yml To use mkdocs to deploy lessons:","title":"Editing Schedule and Sidebar Navigation:"},{"location":"for-instructors/#if-mkdocs-is-already-set-up","text":"All docs should be written in md in the docs folder, and committed to the repo as usual. If you added new documentation, be sure to add it into the site navigation in mkdocs.yml , or link to it from another doc. Install mkdocs and ghp-import if necessary: conda install -c conda-forge mkdocs conda install -c conda-forge ghp-import Build docs folder into site/ mkdocs build View changes locally: mkdocs serve Push docs changes to gh-pages branch ghp-import site -p","title":"If mkdocs is already set up"},{"location":"for-instructors/#if-setting-up-mkdocs-for-a-new-repo","text":"Grab the mkdocs repo, dib lab flavor, and follow setup instructions git clone https://github.com/dib-lab/mkdocs-material-dib.git","title":"If setting up mkdocs for a new repo:"},{"location":"future-installation/","text":"Installing this software in the future In the future, if you want to run the tutorials on your own, you'll need to set up conda in your own account - see instructions ). To install all the tools we used in the future, we can 1. install from an environment yaml file, or 2. install everything manually. Option 1: Installing from a file At the end of the course, we exported info from the tara environment using: conda env export -n tara -f $PROJECT/tara_conda_environment.yaml You can find download this file here To make a new conda env like this, download that file, then run: conda env create -f tara_conda_environment.yaml you shuld then be able to: source activate tara and you remember you can exit this environment with source deactivate Option 2: Installing Manualling Then create an environment to work in: conda create -n tara and activate it: source activate tara Now, install the software! conda install fastqc multiqc trimmomatic khmer busco megahit sourmash salmon r dammit cd-hit To make sure you have access to these conda-installed programs, try to execute some of them: sourmash Conda works for most of the software we'll use in the workshop. However, there are some exceptions: notably, transrate. To install transrate, follow the instructions here . You can also try the following: cd <location-to-put-transrate> wget https://bintray.com/artifact/download/blahah/generic/transrate-1.0.3-linux-x86_64.tar.gz tar xvf transrate-1.0.3-linux-x86_64.tar.gz To put transrate in your path, you can execute: echo 'export PATH=/LUSTRE/apps/workshop/transrate-1.0.3-linux-x86_64:$PATH' >> ~/.bashrc source ~/.bashrc Then check that transrate is properly installed with transrate -h","title":"Installing this software in the future"},{"location":"future-installation/#installing-this-software-in-the-future","text":"In the future, if you want to run the tutorials on your own, you'll need to set up conda in your own account - see instructions ). To install all the tools we used in the future, we can 1. install from an environment yaml file, or 2. install everything manually.","title":"Installing this software in the future"},{"location":"future-installation/#option-1-installing-from-a-file","text":"At the end of the course, we exported info from the tara environment using: conda env export -n tara -f $PROJECT/tara_conda_environment.yaml You can find download this file here To make a new conda env like this, download that file, then run: conda env create -f tara_conda_environment.yaml you shuld then be able to: source activate tara and you remember you can exit this environment with source deactivate","title":"Option 1: Installing from a file"},{"location":"future-installation/#option-2-installing-manualling","text":"Then create an environment to work in: conda create -n tara and activate it: source activate tara Now, install the software! conda install fastqc multiqc trimmomatic khmer busco megahit sourmash salmon r dammit cd-hit To make sure you have access to these conda-installed programs, try to execute some of them: sourmash Conda works for most of the software we'll use in the workshop. However, there are some exceptions: notably, transrate. To install transrate, follow the instructions here . You can also try the following: cd <location-to-put-transrate> wget https://bintray.com/artifact/download/blahah/generic/transrate-1.0.3-linux-x86_64.tar.gz tar xvf transrate-1.0.3-linux-x86_64.tar.gz To put transrate in your path, you can execute: echo 'export PATH=/LUSTRE/apps/workshop/transrate-1.0.3-linux-x86_64:$PATH' >> ~/.bashrc source ~/.bashrc Then check that transrate is properly installed with transrate -h","title":"Option 2: Installing Manualling"},{"location":"getting-started-rstudio/","text":"Rstudio - Getting started Connect to RStudio by setting your password (note, password will not be visible on the screen): sudo passwd $USER figuring out your username: echo My username is $USER and finding YOUR RStudio server interface Web address: echo http://$(hostname):8787/ Now go to that Web address in your Web browser, and log in with the username and password from above.","title":"Getting started rstudio"},{"location":"getting-started-rstudio/#rstudio-getting-started","text":"Connect to RStudio by setting your password (note, password will not be visible on the screen): sudo passwd $USER figuring out your username: echo My username is $USER and finding YOUR RStudio server interface Web address: echo http://$(hostname):8787/ Now go to that Web address in your Web browser, and log in with the username and password from above.","title":"Rstudio - Getting started"},{"location":"khmer-trimming-viz-2017/","text":"K-mer Error Trimming (Optional) khmer documentation: http://khmer.readthedocs.io/en/latest Why (or why not) do k-mer trimming? If you can assemble your data set without k-mer trimming, there's no reason to do it. The reason we're error trimming here is to speed up the assembler (by removing data) and to decrease the memory requirements of the assembler (by removing a number of k-mers). Set up workspace and install khmer conda install khmer To run error trimming, use the khmer script trim-low-abund.py : cd ${PROJECT}/trim for filename in *_1.qc.fq.gz do #Use the program basename to remove _1.qc.fq.gz to generate the base base=$(basename $filename _1.qc.fq.gz) echo $base #Run khmer trimming (interleave-reads.py ${base}_1.qc.fq.gz ${base}_2.qc.fq.gz )| \\ (trim-low-abund.py - -V -Z 10 -C 3 -o - --gzip -M 8e9) | \\ (extract-paired-reads.py --gzip -p ${base}.khmer.pe.fq.gz -s ${base}.khmer.se.fq.gz) done Assess changes in kmer abundance To see how many k-mers we removed, you can examine the distribution as above, or use the unique-kmers.py script. Let's compare kmers for one sample. unique-kmers.py TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz unique-kmers.py TARA_135_SRF_5-20_rep1_1m.khmer.pe.fq.gz Visualization of Khmer error trimming (2017 workshop) Let's plot a kmer abundance histogram of one sample cd $PROJECT abundance-dist-single.py -M 1e9 -k 21 SRR1976948_1.fastq.gz SRR1976948_1.fastq.gz.dist If you plot a k-mer abundance histogram of the samples, you'll notice something: there's an awful lot of unique (abundance=1) k-mers. These are erroneous k-mers caused by sequencing errors. and in another cell:: %matplotlib inline import numpy from pylab import * dist1 = numpy.loadtxt('SRR1976948_1.fastq.gz.dist', skiprows=1, delimiter=',') plot(dist1[:,0], dist1[:,1]) axis(xmax=50) Many of these errors remain even after you do the Trimmomatic run; you can see this with:: !abundance-dist-single.py -M 1e9 -k 21 SRR1976948_1.qc.fq.gz SRR1976948_1.qc.fq.gz.dist and then plot:: dist2 = numpy.loadtxt('SRR1976948_1.qc.fq.gz.dist', skiprows=1, delimiter=',') plot(dist1[:,0], dist1[:,1], label='untrimmed') plot(dist2[:,0], dist2[:,1], label='trimmed') legend(loc='upper right') axis(xmax=50) This is for two reasons: First, Trimmomatic trims based solely on the quality score, which is a statistical statement about the correctness of a base - a Q score of 30 means that, of 1000 bases with that Q score, 1 of those bases will be wrong. So, a base can have a high Q score and still be wrong! (and many bases will have a low Q score and still be correct) Second, we trimmed very lightly - only bases that had a very low quality were removed. This was intentional because with assembly, you want to retain as much coverage as possible, and the assembler will generally figure out what the \"correct\" base is from the coverage. An alternative to trimming based on the quality scores is to trim based on k-mer abundance - this is known as k-mer spectral error trimming. K-mer spectral error trimming always beats quality score trimming in terms of eliminating errors; e.g. look at this table from Zhang et al., 2014 <http://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0101271> __: .. thumbnail:: files/2014-zhang.png :width: 40% The basic logic is this: if you see low abundance k-mers in a high coverage data set, those k-mers are almost certainly the result of errors. (Caveat: strain variation could also create them.) In metagenomic data sets we do have the problem that we may have very low and very high coverage data. So we don't necessarily want to get rid of all low-abundance k-mers, because they may represent truly low abundance (but useful) data. As part of the khmer project in my lab, we have developed an approach that sorts reads into high abundance and low abundance reads, and only error trims the high abundance reads. .. thumbnail:: files/kmer-trimming.png :width: 40% This does mean that many errors may get left in the data set, because we have no way of figuring out if they are errors or simply low coverage, but that's OK (and you can always trim them off if you really care). .. Error profile@@ To run such error trimming, use the command trim-low-abund.py (at the command line, or prefix with a '!' in the notebook):: interleave-reads.py SRR1976948_1.qc.fq.gz SRR1976948_2.qc.fq.gz | trim-low-abund.py -V -M 8e9 -C 3 -Z 10 - -o SRR1976948.trim.fq Why (or why not) do k-mer trimming? If you can assemble your data set without k-mer trimming, there's no reason to do it. The reason we're error trimming here is to speed up the assembler (by removing data) and to decrease the memory requirements of the assembler (by removing a number of k-mers). To see how many k-mers we removed, you can examine the distribution as above, or use the unique-kmers.py script:: unique-kmers.py SRR1976948_1.qc.fq.gz SRR1976948_2.qc.fq.gz unique-kmers.py SRR1976948.trim.fq Next: :doc: assemble","title":"K-mer Error Trimming"},{"location":"khmer-trimming-viz-2017/#k-mer-error-trimming","text":"(Optional) khmer documentation: http://khmer.readthedocs.io/en/latest","title":"K-mer Error Trimming"},{"location":"khmer-trimming-viz-2017/#why-or-why-not-do-k-mer-trimming","text":"If you can assemble your data set without k-mer trimming, there's no reason to do it. The reason we're error trimming here is to speed up the assembler (by removing data) and to decrease the memory requirements of the assembler (by removing a number of k-mers).","title":"Why (or why not) do k-mer trimming?"},{"location":"khmer-trimming-viz-2017/#set-up-workspace-and-install-khmer","text":"conda install khmer To run error trimming, use the khmer script trim-low-abund.py : cd ${PROJECT}/trim for filename in *_1.qc.fq.gz do #Use the program basename to remove _1.qc.fq.gz to generate the base base=$(basename $filename _1.qc.fq.gz) echo $base #Run khmer trimming (interleave-reads.py ${base}_1.qc.fq.gz ${base}_2.qc.fq.gz )| \\ (trim-low-abund.py - -V -Z 10 -C 3 -o - --gzip -M 8e9) | \\ (extract-paired-reads.py --gzip -p ${base}.khmer.pe.fq.gz -s ${base}.khmer.se.fq.gz) done","title":"Set up workspace and install khmer"},{"location":"khmer-trimming-viz-2017/#assess-changes-in-kmer-abundance","text":"To see how many k-mers we removed, you can examine the distribution as above, or use the unique-kmers.py script. Let's compare kmers for one sample. unique-kmers.py TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz unique-kmers.py TARA_135_SRF_5-20_rep1_1m.khmer.pe.fq.gz","title":"Assess changes in kmer abundance"},{"location":"khmer-trimming-viz-2017/#visualization-of-khmer-error-trimming-2017-workshop","text":"Let's plot a kmer abundance histogram of one sample cd $PROJECT abundance-dist-single.py -M 1e9 -k 21 SRR1976948_1.fastq.gz SRR1976948_1.fastq.gz.dist If you plot a k-mer abundance histogram of the samples, you'll notice something: there's an awful lot of unique (abundance=1) k-mers. These are erroneous k-mers caused by sequencing errors. and in another cell:: %matplotlib inline import numpy from pylab import * dist1 = numpy.loadtxt('SRR1976948_1.fastq.gz.dist', skiprows=1, delimiter=',') plot(dist1[:,0], dist1[:,1]) axis(xmax=50) Many of these errors remain even after you do the Trimmomatic run; you can see this with:: !abundance-dist-single.py -M 1e9 -k 21 SRR1976948_1.qc.fq.gz SRR1976948_1.qc.fq.gz.dist and then plot:: dist2 = numpy.loadtxt('SRR1976948_1.qc.fq.gz.dist', skiprows=1, delimiter=',') plot(dist1[:,0], dist1[:,1], label='untrimmed') plot(dist2[:,0], dist2[:,1], label='trimmed') legend(loc='upper right') axis(xmax=50) This is for two reasons: First, Trimmomatic trims based solely on the quality score, which is a statistical statement about the correctness of a base - a Q score of 30 means that, of 1000 bases with that Q score, 1 of those bases will be wrong. So, a base can have a high Q score and still be wrong! (and many bases will have a low Q score and still be correct) Second, we trimmed very lightly - only bases that had a very low quality were removed. This was intentional because with assembly, you want to retain as much coverage as possible, and the assembler will generally figure out what the \"correct\" base is from the coverage. An alternative to trimming based on the quality scores is to trim based on k-mer abundance - this is known as k-mer spectral error trimming. K-mer spectral error trimming always beats quality score trimming in terms of eliminating errors; e.g. look at this table from Zhang et al., 2014 <http://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0101271> __: .. thumbnail:: files/2014-zhang.png :width: 40% The basic logic is this: if you see low abundance k-mers in a high coverage data set, those k-mers are almost certainly the result of errors. (Caveat: strain variation could also create them.) In metagenomic data sets we do have the problem that we may have very low and very high coverage data. So we don't necessarily want to get rid of all low-abundance k-mers, because they may represent truly low abundance (but useful) data. As part of the khmer project in my lab, we have developed an approach that sorts reads into high abundance and low abundance reads, and only error trims the high abundance reads. .. thumbnail:: files/kmer-trimming.png :width: 40% This does mean that many errors may get left in the data set, because we have no way of figuring out if they are errors or simply low coverage, but that's OK (and you can always trim them off if you really care). .. Error profile@@ To run such error trimming, use the command trim-low-abund.py (at the command line, or prefix with a '!' in the notebook):: interleave-reads.py SRR1976948_1.qc.fq.gz SRR1976948_2.qc.fq.gz | trim-low-abund.py -V -M 8e9 -C 3 -Z 10 - -o SRR1976948.trim.fq","title":"Visualization of Khmer error trimming (2017 workshop)"},{"location":"khmer-trimming-viz-2017/#why-or-why-not-do-k-mer-trimming_1","text":"If you can assemble your data set without k-mer trimming, there's no reason to do it. The reason we're error trimming here is to speed up the assembler (by removing data) and to decrease the memory requirements of the assembler (by removing a number of k-mers). To see how many k-mers we removed, you can examine the distribution as above, or use the unique-kmers.py script:: unique-kmers.py SRR1976948_1.qc.fq.gz SRR1976948_2.qc.fq.gz unique-kmers.py SRR1976948.trim.fq Next: :doc: assemble","title":"Why (or why not) do k-mer trimming?"},{"location":"khmer-trimming/","text":"Why (or why not) do k-mer trimming? Even after quality trimming with Trimmomatic, our reads will still contain errors. Why? First, Trimmomatic trims based solely on the quality score, which is a statistical statement about the correctness of a base - a Q score of 30 means that, of 1000 bases with that Q score, 1 of those bases will be wrong. So, a base can have a high Q score and still be wrong! (and many bases will have a low Q score and still be correct) Second, we trimmed very lightly - only bases that had a very low quality were removed. This was intentional because with assembly, you want to retain as much coverage as possible, and the assembler will generally figure out what the \u201ccorrect\u201d base is from the coverage. An alternative to trimming based on the quality scores is to trim based on k-mer abundance - this is known as k-mer spectral error trimming. K-mer spectral error trimming always beats quality score trimming in terms of eliminating errors; e.g. look at this table from Zhang et al., 2014 : The basic logic is this: if you see low abundance k-mers in a high coverage data set, those k-mers are almost certainly the result of errors. (Caveat: strain variation could also create them.) In metatranscriptomic data sets we do have the problem that we may have very low and very high coverage data. So we don\u2019t necessarily want to get rid of all low-abundance k-mers, because they may represent truly low abundance (but useful) data. As part of the khmer project in our lab, we have developed an approach that sorts reads into high abundance and low abundance reads, and only error trims the high abundance reads. This does mean that many errors may get left in the data set, because we have no way of figuring out if they are errors or simply low coverage, but that\u2019s OK (and you can always trim them off if you really care). Kmer trimming with Khmer To properly compare our TARA samples, it would be best to remove these sequence errors. This can also speed up assembly and reduce memory requirements (although many assemblers have built in k-mer trimming mechanisms as well). Set up workspace and install khmer khmer documentation We've already installed khmer for you, but here's the command if you need to install it in the future: conda install khmer Make sure you have the $PROJECT variable defined: echo $PROJECT if you don't see any output, make sure to redefine the $PROJECT variable. Now, let's create a directory to work in: cd ${PROJECT} mkdir -p khmer_trim cd khmer_trim Let's choose a sample to start with: TARA_135_SRF_5-20 And link in the qc trimmed files. ln -s ${PROJECT}/trim/TARA_135_SRF_5-20_*qc.fq.gz ./ Run Khmer To run error trimming, use the khmer script trim-low-abund.py : for filename in *_1.qc.fq.gz do #Use the program basename to remove _1.qc.fq.gz to generate the base base=$(basename $filename _1.qc.fq.gz) echo $base interleave-reads.py ${base}_1.qc.fq.gz ${base}_2.qc.fq.gz | \\ trim-low-abund.py - -V -Z 10 -C 3 -o - --gzip -M 8e9 | \\ extract-paired-reads.py --gzip -p ${base}.khmer.pe.fq.gz -s ${base}.khmer.se.fq.gz done Assess changes in kmer abundance To see how many k-mers we removed, you can examine the distribution as above, or use the unique-kmers.py script. Let's compare kmers for one sample. unique-kmers.py TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz unique-kmers.py TARA_135_SRF_5-20_rep1_1m.khmer.pe.fq.gz The first two files (the adapter-trimmed inputs) have a total estimated number of unique 32-mers of 26760613; the second (trimmed) file has a total estimated number of unique 32-mers 26659070. So the trimming removed approximately 100,000 k-mers. These numbers are virtually the same BECAUSE WE ARE USING SMALL SUBSET DATA SETS. For any real data sets, the second number will be MUCH smaller than the first, indicating that many low-abundance k-mers were removed as likely errors. For more info and some neat visualization of kmer trimming, go here","title":"Khmer Error Trimming"},{"location":"khmer-trimming/#why-or-why-not-do-k-mer-trimming","text":"Even after quality trimming with Trimmomatic, our reads will still contain errors. Why? First, Trimmomatic trims based solely on the quality score, which is a statistical statement about the correctness of a base - a Q score of 30 means that, of 1000 bases with that Q score, 1 of those bases will be wrong. So, a base can have a high Q score and still be wrong! (and many bases will have a low Q score and still be correct) Second, we trimmed very lightly - only bases that had a very low quality were removed. This was intentional because with assembly, you want to retain as much coverage as possible, and the assembler will generally figure out what the \u201ccorrect\u201d base is from the coverage. An alternative to trimming based on the quality scores is to trim based on k-mer abundance - this is known as k-mer spectral error trimming. K-mer spectral error trimming always beats quality score trimming in terms of eliminating errors; e.g. look at this table from Zhang et al., 2014 : The basic logic is this: if you see low abundance k-mers in a high coverage data set, those k-mers are almost certainly the result of errors. (Caveat: strain variation could also create them.) In metatranscriptomic data sets we do have the problem that we may have very low and very high coverage data. So we don\u2019t necessarily want to get rid of all low-abundance k-mers, because they may represent truly low abundance (but useful) data. As part of the khmer project in our lab, we have developed an approach that sorts reads into high abundance and low abundance reads, and only error trims the high abundance reads. This does mean that many errors may get left in the data set, because we have no way of figuring out if they are errors or simply low coverage, but that\u2019s OK (and you can always trim them off if you really care).","title":"Why (or why not) do k-mer trimming?"},{"location":"khmer-trimming/#kmer-trimming-with-khmer","text":"To properly compare our TARA samples, it would be best to remove these sequence errors. This can also speed up assembly and reduce memory requirements (although many assemblers have built in k-mer trimming mechanisms as well).","title":"Kmer trimming with Khmer"},{"location":"khmer-trimming/#set-up-workspace-and-install-khmer","text":"khmer documentation We've already installed khmer for you, but here's the command if you need to install it in the future: conda install khmer Make sure you have the $PROJECT variable defined: echo $PROJECT if you don't see any output, make sure to redefine the $PROJECT variable. Now, let's create a directory to work in: cd ${PROJECT} mkdir -p khmer_trim cd khmer_trim Let's choose a sample to start with: TARA_135_SRF_5-20 And link in the qc trimmed files. ln -s ${PROJECT}/trim/TARA_135_SRF_5-20_*qc.fq.gz ./","title":"Set up workspace and install khmer"},{"location":"khmer-trimming/#run-khmer","text":"To run error trimming, use the khmer script trim-low-abund.py : for filename in *_1.qc.fq.gz do #Use the program basename to remove _1.qc.fq.gz to generate the base base=$(basename $filename _1.qc.fq.gz) echo $base interleave-reads.py ${base}_1.qc.fq.gz ${base}_2.qc.fq.gz | \\ trim-low-abund.py - -V -Z 10 -C 3 -o - --gzip -M 8e9 | \\ extract-paired-reads.py --gzip -p ${base}.khmer.pe.fq.gz -s ${base}.khmer.se.fq.gz done","title":"Run Khmer"},{"location":"khmer-trimming/#assess-changes-in-kmer-abundance","text":"To see how many k-mers we removed, you can examine the distribution as above, or use the unique-kmers.py script. Let's compare kmers for one sample. unique-kmers.py TARA_135_SRF_5-20_rep1_1m_1.qc.fq.gz TARA_135_SRF_5-20_rep1_1m_2.qc.fq.gz unique-kmers.py TARA_135_SRF_5-20_rep1_1m.khmer.pe.fq.gz The first two files (the adapter-trimmed inputs) have a total estimated number of unique 32-mers of 26760613; the second (trimmed) file has a total estimated number of unique 32-mers 26659070. So the trimming removed approximately 100,000 k-mers. These numbers are virtually the same BECAUSE WE ARE USING SMALL SUBSET DATA SETS. For any real data sets, the second number will be MUCH smaller than the first, indicating that many low-abundance k-mers were removed as likely errors. For more info and some neat visualization of kmer trimming, go here","title":"Assess changes in kmer abundance"},{"location":"megahit-assembly/","text":"Assembling a Metatranscriptome Learning objectives: What is metatranscriptome assembly? How do assemblers work? Checking the quality of assembly Understanding metatranscriptome assembly The basic idea with any transcriptome assembly is you feed in your reads and you get out a bunch of contigs that represent transcripts, or stretches of RNA present in the reads that don't have any long repeats or much significant polymorphism. You run a transcriptome assembly program using the trimmed reads as input and get out a pile of assembled RNA. These contigs will represent transcripts that come from the eukaryotic organisms (poly-A mRNAseq reads) found in each environmental sample. Install Megahit We already installed megahit for you in setup , but here's the installation command for future reference. conda install megahit Link in the trimmed data We will be using the same set of TARA oceans mRNAseq reads that we trimmed in the last lesson from Alberti et al., 2017 . Create a new folder assembly to work in cd $PROJECT mkdir -p assembly cd assembly Link the khmer-trimmed data we prepared earlier in the newly created folder: ln -fs ${PROJECT}/khmer_trim/*.khmer.pe.fq.gz . ls Run the assembler Let's run an assembly: time megahit --12 TARA_135_SRF_5-20_rep1_1m.khmer.pe.fq.gz,TARA_135_SRF_5-20_rep2_1m.khmer.pe.fq.gz --memory 8e9 --num-cpu-threads 2 --out-prefix TARA_135_SRF --out-dir ./TARA_135_SRF_khmer -f This will take about 10 minutes; at the end you should see output like this: --- [STAT] 11733 contigs, total 5202861 bp, min 200 bp, max 4235 bp, avg 443 bp, N50 465 bp --- [Wed Nov 7 02:13:12 2018] ALL DONE. Time elapsed: 431.097547 seconds --- The output assembly will be TARA_135_SRF_khmer/TARA_135_SRF.contigs.fa . Looking at the assembly First, let's copy the assembly into our current directory: cp ./TARA_135_SRF_khmer/*contigs.fa tara135_SRF_megahit.fasta Now, look at the beginning: head tara135_SRF_megahit.fasta These are the transcripts! Yay! What can we do with an assembly? Why would we do an assembly? What advantages does an assembly have over the reads? And what can we do with this assembly? assembly squashes redundant reads, so the assembly should have approximately one sequence per transcript, as opposed to the reads, which many have many reads per transcript. assembled contigs are longer than reads, so it is easier to do gene search on them. We'll cover this tomorrow! assemblies also have fewer errors than the reads do, so sample comparisons and so on may be more accurate. However, assembly also may eliminate some of the data if it's really low coverage, and abundance information is lost as well. Further Reference There are other asssemblers you can try on your data, some of which are listed on our References page . One that we are excited about is PLASS , which does assembly at the protein level!","title":"Assembly with Megahit"},{"location":"megahit-assembly/#assembling-a-metatranscriptome","text":"Learning objectives: What is metatranscriptome assembly? How do assemblers work? Checking the quality of assembly Understanding metatranscriptome assembly The basic idea with any transcriptome assembly is you feed in your reads and you get out a bunch of contigs that represent transcripts, or stretches of RNA present in the reads that don't have any long repeats or much significant polymorphism. You run a transcriptome assembly program using the trimmed reads as input and get out a pile of assembled RNA. These contigs will represent transcripts that come from the eukaryotic organisms (poly-A mRNAseq reads) found in each environmental sample.","title":"Assembling a Metatranscriptome"},{"location":"megahit-assembly/#install-megahit","text":"We already installed megahit for you in setup , but here's the installation command for future reference. conda install megahit","title":"Install Megahit"},{"location":"megahit-assembly/#link-in-the-trimmed-data","text":"We will be using the same set of TARA oceans mRNAseq reads that we trimmed in the last lesson from Alberti et al., 2017 . Create a new folder assembly to work in cd $PROJECT mkdir -p assembly cd assembly Link the khmer-trimmed data we prepared earlier in the newly created folder: ln -fs ${PROJECT}/khmer_trim/*.khmer.pe.fq.gz . ls","title":"Link in the trimmed data"},{"location":"megahit-assembly/#run-the-assembler","text":"Let's run an assembly: time megahit --12 TARA_135_SRF_5-20_rep1_1m.khmer.pe.fq.gz,TARA_135_SRF_5-20_rep2_1m.khmer.pe.fq.gz --memory 8e9 --num-cpu-threads 2 --out-prefix TARA_135_SRF --out-dir ./TARA_135_SRF_khmer -f This will take about 10 minutes; at the end you should see output like this: --- [STAT] 11733 contigs, total 5202861 bp, min 200 bp, max 4235 bp, avg 443 bp, N50 465 bp --- [Wed Nov 7 02:13:12 2018] ALL DONE. Time elapsed: 431.097547 seconds --- The output assembly will be TARA_135_SRF_khmer/TARA_135_SRF.contigs.fa .","title":"Run the assembler"},{"location":"megahit-assembly/#looking-at-the-assembly","text":"First, let's copy the assembly into our current directory: cp ./TARA_135_SRF_khmer/*contigs.fa tara135_SRF_megahit.fasta Now, look at the beginning: head tara135_SRF_megahit.fasta These are the transcripts! Yay!","title":"Looking at the assembly"},{"location":"megahit-assembly/#what-can-we-do-with-an-assembly","text":"Why would we do an assembly? What advantages does an assembly have over the reads? And what can we do with this assembly? assembly squashes redundant reads, so the assembly should have approximately one sequence per transcript, as opposed to the reads, which many have many reads per transcript. assembled contigs are longer than reads, so it is easier to do gene search on them. We'll cover this tomorrow! assemblies also have fewer errors than the reads do, so sample comparisons and so on may be more accurate. However, assembly also may eliminate some of the data if it's really low coverage, and abundance information is lost as well.","title":"What can we do with an assembly?"},{"location":"megahit-assembly/#further-reference","text":"There are other asssemblers you can try on your data, some of which are listed on our References page . One that we are excited about is PLASS , which does assembly at the protein level!","title":"Further Reference"},{"location":"plass-paladin/","text":"Assembling with PLASS The basic idea with any (meta)transcriptome assembly is you feed in your reads and you get out a bunch of contigs that represent transcripts, or stretches of RNA present in the reads. You run a transcriptome assembly program using the adapter & k-mer trimmed reads as input and get out a pile of assembled RNA. We used MEGAHIT earlier to build this sort of assembly. MEGAHIT and many other assemblers work in nucleotide space. This means that they find direct overlaps between the As, Ts, Cs, and Gs in the reads and use information about those overlaps to make contigs. Although this is a powerful method, it often fails when: There is sequencing errors There are repetitive regions There is strain variation When assembly fails, contigs either break or don't assemble at all. Given that nucleotide sequences are far more variable than protein sequences (third base pair wobble in strain variation in particular), assembling in amino acid space can overcome a lot of the difficulties encountered when assembling in nucleotide space. PLASS is a new assembler that assembles in amino acid space. Unlike MEGAHIT, it does not have built in error correction and so it is best to adapter and k-mer trim the reads before using it. It sometimes performs better than nucleotide assemblers and so is good to test on samples to see what type of improvement it can give. The contigs output by PLASS will represent protein sequences that come from the eukaryotic organisms found in each environmental sample. Install PLASS We already installed plass for you, but here's the installation command for future reference. conda install plass Link in the trimmed data We will be using the same set of TARA oceans mRNAseq reads that we trimmed in the last lesson from Alberti et al., 2017 . Create a new folder assembly to work in cd $PROJECT mkdir -p assembly_plass cd assembly_plass Link the khmer-trimmed data we prepared earlier in the newly created folder: ln -fs ${PROJECT}/trim/*.khmer.fq.gz . ls Plass needs separate files for _1 and _2 , so let's split the files for filename in *.khmer.fq.gz do #Use the program basename to remove _1.qc.fq.gz to generate the base base=$(basename $filename .khmer.fq.gz) echo $base #Run khmer trimming split-paired-reads.py --gzip ${base}.khmer.fq.gz -1 ${base}_1.khmer.fq.gz -2 ${base}_2.khmer.fq.gz done Run the assembler Let's run an assembly: plass assemble TARA_135_SRF_5-20_rep1_1m_1.khmer.fq.gz TARA_135_SRF_5-20_rep2_1m_1.khmer.fq.gz TARA_135_SRF_5-20_rep1_1m_2.khmer.fq.gz TARA_135_SRF_5-20_rep2_1m_2.khmer.fq.gz tara135_srf_plass.fasta --threads 2 PLASS is not as fast as megahit, but not as slow as some other assemblers. The output assembly will be tara135_srf_plass.fasta . Looking at the assembly Let's look at the beginning head tara135_srf_plass.fasta How is this assembly different from the megahit transcripts? Mapping to the PLASS Assembly Set up the workspace We'll be using Paladin to map back to the plass assembly. Paladin solves the problem of mapping nucleotide sequences back to amino acid sequences. Let's make a directory to work in cd $PROJECT mkdir -p paladin_mapping cd paladin_mapping Link in the qc trimmed reads ln -s ${PROJECT}/trim/*qc.fq.gz ./ Index the Assembly: paladin index -r3 tara135_srf_plass.fasta Run the mapping: paladin align -f 125 -t 2 tara135_srf_plass.fasta TARA_135_SRF_5-20_rep1_1m_1.khmer.fq.gz This will output a SAM file with the names of the reads that mapped, as well as the amino acid sequences that have been translated into amino acid space.","title":"Assembling with PLASS"},{"location":"plass-paladin/#assembling-with-plass","text":"The basic idea with any (meta)transcriptome assembly is you feed in your reads and you get out a bunch of contigs that represent transcripts, or stretches of RNA present in the reads. You run a transcriptome assembly program using the adapter & k-mer trimmed reads as input and get out a pile of assembled RNA. We used MEGAHIT earlier to build this sort of assembly. MEGAHIT and many other assemblers work in nucleotide space. This means that they find direct overlaps between the As, Ts, Cs, and Gs in the reads and use information about those overlaps to make contigs. Although this is a powerful method, it often fails when: There is sequencing errors There are repetitive regions There is strain variation When assembly fails, contigs either break or don't assemble at all. Given that nucleotide sequences are far more variable than protein sequences (third base pair wobble in strain variation in particular), assembling in amino acid space can overcome a lot of the difficulties encountered when assembling in nucleotide space. PLASS is a new assembler that assembles in amino acid space. Unlike MEGAHIT, it does not have built in error correction and so it is best to adapter and k-mer trim the reads before using it. It sometimes performs better than nucleotide assemblers and so is good to test on samples to see what type of improvement it can give. The contigs output by PLASS will represent protein sequences that come from the eukaryotic organisms found in each environmental sample.","title":"Assembling with PLASS"},{"location":"plass-paladin/#install-plass","text":"We already installed plass for you, but here's the installation command for future reference. conda install plass","title":"Install PLASS"},{"location":"plass-paladin/#link-in-the-trimmed-data","text":"We will be using the same set of TARA oceans mRNAseq reads that we trimmed in the last lesson from Alberti et al., 2017 . Create a new folder assembly to work in cd $PROJECT mkdir -p assembly_plass cd assembly_plass Link the khmer-trimmed data we prepared earlier in the newly created folder: ln -fs ${PROJECT}/trim/*.khmer.fq.gz . ls Plass needs separate files for _1 and _2 , so let's split the files for filename in *.khmer.fq.gz do #Use the program basename to remove _1.qc.fq.gz to generate the base base=$(basename $filename .khmer.fq.gz) echo $base #Run khmer trimming split-paired-reads.py --gzip ${base}.khmer.fq.gz -1 ${base}_1.khmer.fq.gz -2 ${base}_2.khmer.fq.gz done","title":"Link in the trimmed data"},{"location":"plass-paladin/#run-the-assembler","text":"Let's run an assembly: plass assemble TARA_135_SRF_5-20_rep1_1m_1.khmer.fq.gz TARA_135_SRF_5-20_rep2_1m_1.khmer.fq.gz TARA_135_SRF_5-20_rep1_1m_2.khmer.fq.gz TARA_135_SRF_5-20_rep2_1m_2.khmer.fq.gz tara135_srf_plass.fasta --threads 2 PLASS is not as fast as megahit, but not as slow as some other assemblers. The output assembly will be tara135_srf_plass.fasta .","title":"Run the assembler"},{"location":"plass-paladin/#looking-at-the-assembly","text":"Let's look at the beginning head tara135_srf_plass.fasta How is this assembly different from the megahit transcripts?","title":"Looking at the assembly"},{"location":"plass-paladin/#mapping-to-the-plass-assembly","text":"","title":"Mapping to the PLASS Assembly"},{"location":"plass-paladin/#set-up-the-workspace","text":"We'll be using Paladin to map back to the plass assembly. Paladin solves the problem of mapping nucleotide sequences back to amino acid sequences. Let's make a directory to work in cd $PROJECT mkdir -p paladin_mapping cd paladin_mapping Link in the qc trimmed reads ln -s ${PROJECT}/trim/*qc.fq.gz ./","title":"Set up the workspace"},{"location":"plass-paladin/#index-the-assembly","text":"paladin index -r3 tara135_srf_plass.fasta","title":"Index the Assembly:"},{"location":"plass-paladin/#run-the-mapping","text":"paladin align -f 125 -t 2 tara135_srf_plass.fasta TARA_135_SRF_5-20_rep1_1m_1.khmer.fq.gz This will output a SAM file with the names of the reads that mapped, as well as the amino acid sequences that have been translated into amino acid space.","title":"Run the mapping:"},{"location":"references/","text":"Some other useful references We've covered some tools during this workshop that we often use in our workflows, but there are others that may suit your purposes better. We have listed some of them below, as well as some useful observations we have found when using the tools. Finding Data Sequence Read Archive European Nucleotide Archive GenBank NCBI Taxonomy Quality Trimming and Evaluation Trimmomatic FastQC MultiQC CutAdapt -- Very good at removing polyA tails khmer - also many other functions Transcriptome and Metatranscriptome assembly MEGAHIT Trinity PLASS rnaSPADES Assembly Evaluation BUSCO Transrate Annotation and Finding Genes Dammit HMMER NCBI-BLAST Prokka GhostKOALA Read Mapping and Quantification Salmon STAR BWA Bowtie2 HISAT PALADIN Handling Alignment files samtools BEDTools BAMTools Differential Expression deseq2 edgeR CLUST Variant Analysis BCFtools Taxonomic Classification Sourmash Amplicon Sequencing/16S analysis QIIME2 DADA2 MOTHUR","title":"References"},{"location":"references/#some-other-useful-references","text":"We've covered some tools during this workshop that we often use in our workflows, but there are others that may suit your purposes better. We have listed some of them below, as well as some useful observations we have found when using the tools.","title":"Some other useful references"},{"location":"references/#finding-data","text":"Sequence Read Archive European Nucleotide Archive GenBank NCBI Taxonomy","title":"Finding Data"},{"location":"references/#quality-trimming-and-evaluation","text":"Trimmomatic FastQC MultiQC CutAdapt -- Very good at removing polyA tails khmer - also many other functions","title":"Quality Trimming and Evaluation"},{"location":"references/#transcriptome-and-metatranscriptome-assembly","text":"MEGAHIT Trinity PLASS rnaSPADES","title":"Transcriptome and Metatranscriptome assembly"},{"location":"references/#assembly-evaluation","text":"BUSCO Transrate","title":"Assembly Evaluation"},{"location":"references/#annotation-and-finding-genes","text":"Dammit HMMER NCBI-BLAST Prokka GhostKOALA","title":"Annotation and Finding Genes"},{"location":"references/#read-mapping-and-quantification","text":"Salmon STAR BWA Bowtie2 HISAT PALADIN","title":"Read Mapping and Quantification"},{"location":"references/#handling-alignment-files","text":"samtools BEDTools BAMTools","title":"Handling Alignment files"},{"location":"references/#differential-expression","text":"deseq2 edgeR CLUST","title":"Differential Expression"},{"location":"references/#variant-analysis","text":"BCFtools","title":"Variant Analysis"},{"location":"references/#taxonomic-classification","text":"Sourmash","title":"Taxonomic Classification"},{"location":"references/#amplicon-sequencing16s-analysis","text":"QIIME2 DADA2 MOTHUR","title":"Amplicon Sequencing/16S analysis"},{"location":"salmon-testing/","text":"Index the Assembly #for assembly in *_megahit.fasta for assembly in *.contigs.fa do base=$(basename $assembly .contigs.fa) echo $base salmon index -t ${base}.contigs.fa -i ${base}_salmon --threads 2 #salmon index -t ${base}.fasta -i ${base}_salmon --threads 2 done Run quantification for filename in *_1.qc.fq.gz do base=$(basename $filename _1.qc.fq.gz) echo $base for assembly_index in *_salmon do time salmon quant -i ${assembly_index} -l A -1 ${base}_1.qc.fq.gz -2 ${base}_2.qc.fq.gz -o ${base}_quant -p 2 done done","title":"Salmon testing"},{"location":"salmon-testing/#index-the-assembly","text":"#for assembly in *_megahit.fasta for assembly in *.contigs.fa do base=$(basename $assembly .contigs.fa) echo $base salmon index -t ${base}.contigs.fa -i ${base}_salmon --threads 2 #salmon index -t ${base}.fasta -i ${base}_salmon --threads 2 done","title":"Index the Assembly"},{"location":"salmon-testing/#run-quantification","text":"for filename in *_1.qc.fq.gz do base=$(basename $filename _1.qc.fq.gz) echo $base for assembly_index in *_salmon do time salmon quant -i ${assembly_index} -l A -1 ${base}_1.qc.fq.gz -2 ${base}_2.qc.fq.gz -o ${base}_quant -p 2 done done","title":"Run quantification"},{"location":"sample-comparison/","text":"Comparing Metatranscriptome samples Often times, the goal of a (meta)transcriptome sequencing project is to compare the differences in functional profiles between samples. One way to do this is with differential expression. To do differential expression, we de novo assemble a metatranscriptome, quantify the number of reads that \"map\" back to the metatranscriptome, and use those counts to find differences. However, assembly can be a long process, and often we want to get to know our data a bit before we launch into that process. We can use k-mer profiles of the reads to compare samples using sourmash compare . First, let's make a directory that we will be working in: mkdir -p ${PROJECT}/sourmash-compare cd ${PROJECT}/sourmash-compare Let's link in our trimmed reads. ln -s $PROJECT/trim/*1.qc.fq.gz . ln -s $PROJECT/trim/*2.qc.fq.gz . Now we can calculate signatures for each of the files. This will take 5 or 10 minutes to run for infile in *1.qc.fq.gz do j=$(basename ${infile} 1.qc.fq.gz) sourmash compute -k 31 --scaled 10000 --track-abundance --merge ${j} -o ${j}.sig ${j}2.qc.fq.gz ${infile} done Using these signatures, we can compare our samples. sourmash compare -k 31 -o tara-trimmed.comp *sig Now let's plot! Sourmash has a built in plot utility that we can take advantage of. The output is a heatmap. sourmash plot --labels tara-trimmed.comp This produces a file, tara-trimmed.comp.matrix.png , that contains a similarity matrix. You can see the heatmap here We can also use the output of sourmash compare to calculate an MDS plot. Let's rerun sourmash compare , this time saving the output in csv format. sourmash compare -k 31 --csv tara-trimmed.comp.csv *sig We can use this output to make a Multidimensional Scaling plot. MDS plots are commonly used in transcriptome workflows to visualize distance between samples. Here the strength is we used all of our reads to calculate these distances. To make an MDS plot, run: ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/scripts/mds_plot.R . Rscript mds_plot.R tara-trimmed.comp.csv tara-trimmed-comp-mds.pdf The script source is here if you are interested! This outputs a file tara-trimmed-comp-mds.pdf . You can see what that visualization looks like here . We see that our samples cluster by site ( TARA_135 vs TARA_136 ) and then by depth (SRF for surface vs DCM for deep cholorophyl maximum). Throughout this lesson we have been working with raw reads. Raw reads contain a lot of errors, and these errors are included in the signatures. Next we will learn to k-mer trim our reads, and then re-run compare to see if makes a difference!","title":"Comparing Samples"},{"location":"sample-comparison/#comparing-metatranscriptome-samples","text":"Often times, the goal of a (meta)transcriptome sequencing project is to compare the differences in functional profiles between samples. One way to do this is with differential expression. To do differential expression, we de novo assemble a metatranscriptome, quantify the number of reads that \"map\" back to the metatranscriptome, and use those counts to find differences. However, assembly can be a long process, and often we want to get to know our data a bit before we launch into that process. We can use k-mer profiles of the reads to compare samples using sourmash compare . First, let's make a directory that we will be working in: mkdir -p ${PROJECT}/sourmash-compare cd ${PROJECT}/sourmash-compare Let's link in our trimmed reads. ln -s $PROJECT/trim/*1.qc.fq.gz . ln -s $PROJECT/trim/*2.qc.fq.gz . Now we can calculate signatures for each of the files. This will take 5 or 10 minutes to run for infile in *1.qc.fq.gz do j=$(basename ${infile} 1.qc.fq.gz) sourmash compute -k 31 --scaled 10000 --track-abundance --merge ${j} -o ${j}.sig ${j}2.qc.fq.gz ${infile} done Using these signatures, we can compare our samples. sourmash compare -k 31 -o tara-trimmed.comp *sig Now let's plot! Sourmash has a built in plot utility that we can take advantage of. The output is a heatmap. sourmash plot --labels tara-trimmed.comp This produces a file, tara-trimmed.comp.matrix.png , that contains a similarity matrix. You can see the heatmap here We can also use the output of sourmash compare to calculate an MDS plot. Let's rerun sourmash compare , this time saving the output in csv format. sourmash compare -k 31 --csv tara-trimmed.comp.csv *sig We can use this output to make a Multidimensional Scaling plot. MDS plots are commonly used in transcriptome workflows to visualize distance between samples. Here the strength is we used all of our reads to calculate these distances. To make an MDS plot, run: ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/scripts/mds_plot.R . Rscript mds_plot.R tara-trimmed.comp.csv tara-trimmed-comp-mds.pdf The script source is here if you are interested! This outputs a file tara-trimmed-comp-mds.pdf . You can see what that visualization looks like here . We see that our samples cluster by site ( TARA_135 vs TARA_136 ) and then by depth (SRF for surface vs DCM for deep cholorophyl maximum). Throughout this lesson we have been working with raw reads. Raw reads contain a lot of errors, and these errors are included in the signatures. Next we will learn to k-mer trim our reads, and then re-run compare to see if makes a difference!","title":"Comparing Metatranscriptome samples"},{"location":"setting-up-tara-environment/","text":"What is conda? Conda is a \"package manager\" or software installer. See the full list of commands . conda install to install a package. conda list to list installed packages. conda search to search packages. Note that you'll see one package for every version of the software and for every version of Python (e.g. conda search sourmash ). What is bioconda? See the bioconda paper and the bioconda web site . Bioconda is a community-enabled repository of 3,000+ bioinformatics packages, installable via the conda package manager. It consists of a set of recipes, like this one, for sourmash , that are maintained by the community. What problems does conda (and therefore bioconda) solve? Conda tracks installed packages and their versions, and makes sure that different installed packages don't have conflicting dependencies. It makes software installation so much better! Constructing and using multiple environments A feature that we do not use much here, but that can be very handy in some circumstances, is different environments. \"Environments\" are multiple different collections of installed software. There are two reasons you might want to do this: first, you might want to try to exactly replicate a specific software install, so that you can replicate a paper or an old condition. second, you might be working with incompatible software, e.g. sometimes different software pipelines need different version of the same software. An example of this is older bioinformatics software that needs python2, while other software needs python3. Installing the software for this course We already installed conda for you, but you will need to tell your terminal where to look for that software. You can do this by executing the following: echo 'export PATH=/LUSTRE/apps/workshop/miniconda3/bin:$PATH' >> ~/.bashrc echo 'export PATH=/LUSTRE/apps/workshop/transrate-1.0.3-linux-x86_64:$PATH' >> ~/.bashrc source ~/.bashrc After executing this, you should be able to run conda and look for software environments To see the installed environments, run conda info --envs To activate the tara environment, which contains all software we'll use in the workshop, run source activate tara When you want to exit this environment later, you can execute source deactivate to return to the base env. Finally -- Run ~/works18 and then source activate tara so that we are all working on different distinct computers on the omica cluster.","title":"Software setup"},{"location":"setting-up-tara-environment/#what-is-conda","text":"Conda is a \"package manager\" or software installer. See the full list of commands . conda install to install a package. conda list to list installed packages. conda search to search packages. Note that you'll see one package for every version of the software and for every version of Python (e.g. conda search sourmash ).","title":"What is conda?"},{"location":"setting-up-tara-environment/#what-is-bioconda","text":"See the bioconda paper and the bioconda web site . Bioconda is a community-enabled repository of 3,000+ bioinformatics packages, installable via the conda package manager. It consists of a set of recipes, like this one, for sourmash , that are maintained by the community.","title":"What is bioconda?"},{"location":"setting-up-tara-environment/#what-problems-does-conda-and-therefore-bioconda-solve","text":"Conda tracks installed packages and their versions, and makes sure that different installed packages don't have conflicting dependencies. It makes software installation so much better!","title":"What problems does conda (and therefore bioconda) solve?"},{"location":"setting-up-tara-environment/#constructing-and-using-multiple-environments","text":"A feature that we do not use much here, but that can be very handy in some circumstances, is different environments. \"Environments\" are multiple different collections of installed software. There are two reasons you might want to do this: first, you might want to try to exactly replicate a specific software install, so that you can replicate a paper or an old condition. second, you might be working with incompatible software, e.g. sometimes different software pipelines need different version of the same software. An example of this is older bioinformatics software that needs python2, while other software needs python3.","title":"Constructing and using multiple environments"},{"location":"setting-up-tara-environment/#installing-the-software-for-this-course","text":"We already installed conda for you, but you will need to tell your terminal where to look for that software. You can do this by executing the following: echo 'export PATH=/LUSTRE/apps/workshop/miniconda3/bin:$PATH' >> ~/.bashrc echo 'export PATH=/LUSTRE/apps/workshop/transrate-1.0.3-linux-x86_64:$PATH' >> ~/.bashrc source ~/.bashrc After executing this, you should be able to run conda and look for software environments To see the installed environments, run conda info --envs To activate the tara environment, which contains all software we'll use in the workshop, run source activate tara When you want to exit this environment later, you can execute source deactivate to return to the base env.","title":"Installing the software for this course"},{"location":"setting-up-tara-environment/#finally-","text":"Run ~/works18 and then source activate tara so that we are all working on different distinct computers on the omica cluster.","title":"Finally --"},{"location":"short-read-quality-control/","text":"Short Read Quality Control You should now be logged into your cluster! If not, go back to Cluster Login Data source We will be using RNAseq reads from a small subset of data from the TARA Oceans Expedition , from Alberti et al., 2017 and analyzed as part of A global ocean atlas of eukaryotic genes . Set up workspace and download the data First, make some directories to work in: cd mkdir -p work/data Next, change into the data dir and download the data subsets: cd work/data wget https://osf.io/76qm3/download -O tara135_1m.zip wget https://osf.io/y5dfh/download -O tara136-137_1m.zip Now, let's unzip and make the files difficult to delete unzip tara135_1m.zip unzip tara136-137_1m.zip chmod u-w *fq.gz To make life easier, let's define a variable for the location of this tara working directory: export PROJECT=~/work Check that your data is where it should be ls $PROJECT/data/ If you see all the files you think you should, good! Otherwise, debug. These are FASTQ files -- let's take a look at them: zless $PROJECT/data//TARA_135_DCM_5-20_rep1_1m_1.fq.gz (use the spacebar to scroll down, and type 'q' to exit 'zless') Question: where does the filename come from? why are there 1 and 2 in the file names? Links: FASTQ Format Quality trimming and light quality filtering Make sure you've got the PROJECT location defined, and your data is there: set -u printf \"\\nMy raw data is in $PROJECT/data/, and consists of $(ls -1 ${PROJECT}/data/*.fq.gz | wc -l) files\\n\\n\" set +u Important: If you get an error above or the count of files is wrong...STOP!! Revisit the download & unzip instructions! Link your data into your working directory Change into your project directory and make a workspace for quality trimming: cd ${PROJECT} mkdir -p quality cd quality Now, link the data files into your new workspace ln -s ../data/*.fq.gz ./ (Linking with ln avoids having to make a copy of the files, which will take up storage space.) Check to make sure it worked printf \"I see $(ls -1 *.fq.gz | wc -l) files here.\\n\" You can also do an ls to list the files. If you see only one entry, *.fq.gz , then the ln command above didn't work properly. One possibility is that your files aren't in your data directory; another is that their names don't end with .fq.gz . FastQC We're going to use FastQC to summarize the data. To install fastqc via conda: Note: make sure you've followed the conda setup instructions . conda install fastqc Now, run FastQC on the files: fastqc *.fq.gz After this finishes running (has to run on each file so might take a while), type 'ls': ls -d *fastqc.zip* to list the files, and you should see a number of files with the extensions .fastqc.zip . Inside each of the fastqc directories you will find reports from the fastqc. We can view one of these files here , and the second pair of that sample here You can download these files using a technique we'll show you later. Questions: What should you pay attention to in the FastQC report? Which is \"better\", file 1 or file 2? And why? Links: FastQC FastQC tutorial video Examples of fastqc after technical sequencer problems (starting on slide 40) There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file. MultiQC If you would like to aggregate all of your fastqc reports across many samples, MultiQC will do this into a single report for easy comparison. Install MultiQC with conda: conda install multiqc Run MultiQC: multiqc . The terminal output should look like this: [INFO ] multiqc : This is MultiQC v1.6 [INFO ] multiqc : Template : default [INFO ] multiqc : Searching '.' [INFO ] fastqc : Found 8 reports [INFO ] multiqc : Compressing plot data [INFO ] multiqc : Report : multiqc_report.html [INFO ] multiqc : Data : multiqc_data [INFO ] multiqc : MultiQC complete You can view this report here If you are unable to use scp though a terminal output, you can see the fastqc html output here Adapter trim each pair of files Install Trimmomatic: conda install trimmomatic Setup trim directory: cd $PROJECT mkdir -p trim cd trim ln -s ../data/*.fq.gz . cat /LUSTRE/apps/workshop/miniconda3/envs/tara/share/trimmomatic*/adapters/* > combined.fa See this excellent paper on trimming from MacManes 2014 . Run: for filename in *1.fq.gz do #Use the program basename to remove _1.fq.gz to generate the base base=$(basename $filename _1.fq.gz) echo $base # run Trimmomatic trimmomatic PE ${base}_1.fq.gz \\ ${base}_2.fq.gz \\ ${base}_1.qc.fq.gz s1_se \\ ${base}_2.qc.fq.gz s2_se \\ ILLUMINACLIP:combined.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25 # save the orphans gzip -9c s1_se s2_se >> orphans.qc.fq.gz rm -f s1_se s2_se done Now, run fastqc again on trimmed files: fastqc *.qc.fq.gz multiqc . The paired sequences output by this set of commands will be in the files ending in .qc.fq.gz , with any orphaned sequences all together in orphans.qc.fq.gz . Make these trimmed reads read-only and keep them, as we will reuse them later. chmod a-w ${PROJECT}/trim/*.qc.fq.gz Questions: How do you figure out what the parameters mean? How do you figure out what parameters to use? What adapters do you use? What version of Trimmomatic are we using here? (And FastQC?) Do you think parameters are different for RNAseq and genomic data sets? What's with these annoyingly long and complicated filenames? why are we running R1 and R2 together? For a discussion of optimal trimming strategies, see MacManes, 2014 -- it's about RNAseq but similar arguments should apply to metagenome assembly. Links: Trimmomatic Questions: is the quality trimmed data \"better\" than before? Does it matter that you still have adapters!?","title":"Short Read Quality Control"},{"location":"short-read-quality-control/#short-read-quality-control","text":"You should now be logged into your cluster! If not, go back to Cluster Login","title":"Short Read Quality Control"},{"location":"short-read-quality-control/#data-source","text":"We will be using RNAseq reads from a small subset of data from the TARA Oceans Expedition , from Alberti et al., 2017 and analyzed as part of A global ocean atlas of eukaryotic genes .","title":"Data source"},{"location":"short-read-quality-control/#set-up-workspace-and-download-the-data","text":"First, make some directories to work in: cd mkdir -p work/data Next, change into the data dir and download the data subsets: cd work/data wget https://osf.io/76qm3/download -O tara135_1m.zip wget https://osf.io/y5dfh/download -O tara136-137_1m.zip Now, let's unzip and make the files difficult to delete unzip tara135_1m.zip unzip tara136-137_1m.zip chmod u-w *fq.gz To make life easier, let's define a variable for the location of this tara working directory: export PROJECT=~/work Check that your data is where it should be ls $PROJECT/data/ If you see all the files you think you should, good! Otherwise, debug. These are FASTQ files -- let's take a look at them: zless $PROJECT/data//TARA_135_DCM_5-20_rep1_1m_1.fq.gz (use the spacebar to scroll down, and type 'q' to exit 'zless') Question: where does the filename come from? why are there 1 and 2 in the file names? Links: FASTQ Format","title":"Set up workspace and download the data"},{"location":"short-read-quality-control/#quality-trimming-and-light-quality-filtering","text":"Make sure you've got the PROJECT location defined, and your data is there: set -u printf \"\\nMy raw data is in $PROJECT/data/, and consists of $(ls -1 ${PROJECT}/data/*.fq.gz | wc -l) files\\n\\n\" set +u Important: If you get an error above or the count of files is wrong...STOP!! Revisit the download & unzip instructions!","title":"Quality trimming and light quality filtering"},{"location":"short-read-quality-control/#link-your-data-into-your-working-directory","text":"Change into your project directory and make a workspace for quality trimming: cd ${PROJECT} mkdir -p quality cd quality Now, link the data files into your new workspace ln -s ../data/*.fq.gz ./ (Linking with ln avoids having to make a copy of the files, which will take up storage space.) Check to make sure it worked printf \"I see $(ls -1 *.fq.gz | wc -l) files here.\\n\" You can also do an ls to list the files. If you see only one entry, *.fq.gz , then the ln command above didn't work properly. One possibility is that your files aren't in your data directory; another is that their names don't end with .fq.gz .","title":"Link your data into your working directory"},{"location":"short-read-quality-control/#fastqc","text":"We're going to use FastQC to summarize the data. To install fastqc via conda: Note: make sure you've followed the conda setup instructions . conda install fastqc Now, run FastQC on the files: fastqc *.fq.gz After this finishes running (has to run on each file so might take a while), type 'ls': ls -d *fastqc.zip* to list the files, and you should see a number of files with the extensions .fastqc.zip . Inside each of the fastqc directories you will find reports from the fastqc. We can view one of these files here , and the second pair of that sample here You can download these files using a technique we'll show you later. Questions: What should you pay attention to in the FastQC report? Which is \"better\", file 1 or file 2? And why? Links: FastQC FastQC tutorial video Examples of fastqc after technical sequencer problems (starting on slide 40) There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file.","title":"FastQC"},{"location":"short-read-quality-control/#multiqc","text":"If you would like to aggregate all of your fastqc reports across many samples, MultiQC will do this into a single report for easy comparison. Install MultiQC with conda: conda install multiqc Run MultiQC: multiqc . The terminal output should look like this: [INFO ] multiqc : This is MultiQC v1.6 [INFO ] multiqc : Template : default [INFO ] multiqc : Searching '.' [INFO ] fastqc : Found 8 reports [INFO ] multiqc : Compressing plot data [INFO ] multiqc : Report : multiqc_report.html [INFO ] multiqc : Data : multiqc_data [INFO ] multiqc : MultiQC complete You can view this report here If you are unable to use scp though a terminal output, you can see the fastqc html output here","title":"MultiQC"},{"location":"short-read-quality-control/#adapter-trim-each-pair-of-files","text":"Install Trimmomatic: conda install trimmomatic Setup trim directory: cd $PROJECT mkdir -p trim cd trim ln -s ../data/*.fq.gz . cat /LUSTRE/apps/workshop/miniconda3/envs/tara/share/trimmomatic*/adapters/* > combined.fa See this excellent paper on trimming from MacManes 2014 . Run: for filename in *1.fq.gz do #Use the program basename to remove _1.fq.gz to generate the base base=$(basename $filename _1.fq.gz) echo $base # run Trimmomatic trimmomatic PE ${base}_1.fq.gz \\ ${base}_2.fq.gz \\ ${base}_1.qc.fq.gz s1_se \\ ${base}_2.qc.fq.gz s2_se \\ ILLUMINACLIP:combined.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25 # save the orphans gzip -9c s1_se s2_se >> orphans.qc.fq.gz rm -f s1_se s2_se done Now, run fastqc again on trimmed files: fastqc *.qc.fq.gz multiqc . The paired sequences output by this set of commands will be in the files ending in .qc.fq.gz , with any orphaned sequences all together in orphans.qc.fq.gz . Make these trimmed reads read-only and keep them, as we will reuse them later. chmod a-w ${PROJECT}/trim/*.qc.fq.gz Questions: How do you figure out what the parameters mean? How do you figure out what parameters to use? What adapters do you use? What version of Trimmomatic are we using here? (And FastQC?) Do you think parameters are different for RNAseq and genomic data sets? What's with these annoyingly long and complicated filenames? why are we running R1 and R2 together? For a discussion of optimal trimming strategies, see MacManes, 2014 -- it's about RNAseq but similar arguments should apply to metagenome assembly. Links: Trimmomatic Questions: is the quality trimmed data \"better\" than before? Does it matter that you still have adapters!?","title":"Adapter trim each pair of files"},{"location":"sourmash-taxonomic-classification/","text":"Taxonomic Classification with Sourmash Knowing what species are in our metatranscriptome allows us to take advantage of other resources already developed for those species, such as genome sequences and genome annotations. We will use a tool called sourmash to get a first pass look at what's in our metatranscriptome. Sourmash works by calculating a \"signature\" for a sequence. For taxonomic classification, you can then compare that signature to signatures made from publicly available data. To make this comparison faster, we put the publicly available sequences in a database that we can compare against. We have made databases from eukaryotic RNA sequences available in GenBank, as well as the MMETSP transcriptomes that were recently reassembled . After making signatures for our assembly, we will search against all of our databases at once. Note that sourmash cannot find matches across large evolutionary distances. sourmash seems to work well to search and compare data sets for matches at the species and genus level, but does not have much sensitivity beyond that. It seems to be particularly good at strain-level analysis. You should use protein-based analyses to do searches across larger evolutionary distances (e.g. with a tool like KEGG GhostKOALA ). Let's get started! First, let's make a directory and link in our assembly. We are going to work with a metatranscriptome assembly that we'll show you how to run later - here we link in a copy that we placed on the cluster already. mkdir -p ${PROJECT}/sourmash-gather cd ${PROJECT}/sourmash-gather ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/tara_f135_full_megahit.fasta . Next, let's make a signature of our assembly. sourmash uses k-mers to calculate signatures. A k-mer size of 21 is approximately specific at the genus level, a 31 is at the species level, and 51 at the strain level. We will calculate our signature with all three k-mer sizes so we can choose which one we want to use later. The --scaled parameter in the command tells sourmash to only look at 1/10,000th of the unique k-mer space. This parameter is agnostic to sequence content, but is consistent across signatures. Lastly, we use the --track-abundance flag. This tells sourmash to keep track of how often it sees a k-mer. This is meaningful for transcriptomic data because expression levels are variable. sourmash compute -k 21,31,51 --scaled 10000 --track-abundance -o tara_f135_full_megahit.sig tara_f135_full_megahit.fasta Now let's download some databases that contain signatures from transcriptomes of publicly-available eukaryotic sequences. We will download databases for plants, fungi, protozoa, invertebrates, and vertebrates, as well a database that contains sequences from the MMETSP re-assembly project (>700 marine eukaryotic transcriptomes). Even if you don't think a certain class of eukaryote is likely to be in your sample (for instance, vertebrate_mammalian), it's a good idea to include all of the databases anyway. It can help you quickly locate potential contamination in your sequencing (we wouldn't expect to find mouse RNA in the ocean, but it is a commonly sequenced organism and thus if you find it, if may be a contaminant). ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-vertebrate_other-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-vertebrate_mammalian-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-invertebrate-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-fungi-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-plant-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-protozoa-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/mmetsp-k31-named.tar.gz . Note, if you want to download these files instead, do: wget -O genbank-rna-vertebrate_other-k31.tar.gz https://osf.io/qgyax/download wget -O genbank-rna-vertebrate_mammalian-k31.tar.gz https://osf.io/6c9uy/download wget -O genbank-rna-invertebrate-k31.tar.gz https://osf.io/7v8ck/download wget -O genbank-rna-fungi-k31.tar.gz https://osf.io/g6mcr/download wget -O genbank-rna-plant-k31.tar.gz https://osf.io/kctus/download wget -O genbank-rna-protozoa-k31.tar.gz https://osf.io/fnu2q/download wget -O mmetsp-k31-named.tar.gz https://osf.io/cdvqn/download but this will not work on the nodes on the cicese cluster! Next, we need to uncompress the databases: for infile in *.tar.gz do tar xf ${infile} done And now we can perform taxonomic classification! sourmash gather -k 31 --scaled 10000 -o tara_f135_full_megahit.csv \\ tara_f135_full_megahit.sig \\ *.sbt.json */*.sbt.json You should see an output that looks something like this: overlap p_query p_match avg_abund --------- ------- ------- --------- 4.3 Mbp 3.3% 5.5% 1.0 Neoceratium fusus PA161109 MMETSP1074 1.2 Mbp 0.9% 3.8% 1.0 Calcidiscus leptoporus RCC1130 MMETSP... 4.2 Mbp 0.2% 0.3% 1.1 Neoceratium fusus PA161109 MMETSP1075 130.0 kbp 0.1% 0.2% 1.0 Brandtodinium nutricula RCC3387 MMETS... 90.0 kbp 0.1% 0.3% 1.0 Emiliania huxleyi CCMP370 MMETSP1157 70.0 kbp 0.0% 0.3% 1.0 Pelagomonas calceolata RCC969 MMETSP1328 70.0 kbp 0.1% 0.5% 1.2 Prasinoderma singularis RCC927 MMETSP... 80.0 kbp 0.0% 0.1% 1.2 Protoceratium reticulatum CCCM535=CCM... 50.0 kbp 0.0% 0.0% 1.2 Symbiodinium sp. CCMP421 MMETSP1110 50.0 kbp 0.0% 0.1% 1.0 Heterocapsa rotundata SCCAPK-0483 MME... 50.0 kbp 0.0% 0.0% 1.5 Scrippsiella trochoidea CCMP3099 MMET... 60.0 kbp 0.0% 0.1% 1.5 Prorocentrum minimum CCMP2233 MMETSP0269 found less than 40.0 kbp in common. => exiting found 12 matches total; the recovered matches hit 4.9% of the query This will take about 5 to 10 minutes to run. The two columns to pay attention to are p_query and p_match . p_query is the percent of the metatranscriptome assembly that is (estimated to be) from the named organism. p_match is the percent of the matched transcriptome that is found in the query. These numbers are decreased by both evolutionary distance AND by low coverage of the organism's gene set (low sequencing coverage, or little expression). sourmash also outputs a csv with this and other information. We will use this csv to visualize our results later. We just ran sourmash on our assembly to give us an idea of the taxonomic make up of our sample in relation to known sequences. We could also run sourmash on our raw reads. This would give us an idea of the number of organisms we fail to assemble, or the amount of taxonomic diversity that is missing from our assembly. Our sample has over 34 million reads and running gather on a sample this size would take about an hour or two. We ran sourmash like we did above, but with the reads, before this workshop. We will now download the csv and compare the results from the assembly and from the reads. First, link in the gather results from the raw reads. In the future, you can download them from here ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/scripts/ERR1719497_paired_gather_all.csv . Let's also download a script that we will use to plot the results. There are many visualizations we could use, however here we will use an upset plot. Upset plots are similar to Venn diagrams, but will work with many samples. In the future, you can download this script from here ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/scripts/plot-gather.py . python plot-gather.py this will produce a file 'plot-gather.png' that we need to download in order to visualize. Here is a copy of this output visualization. We can see that the reads have more matches than the assembly. Other notes We used the raw reads to do taxonomic classification. Although the errors present in the raw reads may slow sourmash down a little, there is a chance when error trimming transcriptomes that real biological variation is removed. Because sourmash is so specific, the errors don't give false positives and so keeping all of the possible real variation could improve taxonomic recall in some cases. This is a good way to detect contamination or wrong data! We recommend doing sourmash gather immediately after receiving your data from the sequencing facility. If your environmental metagenome has a tremendous amount of mouse sequence in it... maybe the sequencing facility sent you the wrong data?","title":"Sourmash Taxonomic Classification"},{"location":"sourmash-taxonomic-classification/#taxonomic-classification-with-sourmash","text":"Knowing what species are in our metatranscriptome allows us to take advantage of other resources already developed for those species, such as genome sequences and genome annotations. We will use a tool called sourmash to get a first pass look at what's in our metatranscriptome. Sourmash works by calculating a \"signature\" for a sequence. For taxonomic classification, you can then compare that signature to signatures made from publicly available data. To make this comparison faster, we put the publicly available sequences in a database that we can compare against. We have made databases from eukaryotic RNA sequences available in GenBank, as well as the MMETSP transcriptomes that were recently reassembled . After making signatures for our assembly, we will search against all of our databases at once. Note that sourmash cannot find matches across large evolutionary distances. sourmash seems to work well to search and compare data sets for matches at the species and genus level, but does not have much sensitivity beyond that. It seems to be particularly good at strain-level analysis. You should use protein-based analyses to do searches across larger evolutionary distances (e.g. with a tool like KEGG GhostKOALA ). Let's get started! First, let's make a directory and link in our assembly. We are going to work with a metatranscriptome assembly that we'll show you how to run later - here we link in a copy that we placed on the cluster already. mkdir -p ${PROJECT}/sourmash-gather cd ${PROJECT}/sourmash-gather ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/assembly/tara_f135_full_megahit.fasta . Next, let's make a signature of our assembly. sourmash uses k-mers to calculate signatures. A k-mer size of 21 is approximately specific at the genus level, a 31 is at the species level, and 51 at the strain level. We will calculate our signature with all three k-mer sizes so we can choose which one we want to use later. The --scaled parameter in the command tells sourmash to only look at 1/10,000th of the unique k-mer space. This parameter is agnostic to sequence content, but is consistent across signatures. Lastly, we use the --track-abundance flag. This tells sourmash to keep track of how often it sees a k-mer. This is meaningful for transcriptomic data because expression levels are variable. sourmash compute -k 21,31,51 --scaled 10000 --track-abundance -o tara_f135_full_megahit.sig tara_f135_full_megahit.fasta Now let's download some databases that contain signatures from transcriptomes of publicly-available eukaryotic sequences. We will download databases for plants, fungi, protozoa, invertebrates, and vertebrates, as well a database that contains sequences from the MMETSP re-assembly project (>700 marine eukaryotic transcriptomes). Even if you don't think a certain class of eukaryote is likely to be in your sample (for instance, vertebrate_mammalian), it's a good idea to include all of the databases anyway. It can help you quickly locate potential contamination in your sequencing (we wouldn't expect to find mouse RNA in the ocean, but it is a commonly sequenced organism and thus if you find it, if may be a contaminant). ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-vertebrate_other-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-vertebrate_mammalian-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-invertebrate-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-fungi-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-plant-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/genbank-rna-protozoa-k31.tar.gz . ln -fs /LUSTRE/bioinformatica_data/bioinformatica2018/sourmash_databases/mmetsp-k31-named.tar.gz . Note, if you want to download these files instead, do: wget -O genbank-rna-vertebrate_other-k31.tar.gz https://osf.io/qgyax/download wget -O genbank-rna-vertebrate_mammalian-k31.tar.gz https://osf.io/6c9uy/download wget -O genbank-rna-invertebrate-k31.tar.gz https://osf.io/7v8ck/download wget -O genbank-rna-fungi-k31.tar.gz https://osf.io/g6mcr/download wget -O genbank-rna-plant-k31.tar.gz https://osf.io/kctus/download wget -O genbank-rna-protozoa-k31.tar.gz https://osf.io/fnu2q/download wget -O mmetsp-k31-named.tar.gz https://osf.io/cdvqn/download but this will not work on the nodes on the cicese cluster! Next, we need to uncompress the databases: for infile in *.tar.gz do tar xf ${infile} done And now we can perform taxonomic classification! sourmash gather -k 31 --scaled 10000 -o tara_f135_full_megahit.csv \\ tara_f135_full_megahit.sig \\ *.sbt.json */*.sbt.json You should see an output that looks something like this: overlap p_query p_match avg_abund --------- ------- ------- --------- 4.3 Mbp 3.3% 5.5% 1.0 Neoceratium fusus PA161109 MMETSP1074 1.2 Mbp 0.9% 3.8% 1.0 Calcidiscus leptoporus RCC1130 MMETSP... 4.2 Mbp 0.2% 0.3% 1.1 Neoceratium fusus PA161109 MMETSP1075 130.0 kbp 0.1% 0.2% 1.0 Brandtodinium nutricula RCC3387 MMETS... 90.0 kbp 0.1% 0.3% 1.0 Emiliania huxleyi CCMP370 MMETSP1157 70.0 kbp 0.0% 0.3% 1.0 Pelagomonas calceolata RCC969 MMETSP1328 70.0 kbp 0.1% 0.5% 1.2 Prasinoderma singularis RCC927 MMETSP... 80.0 kbp 0.0% 0.1% 1.2 Protoceratium reticulatum CCCM535=CCM... 50.0 kbp 0.0% 0.0% 1.2 Symbiodinium sp. CCMP421 MMETSP1110 50.0 kbp 0.0% 0.1% 1.0 Heterocapsa rotundata SCCAPK-0483 MME... 50.0 kbp 0.0% 0.0% 1.5 Scrippsiella trochoidea CCMP3099 MMET... 60.0 kbp 0.0% 0.1% 1.5 Prorocentrum minimum CCMP2233 MMETSP0269 found less than 40.0 kbp in common. => exiting found 12 matches total; the recovered matches hit 4.9% of the query This will take about 5 to 10 minutes to run. The two columns to pay attention to are p_query and p_match . p_query is the percent of the metatranscriptome assembly that is (estimated to be) from the named organism. p_match is the percent of the matched transcriptome that is found in the query. These numbers are decreased by both evolutionary distance AND by low coverage of the organism's gene set (low sequencing coverage, or little expression). sourmash also outputs a csv with this and other information. We will use this csv to visualize our results later. We just ran sourmash on our assembly to give us an idea of the taxonomic make up of our sample in relation to known sequences. We could also run sourmash on our raw reads. This would give us an idea of the number of organisms we fail to assemble, or the amount of taxonomic diversity that is missing from our assembly. Our sample has over 34 million reads and running gather on a sample this size would take about an hour or two. We ran sourmash like we did above, but with the reads, before this workshop. We will now download the csv and compare the results from the assembly and from the reads. First, link in the gather results from the raw reads. In the future, you can download them from here ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/scripts/ERR1719497_paired_gather_all.csv . Let's also download a script that we will use to plot the results. There are many visualizations we could use, however here we will use an upset plot. Upset plots are similar to Venn diagrams, but will work with many samples. In the future, you can download this script from here ln -s /LUSTRE/bioinformatica_data/bioinformatica2018/scripts/plot-gather.py . python plot-gather.py this will produce a file 'plot-gather.png' that we need to download in order to visualize. Here is a copy of this output visualization. We can see that the reads have more matches than the assembly.","title":"Taxonomic Classification with Sourmash"},{"location":"sourmash-taxonomic-classification/#other-notes","text":"We used the raw reads to do taxonomic classification. Although the errors present in the raw reads may slow sourmash down a little, there is a chance when error trimming transcriptomes that real biological variation is removed. Because sourmash is so specific, the errors don't give false positives and so keeping all of the possible real variation could improve taxonomic recall in some cases.","title":"Other notes"},{"location":"sourmash-taxonomic-classification/#this-is-a-good-way-to-detect-contamination-or-wrong-data","text":"We recommend doing sourmash gather immediately after receiving your data from the sequencing facility. If your environmental metagenome has a tremendous amount of mouse sequence in it... maybe the sequencing facility sent you the wrong data?","title":"This is a good way to detect contamination or wrong data!"},{"location":"tara-sample-data/","text":"Intro to sample dataset: TARA Oceans The TARA Oceans Expedition (2009\u20132013) sampled contrasting ecosystems of the world oceans, collecting environmental data and plankton, from viruses to metazoans, for later analysis using modern sequencing and state-of-the-art imaging technologies. It surveyed 210 ecosystems in 20 biogeographic provinces, collecting over 35,000 samples of seawater and plankton ( Pesant et al. 2015 ). We chose data analyzed as part of A global ocean atlas of eukaryotic genes (Carradec et al. 2018) . This paper used a metatranscriptomics approach on the TARA Oceans data to generate a global ocean reference catalog of genes from planktonic eukaryotes and to explore their expression patterns with respect to biogeography and environmental conditions. For this workshop, we chose TARA Stations 135, 136, and 137 in the eastern pacific, which you can see on the map below. We analyze data from mRNAseq samples (poly-A selected and thus likely to contain mostly eukaryotic sequences) in the 5-20\u00b5m size fraction, as this fraction had good replication across our chosen TARA stations. For most of the tutorials, we use small subsets of these data, to make program runtimes feasible during the workshop. You can find the full data via Open science resources for the discovery and analysis of Tara Oceans data (Pesant et al. 2015) . Note that we will show you how to get some of the same kinds of answers from your data as the TARA paper, above, did! (It will be on a smaller scale because we need to run these at the workshop!) Recommended Reading: A global ocean atlas of eukaryotic genes Open science resources for the discovery and analysis of Tara Oceans data","title":"TARA Sample Data"},{"location":"tara-sample-data/#intro-to-sample-dataset-tara-oceans","text":"The TARA Oceans Expedition (2009\u20132013) sampled contrasting ecosystems of the world oceans, collecting environmental data and plankton, from viruses to metazoans, for later analysis using modern sequencing and state-of-the-art imaging technologies. It surveyed 210 ecosystems in 20 biogeographic provinces, collecting over 35,000 samples of seawater and plankton ( Pesant et al. 2015 ). We chose data analyzed as part of A global ocean atlas of eukaryotic genes (Carradec et al. 2018) . This paper used a metatranscriptomics approach on the TARA Oceans data to generate a global ocean reference catalog of genes from planktonic eukaryotes and to explore their expression patterns with respect to biogeography and environmental conditions. For this workshop, we chose TARA Stations 135, 136, and 137 in the eastern pacific, which you can see on the map below. We analyze data from mRNAseq samples (poly-A selected and thus likely to contain mostly eukaryotic sequences) in the 5-20\u00b5m size fraction, as this fraction had good replication across our chosen TARA stations. For most of the tutorials, we use small subsets of these data, to make program runtimes feasible during the workshop. You can find the full data via Open science resources for the discovery and analysis of Tara Oceans data (Pesant et al. 2015) . Note that we will show you how to get some of the same kinds of answers from your data as the TARA paper, above, did! (It will be on a smaller scale because we need to run these at the workshop!) Recommended Reading: A global ocean atlas of eukaryotic genes Open science resources for the discovery and analysis of Tara Oceans data","title":"Intro to sample dataset: TARA Oceans"},{"location":"welcome/","text":"Welcome! 1. Learning goals For you: get a first (or second) look at tools; gain some experience in the basic command line; get 80% of way to a complete analysis of some data; introduction to philosophy and perspective of data analysis in science; 2. Safe space and code of conduct This is intended to be a safe and friendly place for learning! Please see the workshop Code of Conduct In particular, please ask questions, because I guarantee you that your question will help others! 3. Instructor introductions Taylor Reiter - grad student at UC Davis Tessa Pierce - postdoc at UC Davis Titus Brown - prof at UC Davis in the School of Vet Med 4. Sticky notes and how they work Basic rules: no sticky note - \"working on it\" green sticky note - \"all is well\" red sticky note - \"need help!\" Place the sticky notes where we can see them from the back of the room -- e.g. on the back of your laptop. Next: Logging into the cluster","title":"Bienvenidos"},{"location":"welcome/#welcome","text":"","title":"Welcome!"},{"location":"welcome/#1-learning-goals","text":"For you: get a first (or second) look at tools; gain some experience in the basic command line; get 80% of way to a complete analysis of some data; introduction to philosophy and perspective of data analysis in science;","title":"1. Learning goals"},{"location":"welcome/#2-safe-space-and-code-of-conduct","text":"This is intended to be a safe and friendly place for learning! Please see the workshop Code of Conduct In particular, please ask questions, because I guarantee you that your question will help others!","title":"2. Safe space and code of conduct"},{"location":"welcome/#3-instructor-introductions","text":"Taylor Reiter - grad student at UC Davis Tessa Pierce - postdoc at UC Davis Titus Brown - prof at UC Davis in the School of Vet Med","title":"3. Instructor introductions"},{"location":"welcome/#4-sticky-notes-and-how-they-work","text":"Basic rules: no sticky note - \"working on it\" green sticky note - \"all is well\" red sticky note - \"need help!\" Place the sticky notes where we can see them from the back of the room -- e.g. on the back of your laptop. Next: Logging into the cluster","title":"4. Sticky notes and how they work"},{"location":"working-with-bioconda/","text":"Software Installation with Bioconda Learning objectives: learn what bioconda is understand basic conda commands learn how to list installed software packages learn how to manage multiple installation environments What is bioconda? See the bioconda paper and the bioconda web site . Bioconda is a community-enabled repository of 3,000+ bioinformatics packages, installable via the conda package manager. It consists of a set of recipes, like this one, for sourmash , that are maintained by the community. It just works, and it's effin' magic!! What problems does conda (and therefore bioconda) solve? Conda tracks installed packages and their versions, and makes sure that different installed packages don't have conflicting dependencies (we'll explain this below). Installing conda and enabling bioconda Download and install miniconda in your home directory: cd curl -O -L https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3 #install in $HOME directory echo export PATH=$PATH:/$HOME/miniconda3/bin >> ~/.bashrc Then, run the following command (or start a new terminal session) in order to activate the conda environment: source ~/.bashrc Configure channels for installing software. Note: It is important to add them in this order so that the priority is set correctly (that is, conda-forge is highest priority). conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Try installing something: conda install sourmash and running it -- sourmash will produce some output. (We'll tell you more about sourmash later.) yay! Using conda Conda is a \"package manager\" or software installer. See the full list of commands . conda install to install a package. conda list to list installed packages. conda search to search packages. Note that you'll see one package for every version of the software and for every version of Python (e.g. conda search sourmash ). During this workshop, we have been using the tara environment. We can expor this environment into a \"yaml\" text file to keep track of everything we have installed, including versions! conda env export -n tara -f $PROJECT/tara_conda_environment.yaml You can read more about exporting environments here Using bioconda bioconda is a channel for conda, which just means that you can \"add\" it to conda as a source of packages. That's what the conda config above does. Note, Bioconda supports only 64-bit Linux and Mac OSX. You can check out the bioconda site . Finding bioconda packages You can use conda search , or you can use google, or you can go visit the list of recipes . Constructing and using multiple environments A feature that we do not use much here, but that can be very handy in some circumstances, is different environments. \"Environments\" are multiple different collections of installed software. There are two reasons you might want to do this: first, you might want to try to exactly replicate a specific software install, so that you can replicate a paper or an old condition. second, you might be working with incompatible software, e.g. sometimes different software pipelines need different version of the same software. An example of this is older bioinformatics software that needs python2, while other software needs python3. To create a new environment named pony , type: conda create -n pony Then to activate (switch to) that environment, type: source activate pony And now when you run conda install , it will install packages into this new environment, e.g. conda install -y checkm-genome (note here that checkm-genome requires python 2). Freezing an environment This will save the list of conda-installed software you have in a particular environment to the file packages.txt : conda list --export packages.txt (it will not record the software versions for software not installed by conda.) conda install --file=packages.txt will install those packages in your local environment. To list environments, type: conda env list and you will see that you have two environments, base and pony , and pony has a * next to it because that's your current environment. Leaving an environment And finally, to switch back to your base environment, do: source deactivate and you'll be back in the original environment. Meditations on reproducibility and provenance If you want to impress reviewers and also keep track of what your software versions are, you can: manage all your software inside of conda use conda list --export software.txt to create a list of all your software and put it in your supplementary material. This is also something that you can record for yourself, so that if you are trying to exactly reproduce Using it on your own compute system (laptop or HPC) conda works on Windows, Mac, and Linux. bioconda works on Mac and Linux. It does not require admin privileges to install, so you can install it on your own local cluster quite easily.","title":"Bioconda"},{"location":"working-with-bioconda/#software-installation-with-bioconda","text":"Learning objectives: learn what bioconda is understand basic conda commands learn how to list installed software packages learn how to manage multiple installation environments","title":"Software Installation with Bioconda"},{"location":"working-with-bioconda/#what-is-bioconda","text":"See the bioconda paper and the bioconda web site . Bioconda is a community-enabled repository of 3,000+ bioinformatics packages, installable via the conda package manager. It consists of a set of recipes, like this one, for sourmash , that are maintained by the community. It just works, and it's effin' magic!!","title":"What is bioconda?"},{"location":"working-with-bioconda/#what-problems-does-conda-and-therefore-bioconda-solve","text":"Conda tracks installed packages and their versions, and makes sure that different installed packages don't have conflicting dependencies (we'll explain this below).","title":"What problems does conda (and therefore bioconda) solve?"},{"location":"working-with-bioconda/#installing-conda-and-enabling-bioconda","text":"Download and install miniconda in your home directory: cd curl -O -L https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3 #install in $HOME directory echo export PATH=$PATH:/$HOME/miniconda3/bin >> ~/.bashrc Then, run the following command (or start a new terminal session) in order to activate the conda environment: source ~/.bashrc Configure channels for installing software. Note: It is important to add them in this order so that the priority is set correctly (that is, conda-forge is highest priority). conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Try installing something: conda install sourmash and running it -- sourmash will produce some output. (We'll tell you more about sourmash later.) yay!","title":"Installing conda and enabling bioconda"},{"location":"working-with-bioconda/#using-conda","text":"Conda is a \"package manager\" or software installer. See the full list of commands . conda install to install a package. conda list to list installed packages. conda search to search packages. Note that you'll see one package for every version of the software and for every version of Python (e.g. conda search sourmash ). During this workshop, we have been using the tara environment. We can expor this environment into a \"yaml\" text file to keep track of everything we have installed, including versions! conda env export -n tara -f $PROJECT/tara_conda_environment.yaml You can read more about exporting environments here","title":"Using conda"},{"location":"working-with-bioconda/#using-bioconda","text":"bioconda is a channel for conda, which just means that you can \"add\" it to conda as a source of packages. That's what the conda config above does. Note, Bioconda supports only 64-bit Linux and Mac OSX. You can check out the bioconda site .","title":"Using bioconda"},{"location":"working-with-bioconda/#finding-bioconda-packages","text":"You can use conda search , or you can use google, or you can go visit the list of recipes .","title":"Finding bioconda packages"},{"location":"working-with-bioconda/#constructing-and-using-multiple-environments","text":"A feature that we do not use much here, but that can be very handy in some circumstances, is different environments. \"Environments\" are multiple different collections of installed software. There are two reasons you might want to do this: first, you might want to try to exactly replicate a specific software install, so that you can replicate a paper or an old condition. second, you might be working with incompatible software, e.g. sometimes different software pipelines need different version of the same software. An example of this is older bioinformatics software that needs python2, while other software needs python3. To create a new environment named pony , type: conda create -n pony Then to activate (switch to) that environment, type: source activate pony And now when you run conda install , it will install packages into this new environment, e.g. conda install -y checkm-genome (note here that checkm-genome requires python 2).","title":"Constructing and using multiple environments"},{"location":"working-with-bioconda/#freezing-an-environment","text":"This will save the list of conda-installed software you have in a particular environment to the file packages.txt : conda list --export packages.txt (it will not record the software versions for software not installed by conda.) conda install --file=packages.txt will install those packages in your local environment. To list environments, type: conda env list and you will see that you have two environments, base and pony , and pony has a * next to it because that's your current environment.","title":"Freezing an environment"},{"location":"working-with-bioconda/#leaving-an-environment","text":"And finally, to switch back to your base environment, do: source deactivate and you'll be back in the original environment.","title":"Leaving an environment"},{"location":"working-with-bioconda/#meditations-on-reproducibility-and-provenance","text":"If you want to impress reviewers and also keep track of what your software versions are, you can: manage all your software inside of conda use conda list --export software.txt to create a list of all your software and put it in your supplementary material. This is also something that you can record for yourself, so that if you are trying to exactly reproduce","title":"Meditations on reproducibility and provenance"},{"location":"working-with-bioconda/#using-it-on-your-own-compute-system-laptop-or-hpc","text":"conda works on Windows, Mac, and Linux. bioconda works on Mac and Linux. It does not require admin privileges to install, so you can install it on your own local cluster quite easily.","title":"Using it on your own compute system (laptop or HPC)"}]}